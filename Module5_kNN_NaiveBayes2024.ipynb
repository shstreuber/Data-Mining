{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3EsZyhZMne2P"
      ],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shstreuber/Data-Mining/blob/master/Module5_kNN_NaiveBayes2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt6qwtGlIJ6G"
      },
      "source": [
        "# **Module 5: CLASSIFICATION: k Nearest Neighbor and Naive Bayes**\n",
        "In this module, we will step into machine learning with the k Nearest Neighbor and Naive Bayes algorithms. At the end of this module, you will be able to:\n",
        "* Outline the concepts of Supervised Learning\n",
        "* Explain what classification is\n",
        "* Describe how k Nearest Neighbor works\n",
        "* Describe how Naive Bayes works\n",
        "* Write code to execute both, k Nearest Neighbor and Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkWv-q8OiWG6"
      },
      "source": [
        "##**Supervised Learning**\n",
        "This time, we are going to take the whole idea of forecasting a step further. With last week's logistic regression, we learned how to build a model that sorts data into 1 of two factors in a target attribute; in other words: A model with binary class outcome. This week, we are going to work with a categorical class attribute and two different machine learning mechanisms: The empirical classifier k Nearest Neighbor and the statistical classifier Naive Bayes.\n",
        "\n",
        "Both are part of **SUPERVISED LEARNING**. A supervised machine learning algorithm relies on labeled input data to learn a function that produces an appropriate output when given new unlabeled data. Imagine a computer is a child, we are its supervisor (e.g. parent, guardian, or teacher), and we want the child (computer) to learn what a book looks like. We will show the child several different pictures, some of which are books and the rest could be pictures of anything (cats, coffee cups, computers, etc).\n",
        "When we see a book, we shout \"book!” When it’s not a pig, we shout “no, not book!” After doing this several times with the child, we show them a picture and ask “book?” and they will correctly (most of the time) say “book!” or “no, not book!” depending on what the picture is. That is supervised learning. Kind of like this:\n",
        "\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20231121154747/Supervised-learning.png\">\n",
        "\n",
        "Now, please watch the video below. It's a great introduction to Supervised Learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj5-hiXkYzzI"
      },
      "source": [
        "from IPython.display import IFrame  # This is just for me so I can embed videos\n",
        "IFrame(src=\"https://www.youtube.com/embed/kE5QZ8G_78c\", width=560, height=315)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0xAsvw1bDCH"
      },
      "source": [
        "##**Classification**\n",
        "Classification is the problem of identifying which of a set of categories an observation belongs to. For example: Classification helps us assign a given email to the \"spam\" or \"non-spam\" class, or a diagnosis to a given patient based on observed characteristics of the patient.\n",
        "\n",
        "In machine learning, classification has these steps:\n",
        "1. Determine what the class attribute in the dataset should be. This will be the attribute you'll predict later on\n",
        "2. Preprocess the data (remove n/a, transform data types as needed, deal with missing data) and ensure that the dependent attribute is CATEGORICAL\n",
        "3. Split the data into a training set and a test set\n",
        "4. Build the model based on the training set\n",
        "5. Test the model on the test set and compare the calculated class values to the actual class values shown in the test set.\n",
        "6. Determine the quality of the model\n",
        "\n",
        "Ready? Let's go.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdkVv9lncnIq"
      },
      "source": [
        "#**0. Preparation and Setup**\n",
        "For these explanations, we will need a model with a dependent attribute that is categorical. The typical explanation uses the famous [iris flower dataset](https://github.com/shstreuber/Data-Mining/blob/master/data/iris.csv), which even has [its own wikipedia page](https://en.wikipedia.org/wiki/Iris_flower_data_set). However, we will use the insurance dataset because it allows us to tackle actual real-world problems. Since we will be working with two different types of classification, the first one called k Nearest Neighbor, and the second on called Naive Bayes, we will import all the libraries upfront."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_OxV6K4YV3B"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import spatial\n",
        "import statsmodels.api as sm\n",
        "\n",
        "from IPython.display import HTML # This is just for me so I can embed videos\n",
        "from IPython.display import Image # This is just for me so I can embed images\n",
        "\n",
        "#Reading in the data as insurance dataframe\n",
        "insurance = pd.read_csv(\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/data/insurance_with_categories.csv\")\n",
        "\n",
        "#Verifying that we can see the data\n",
        "insurance.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_glAv3NIJ6h"
      },
      "source": [
        "# **1. k Nearest Neighbor**\n",
        "The concept of the k-nearest neighbor classifier is part of our everyday life.\n",
        "\n",
        "Imagine you are the owner of an online clothing store and you want to implement a recommendation system to suggest products to your customers based on their preferences and past purchases. This is where the K-Nearest Neighbors (KNN) algorithm can be incredibly useful.\n",
        "\n",
        "**How KNN Works in This Scenario:**\n",
        "1. **Data Collection:** You have data on past purchases and preferences of your customers. Each customer has a profile that includes features like age, gender, preferred clothing styles (e.g., casual, formal), favorite colors, sizes, and previous purchase history. For example:\n",
        "\n",
        "```\n",
        "Customer Data:\n",
        "- Customer A: Age: 25, Gender: Female, Style: Casual, Favorite Colors: Blue, Size: M, Purchased Items: [Jeans, T-shirts]\n",
        "- Customer B: Age: 30, Gender: Male, Style: Formal, Favorite Colors: Black, Size: L, Purchased Items: [Suits, Dress Shirts]\n",
        "```\n",
        "2. **Feature Representation:** Convert each customer’s profile into a feature vector. A feature vector might look like this for Customer A:\n",
        "\n",
        "```\n",
        "[25, 1 (Female), 1 (Casual), 0 (Formal), 0 (Favorite Colors: Blue), 0 (Size: M), 1 (Purchased: Jeans), 1 (Purchased: T-shirts)]\n",
        "```\n",
        "\n",
        "3. **Similarity Measurement:**\n",
        "\n",
        "When a new customer visits your store and browses certain products, you represent their current preferences as a feature vector.\n",
        "For example, a new Customer C might have the following profile:\n",
        "\n",
        "\n",
        "```\n",
        "[22, 1 (Female), 1 (Casual), 0 (Formal), 1 (Favorite Colors: Blue), 0 (Size: M)]\n",
        "\n",
        "```\n",
        "4. **kNN Algorithm:** Calculate the distance (similarity) between Customer C’s feature vector and the feature vectors of all existing customers. This distance can be calculated using various methods such as Euclidean distance.\n",
        " * Then, identify the 'k' customers whose feature vectors are closest to Customer C’s vector. For simplicity, let’s assume k = 3.\n",
        " * Suppose the three nearest neighbors (most similar customers) to Customer C are Customer A, Customer D, and Customer E.\n",
        "\n",
        "5. **Recommendation:** Analyze the purchase history of these 3 nearest neighbors.\n",
        " * Recommend products that these neighbors have purchased but Customer C has not.\n",
        " * For instance, if Customers A, D, and E all bought a particular jacket and Customer C has not bought it yet, the system will recommend this jacket to Customer C.\n",
        "\n",
        "<img src = \"https://recosenselabs.com/wp-content/uploads/2021/07/Category-Page-Recomendation.png\">\n",
        "\n",
        "<hr>\n",
        "\n",
        "In a nutshell, the **principle behind nearest neighbor classification** consists in identifying a predefined number, i.e. the 'k' - of training samples closest in (Euclidian) distance to the new sample that we want to classify. The label of the new sample will be defined based on these neighbors.\n",
        "\n",
        "Here is what this looks like:\n",
        "<img src=\"https://www.kdnuggets.com/wp-content/uploads/arya_knearest_neighbors_classification_3.jpg\">\n",
        "\n",
        "Would you like a more in-depth explanation? The video below gives you all the detail you will want to know as you proceed.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IFrame(src=\"https://www.youtube.com/embed/0p0o5cmgLdE\", width=560, height=315)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "o6cvuRWkftwy",
        "outputId": "e816d30c-a787-443a-88f4-b0d0105e241f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7957e1d38d30>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"560\"\n",
              "            height=\"315\"\n",
              "            src=\"https://www.youtube.com/embed/0p0o5cmgLdE\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ice_fMBvIJ6t"
      },
      "source": [
        "##**1.1 Nearest Neighbor Algorithm**\n",
        "The k Nearest Neighbor Algorithm works like this:\n",
        "1. Load the data\n",
        "2. Initialize K to your chosen number of neighbors\n",
        "3. For each example in the data:\n",
        "  \n",
        "  3.1 Calculate the distance between the query example and the current example from the data.\n",
        "  \n",
        "  3.2 Add the distance and the index of the example to an ordered collection\n",
        "4. Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by the distances\n",
        "5. Pick the first K entries from the sorted collection\n",
        "6. Get the labels of the selected K entries\n",
        "7. If classification, return the mode of the K labels\n",
        "\n",
        "To work with the k Nearest Neighbor algorithm, we use its library from the scikit learn package. We will also learn a new way to build training and test sets (with a process called cross-validation), so we are importing that package, too. Lastly, we will be generating \"pretty pictures\"--so, matplotlib is going to help us out with that.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds7QcWXVIJ7m"
      },
      "source": [
        "# We import all the kNN libraries\n",
        "\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import neighbors, datasets\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxOsBveiIJ8r"
      },
      "source": [
        "##**1.2 Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdUIGgK4s7wz"
      },
      "source": [
        "1. Let's investigate the features (= attributes or dimensions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3jvuK-vIJ-_"
      },
      "source": [
        "insurance.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jh0Uka9kLO2"
      },
      "source": [
        "We have 4 numeric attributes and 3 categorical ones.\n",
        "\n",
        "2. Let's rearrange the numeric features into a dataframe and use them to predict what region a person comes from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzYzLG4UlDvR"
      },
      "source": [
        "insurance2 = pd.DataFrame(insurance, columns = ['age', 'bmi', 'children','charges','region'])\n",
        "insurance2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH1a8QAYIKAV"
      },
      "source": [
        "# Let's check what the levels of region are!\n",
        "insurance2.region.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wudd37G_nQOX"
      },
      "source": [
        "And now for some fun stuff! There is a great package that makes EDA much much easier: **The [Ydata profiling package](https://docs.profiling.ydata.ai/latest/).**\n",
        "\n",
        "Here is how it works:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ydata_profiling import ProfileReport # importing the package\\\n",
        "profile = ProfileReport(insurance2, title=\"Insurance 2 Profiling Report\") # Connecting the package and our data\n",
        "profile # And we call the package. This will take a moment. Prepare to be amazed!"
      ],
      "metadata": {
        "id": "OnCWjsXXRB62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKl7xoQD3dKV"
      },
      "source": [
        "##**1.3 Setting up Training and Test Sets**\n",
        "To set up our training and test sets, we first split the independent variables (which we are assuming are age, bmi, children, and charges) and the class attribute (region), which contains the labels that we want to assign to the \"unknown\" data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PVH8GKCIKEP"
      },
      "source": [
        "x=insurance2.iloc[:,:4] # all parameters\n",
        "y=insurance2['region'] # class labels 'southwest', 'southeast', 'northwest', 'northeast'\n",
        "\n",
        "#print(x) # Uncomment this line to verify your parameters/ independent variables/ attributes/ features\n",
        "#print(y) # Uncomment this line to verify your class labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuzsSUvVHJsM"
      },
      "source": [
        "Now that we have separated the x attributes (independent variables) and the y attribute (class attribute, dependent variable), we build our training and test sets!\n",
        "\n",
        "NOTE that we are not allocating any sizes for the train_test_split below. This will invoke the default, which is a 75% training/ 25% test split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0xJHSiTIKQa"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state = 0)\n",
        "\n",
        "# So, what training data do we have?\n",
        "print(\"X_train shape: {}\".format(X_train.shape))\n",
        "print(\"y_train shape: {}\".format(y_train.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdFwC9M86UqH"
      },
      "source": [
        "##Your Turn\n",
        "How many rows and columns do you have in the test data set? Write the command below and run it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_xfs9j-IKRH"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMdKX2sRIKPZ"
      },
      "source": [
        "##**1.4 Building the Simplest Model with k=1**\n",
        "\n",
        "Remember that, in kNN classification, the output is class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
        "\n",
        "We'll try that out first.[link text](https://)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Q-QH9fAgIKRv"
      },
      "source": [
        "# This is the model with the one nearest neighbor. The default is the Euclidian distance.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "model1 = KNeighborsClassifier(n_neighbors = 1)\n",
        "model1.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7a73cpU_iQL"
      },
      "source": [
        "###**1.4.1 Testing the Model**\n",
        "\n",
        "As you can see, we have built model1, which is the kNN model with just 1 nearest neighbor. Next, we test it.\n",
        "\n",
        "We use y_pred to store the calculated y values (remember y hat?) that the model gives us. Then we can compare them with the actual y values that we know and see what percentage the model identified correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXHRpANIIKSd"
      },
      "source": [
        "y_pred = model1.predict(X_test)\n",
        "print(\"Test set predictions: \\n {}\".format(y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5zUjMFvOcK9"
      },
      "source": [
        "###**1.4.2 Evaluating the model**\n",
        "Here, we use three critical methods to get an idea of how \"good\" our model really is.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPQZ0nMBJ2DQ"
      },
      "source": [
        "####1.4.2.1 The Accuracy Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6gCBHPUIKTS"
      },
      "source": [
        "# Accuracy score\n",
        "print(\"Test set score: {:.2f}\".format(model1.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxZ8G0g0KL8o"
      },
      "source": [
        "Oh boy! The accuracy score means that only 36% of all unlabelled data is classified correctly. That number could be better. Let's see if the other methods indicate the same thing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25XhYJpOX1dq"
      },
      "source": [
        "####1.4.2.2 Data Inspection Calculated vs. Actual values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbvxJdDGJ6Do"
      },
      "source": [
        "# Let's compare the actual y and the predicted y\n",
        "\n",
        "realvsmodel1 = pd.DataFrame(y_pred,y_test)\n",
        "realvsmodel1 = pd.DataFrame({'predicted':y_pred,'original':y_test})\n",
        "realvsmodel1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcu5LeZcKUse"
      },
      "source": [
        "Well, that doesn't look too exciting, does it? Let's go to the last and MOST IMPORTANT way of evaluating the quality of our model: **The Confusion Matrix.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhN3LKPrMAKc"
      },
      "source": [
        "####1.4.2.3 The Confusion Matrix\n",
        "\n",
        "A confusion matrix compares the calculated or predicted values for all labels in the class attribute with the actual, true values that we know. In other words, we check which true values were predicted correctly and which were predicted incorrecty.\n",
        "\n",
        "A longer, more mathematical, explanation is [here](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw-3P8PeZRtN"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred, labels=model1.classes_)\n",
        "cm_display = ConfusionMatrixDisplay(cm, display_labels=model1.classes_).plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXbUuwkANB42"
      },
      "source": [
        "Alrighty. What are we seeing here?\n",
        "\n",
        "The true (=actual original) values appear in rows (label on the left side, down), the predicted values appear in columns (label across the top). Here is how to explain **the first row**:\n",
        "* All the items should have been in the northeast region because the True Label is 'northeast'. That is clearly not the case; otherwise, we would see 112 in the green box in the upper left and zeros across the rest of the row.\n",
        "* Of the actual true 'northeast' region, 35 were predicted correctly (lime); 18 were incorrectly predicted as 'northwest', 17 were incorrectly predicted as 'southeast', and 26 were incorrectly predicted as 'southwest'. So, out of 112 actual 'northeast' rows, only 35 were predicted correctly. The rest were predicted incorrectly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv0cINFSOwSp"
      },
      "source": [
        "###1.4.2.4 The Classification Report\n",
        "\n",
        "The Classification Report gives us even more insights into how well (or, in our case, badly) our model performs. To read it correctly, we first have to define a few terms:\n",
        "1. **precision** (also called positive predictive value) is the number of correctly identified positive results divided by the number of all positive results, including those not identified correctly ((true positives) / (true positives + false positives)). Said another way, “for all instances classified positive, what percent was correct?”\n",
        "2. **recall** (also known as sensitivity) is the number of correctly identified positive results divided by the number of all samples that should have been identified as positive ((true positives) / (true positives + false negatives)). Said another way, “for all instances that were actually positive, what percent was classified correctly?\n",
        "3. **f-1 score** is the harmonic mean of the precision and recall. The highest possible value of F1 is 1, indicating perfect precision and recall, and the lowest possible value is 0, if either the precision or the recall is zero. As a rule of thumb, the weighted average of F1 should be used to compare classifier models, not global accuracy.\n",
        "3. **support** is the number of actual occurrences of the class in the specified dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0S9XzqdM_1u"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred, labels=['southwest', 'southeast', 'northwest', 'northeast']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6yUWu2GPzeK"
      },
      "source": [
        "Look at these values (note that \"support\" is the number of all true label quantities)--would you accept this quality from, say, a dentist? Or from your car, which (in the case of southwest), engages the brakes in 18% of all cases when you step on the pedal?\n",
        "\n",
        "<img src = \"https://www.shutterstock.com/shutterstock/videos/697813/thumb/1.jpg?ip=x480\" height=200>\n",
        "Yeah, I thought so, too. Not good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffwJ5JBqIKUD"
      },
      "source": [
        "## **1.5 Building kNN with 5 nearest neighbors (and a 2/3 to 1/3 train/ test split)**\n",
        "Given our less-than fabulous results above, let's see if instead of assigning the class label from only 1 nearest neighbor, we can increase the accuracy of our predictions by looking at the class labels for the 5 nearest neighbors!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft7hoUyhRMIb"
      },
      "source": [
        "###1.5.1 Setting up Training and Test Set\n",
        "**Note** how here, we use the test_size parameter to split 2/3 of the data into the training set and 1/3 into the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcy-VXSTIKUK"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.33)\n",
        "\n",
        "print(\"X_train shape: {}\".format(X_train.shape))\n",
        "print(\"y_train shape: {}\".format(y_train.shape))\n",
        "print(\"X_test shape: {}\".format(X_test.shape))\n",
        "print(\"y_test shape: {}\".format(y_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfpXDfqyRcO7"
      },
      "source": [
        "###1.5.2 Building the model with k=5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1qY2KmcIKU7"
      },
      "source": [
        "model5 = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "model5.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model5.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGCYWn-aVSc6"
      },
      "source": [
        "Alright, we've got the model built. Everyone ready for the Confusion Matrix and the Classification Report?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbgDfHDIVher"
      },
      "source": [
        "cm = confusion_matrix(y_test, y_pred, labels=model5.classes_)\n",
        "cm_display = ConfusionMatrixDisplay(cm, display_labels=model5.classes_).plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XKnQIxuVroI"
      },
      "source": [
        "print(classification_report(y_test, y_pred, labels=['southwest', 'southeast', 'northwest', 'northeast']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6BE4vI1IKVo"
      },
      "source": [
        "Wait, seriously? Why are we getting a result that is only a little bit different, but we're spending much more processing effort?\n",
        "\n",
        "<img src = \"https://www.shutterstock.com/shutterstock/videos/697813/thumb/1.jpg?ip=x480\" height=200>\n",
        "\n",
        "There's got to be a **BETTER** way to find the optimal number for k!\n",
        "\n",
        "And there is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D_fXddDSLUq"
      },
      "source": [
        "##**1.6 Optimizing k with Cross-Validation**\n",
        "We could spend entire days re-running the kNN and increasing k by 1 until we've found the best value for k. But that would be 1. boring, 2. too much work, 3. not efficient, given that we could instead just cycle through a list of values until we've found the best one.\n",
        "\n",
        "To achieve this most efficiently, we can use another trick (aka a preprocessing method) that we haven't encountered yet: **Cross-validation.** Find out [in this detail description](https://machinelearningmastery.com/k-fold-cross-validation/) how cross-validation works. [This graphic](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png) will also help you understand. Or watch this 5-minute video:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "YQXEtCxrZzkp",
        "outputId": "b80986a5-8586-4b35-88f0-d75df0a753aa"
      },
      "source": [
        "IFrame(src=\"https://www.youtube.com/embed/fSytzGwwBVw\", width=560, height=315)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7957e1d3b520>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"560\"\n",
              "            height=\"315\"\n",
              "            src=\"https://www.youtube.com/embed/fSytzGwwBVw\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c6x3eH_aUYV"
      },
      "source": [
        "This what we'll do now. Let's go!\n",
        "\n",
        "**First**, we build a list of potential k values. Then we create an empty list that will hold cross-validation scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t0e0_mVIKVt"
      },
      "source": [
        "# To determine how to pick k, we are first creating a list of potential k values\n",
        "klist = list(range(1,50,2)) # Our list goes from 1 to 50 in increments of 2\n",
        "\n",
        "# Then we create an empty list that will hold cross-validation scores\n",
        "cv_scores = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnoHxO2uatiV"
      },
      "source": [
        "**Now we can build our cross-validation.** We will cycle through our k values from the k-value list and store the accuracy scores in the cv_scores list. To make things easier on us, we will convert the accuracy score into its opposite--the misclassification error. This misclassification error is really the average of all the misclassifications for one run of k."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzqf4YaFaxOv"
      },
      "source": [
        "# Perform 10-fold cross validation for each k value (we have a small dataset, so we can do this)\n",
        "for k in klist:\n",
        "    model10 = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
        "    scores = cross_val_score(model10, x, y, cv=10, scoring='accuracy')\n",
        "    cv_scores.append(scores.mean())\n",
        "\n",
        "# Changing to misclassification error\n",
        "errors = 1- np.array(cv_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUDbFgn3bIbf"
      },
      "source": [
        "NOW we can use the error number to determine the optimal k! To do so, we look at our errors and pick the row with the k value that produced the smallest error.\n",
        "\n",
        "To make things easier to understand, we plot the misclassification errors in comparison to k so we can see our results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIRA1hbGbJRp"
      },
      "source": [
        "optimal_k = klist[np.argmin(errors)]\n",
        "print(\"The optimal number of neighbors is {}\".format(optimal_k))\n",
        "\n",
        "# plot misclassification error vs k\n",
        "plt.plot(klist, errors)\n",
        "plt.xlabel('Number of Neighbors K')\n",
        "plt.ylabel('Misclassification Error')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Not sure what the code above does?** Here is the explanation:\n",
        "* **klist**: This is a list containing different values of 'k', which represent the number of neighbors considered in the K-Nearest Neighbors (KNN) algorithm.\n",
        "* **errors**: This is a list that contains the misclassification error corresponding to each value of 'k' in klist.\n",
        "* **np.argmin(errors)**: This function returns the index of the smallest value in the errors list. In other words, it finds the value of 'k' that results in the lowest misclassification error.\n",
        "* **klist[np.argmin(errors)]**: This retrieves the value of 'k' from klist that corresponds to the minimum error. This is considered the optimal number of neighbors for the KNN algorithm based on the data."
      ],
      "metadata": {
        "id": "-yvxxG24gYsC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ho8UgYSStbv"
      },
      "source": [
        "##Your Turn\n",
        "Re-run this model with k=10 and k=50; for each of these, build and run a confusion matrix and a classification report. What changes? What do your results say about the data?\n",
        "\n",
        "Use the lines below for your code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pK5Kjhuc6q5"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd1cXY_0c235"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9YkRp3gIKig"
      },
      "source": [
        "# **2. Naive Bayes**\n",
        "Naive Bayes is a probabilistic classifier based on Bayes' Theorem. It assumes that the features (predictors) are independent of each other given the class, which is often not true in real life, hence the term \"naive\". Despite this naive assumption, Naive Bayes works very well in many practical applications.\n",
        "\n",
        "**Bayes' Theorem** is the foundation of Naive Bayes and is expressed as:\n",
        "\n",
        "<img src= \"https://thatware.co/wp-content/uploads/2020/04/naive-bayes.png\">\n",
        "\n",
        "Where:\n",
        "\n",
        "* P(A∣B) is the probability of event A happening given that event B has happened.\n",
        "* P(B∣A) is the probability of event B happening given that event A has happened.\n",
        "* P(A) is the prior probability of event A.\n",
        "* P(B) is the prior probability of event B.\n",
        "\n",
        "##**Example: Email Spam Classification**##\n",
        "\n",
        "Let's use a simple example of classifying emails as \"Spam\" or \"Not Spam\" based on certain words in the email.\n",
        "\n",
        "<img src = \"https://miro.medium.com/v2/resize:fit:720/format:webp/0*mbFBPcPUJD-53v3h.png\">\n",
        "\n",
        "1. **Step 1: Training Data**\n",
        "\n",
        "Suppose we have the following training data:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Email\t   Contains \"Win\"\t Contains \"Money\"\tContains \"Free\"\t  Class\n",
        "Email 1\t Yes\t            Yes\t              Yes\t              Spam\n",
        "Email 2\t No\t             Yes\t              Yes\t              Spam\n",
        "Email 3\t Yes\t            No\t               Yes\t              Spam\n",
        "Email 4\t No\t             No\t               No\t              Not Spam\n",
        "Email 5\t Yes\t            No\t               No\t              Not Spam\n",
        "Email 6\t No\t             Yes\t              No\t              Not Spam\n",
        "```\n",
        "<hr>\n",
        "\n",
        "2. **Step 2: Calculate Probabilities**\n",
        "\n",
        "We need to calculate the **prior probabilities** and the likelihoods.\n",
        "\n",
        "Prior Probabilities:\n",
        "\n",
        "* P(Spam)= 3/6 = 0.5 # 3 rows out of a total of 6 rows\n",
        "* P(NotSpam)= 3/6 = 0.5 # 3 rows out of a total of 6 rows\n",
        "\n",
        "Likelihoods:\n",
        "\n",
        "* P(Contains \"Win\"∣Spam)= 2/3  (i.e. of the 3 rows with Spam, 2 contain Win)\n",
        "* P(Contains \"Money\"∣Spam)= 2/3 (i.e. of the 3 rows with Spam, 2 contain Money)\n",
        "* P(Contains \"Free\"∣Spam)= 3/3 = 1 (i.e. of the 3 rows with Spam, 3 contain Free)\n",
        "* P(Contains \"Win\"∣Not Spam)= 1/3 (i.e. of the 3 rows with Not Spam, 1 contains Win)\n",
        "* P(Contains \"Money\"∣Not Spam)= 2/3\n",
        "* P(Contains \"Free\"∣Not Spam)= 1/3\n",
        "\n",
        "<hr>\n",
        "\n",
        "3. **Step 3: Classify a New Email**\n",
        "\n",
        "Suppose we receive a new email that contains the words \"Win\" and \"Free\" but not \"Money\". We want to classify it as \"Spam\" or \"Not Spam\".\n",
        "\n",
        "<img src =\"https://github.com/shstreuber/Data-Mining/blob/master/images/naivebayes_spam.JPG?raw=true\">\n",
        "\n",
        "<img src = \"https://ih1.redbubble.net/image.490263180.2295/bg,f8f8f8-flat,750x,075,f-pad,750x1000,f8f8f8.jpg\" height = 300>\n",
        "\n",
        "This example shows how Naive Bayes is a straightforward yet powerful classification algorithm. It works well in many real-world situations, such as spam detection, text classification, and more. The key steps involve calculating prior probabilities, likelihoods, and using Bayes' Theorem to determine the posterior probability for each class, ultimately choosing the class with the highest probability.\n",
        "\n",
        "**Need More Information?**\n",
        "* Here is **a [great explanation](https://www.datacamp.com/community/tutorials/naive-bayes-scikit-learn)** of the principle behind the Bayes Theorem.\n",
        "* And here is a great video that explains it very well:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IFrame(src=\"https://www.youtube.com/embed/O2L2Uv9pdDA\", width=560, height=315)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "3YfuVY9FqMZd",
        "outputId": "5789347d-fdc8-4035-a325-bf38a869d7eb"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7957e1c3dea0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"560\"\n",
              "            height=\"315\"\n",
              "            src=\"https://www.youtube.com/embed/O2L2Uv9pdDA\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r8HEjAvrLhI"
      },
      "source": [
        "##Your Turn\n",
        "Given the results from the EDA, should we conduct a Naive Bayes Analysis, at all? What condition does the insurance2 dataset violate? Type your answer below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UwWwpierHF1"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXTUHXNvnpG7"
      },
      "source": [
        "##**2.1 Setting up the Environment**\n",
        "We will be working with Scikit-Learn again. More specifically, we will be working with the Gaussian Naive Bayes algorithm. Are there other Naive Bayes algorithms? [Absolutely](https://scikit-learn.org/stable/modules/naive_bayes.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx7DPgs_IKjm"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0BzHbFflwzU"
      },
      "source": [
        "#Let's verify that the dataset is still what it needs to be:\n",
        "insurance2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5FZEpfBq6f-"
      },
      "source": [
        "insurance2.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQcXqsp6IKlR"
      },
      "source": [
        "##**2.2 Setting up the training and test sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IC9jLLNIKlb"
      },
      "source": [
        "ins_train, ins_test = train_test_split(insurance2, test_size = 0.2)\n",
        "print(ins_train)\n",
        "print(ins_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBEgLbJqt6nX"
      },
      "source": [
        "ins_train_np = np.array([ins_train])\n",
        "ins_test_np = np.array([ins_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h8x3z3sjrn9"
      },
      "source": [
        "##**2.3 Building the model with GaussianNB()**\n",
        "Below, we are fitting the model to the X and y training sets, so that our model can learn what the correct classifications are.\n",
        "The first parameter for the fit function is the X-training set, which contains all of the FEATURES in the training dataframe (i.e. the input variables in ins_train). The second parameter for the fit function is the y-training set, which contains the LABELS, i.e. the known outcomes for ins_train. These outcomes are stored in the 'region' column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdkRBdqljpcy"
      },
      "source": [
        "ins_naivebayes = GaussianNB()\n",
        "ins_naivebayes.fit(ins_train.drop('region',axis=1), ins_train['region'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gdUlh18jy8d"
      },
      "source": [
        "##**2.4 Testing the model and calculating the accuracy score**\n",
        "Below, we are using the ins_naivebayes model on the FEATURES in the test data in order to predict their LABELS. This means that we need to remove the old (known) 'region' data in order to use only the features and the model to predict the new 'region' labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78LcEwVPIKmj"
      },
      "source": [
        "ins_predictions = ins_naivebayes.predict(ins_test.drop('region',axis=1))\n",
        "accuracy_score(ins_test['region'], ins_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ins_predictions"
      ],
      "metadata": {
        "id": "kTvUULUWUYzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7_xFhTIIKnm"
      },
      "source": [
        "##**2.5 Comparing the predicted and the original values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrWTCzRQkPAE"
      },
      "source": [
        "realvsmodel2 = pd.DataFrame(ins_predictions,ins_test)\n",
        "realvsmodel2 = pd.DataFrame({'predicted':ins_predictions,'original':ins_test['region']})\n",
        "realvsmodel2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEEX_roRTRU-"
      },
      "source": [
        "##Your Turn\n",
        "Plot a confusion matrix. For your code, refer to section 1.4.2.3 above:"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fun1DbnPZAvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e72-66uzZBP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiLLUOJKnTGu"
      },
      "source": [
        "## Your Turn\n",
        "Build a classification report. For your code, refer to section 1.4.2.4 above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6iQSgRjndmw"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIWl6D3vneew"
      },
      "source": [
        "##Your Turn\n",
        "Compare the results from the confusion matrix and the classification report for k Nearest Neighbor and Naive Bayes. Which model produces better results? Write your answer into the text field below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU-nAxchn3YW"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g22TRE4mTSLE"
      },
      "source": [
        "##Your Turn\n",
        "How valid are the results for the Naive Bayes classification really? Review the assumptions for work with Naive Bayes, especially regarding dependency among independent attribtues, and then look again at the results of the pandas_profiling output above. Write your answer into the text field below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCiVwilzrles"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVMlu10DoYof"
      },
      "source": [
        "##Your Turn\n",
        "So ... given the data that we have, can we reliably predict the region someone lives in based on their age, bmi, number of children, and the $ amount of their insurance claims? Explain in the field below."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K7WWyF4OpZVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. If you get stuck"
      ],
      "metadata": {
        "id": "3EsZyhZMne2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the kNN model with k = 10\n",
        "model10 = neighbors.KNeighborsClassifier(n_neighbors=10)\n",
        "model10.fit(X_train, y_train)\n",
        "y_pred = model10.predict(X_test)"
      ],
      "metadata": {
        "id": "iuqtoRdenycI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the kNN model with k = 50\n",
        "model50 = neighbors.KNeighborsClassifier(n_neighbors=50)\n",
        "model50.fit(X_train, y_train)\n",
        "y_pred = model50.predict(X_test)"
      ],
      "metadata": {
        "id": "eWD2ODsDn_De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU1Up9ujr8jh"
      },
      "source": [
        "The insurance2 dataset does not contain completely independent attributes. The EDA shows that several attributes are highly correlated. This is known as MULTICOLLINEARITY. It is not ideal for Naive Bayes analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=yourmodelname.classes_)\n",
        "cm_display = ConfusionMatrixDisplay(cm, display_labels=yourmodelname.classes_).plot()"
      ],
      "metadata": {
        "id": "wAMb0cQYo4XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "print(classification_report(y_test, y_pred, labels=['southwest', 'southeast', 'northwest', 'northeast']))"
      ],
      "metadata": {
        "id": "mqCi5mQMovXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the reports between kNN and Naive Bayes--you are looking for greater accuracy in classification outcomes."
      ],
      "metadata": {
        "id": "jtKRtnGDpDx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1qGCbHDKpORu"
      }
    }
  ]
}