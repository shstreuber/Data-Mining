{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shstreuber/Data-Mining/blob/master/Module8_DeepLearning2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3utp-S1ay5r8"
      },
      "source": [
        "#**Module 8: Neural Networks and Deep Learning**\n",
        "Imagine you have lots of data--Big Data, as in 1,000,000 tuples and more per day--and need to build a classification system with utmost reliability because if you're wrong, the consequences may be detrimental to people or property. Would you use a simple tree? Or a k Nearest Neighbor? Or a Random Forest? Or would you want a system that combines a number of self-optimizing algorithm runs with an element of randomization and voting in order to give you the most reliable output?\n",
        "\n",
        "That, then would be a Deep Learning Network. **Deep Learning means nothing more than a Neural Network with multiple hidden layers,** in which data is summarized and analyzed and summarized and analyzed and so on. These pictures say it all:\n",
        "\n",
        "**A Simple Neural Network**\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/simplenn_regression.png\" width=\"350\">\n",
        "</div>\n",
        "\n",
        "**A Deep Neural Network**\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/deepnn_regression.png\" width=\"600\">\n",
        "</div>\n",
        "\n",
        "At the end of this module, you will be able to:\n",
        "\n",
        "* Configure 2 simple and 2 deep learning Networks for Regression and Classification\n",
        "* Describe how a Deep Neural Network works\n",
        "* Configure TensorFlow and Keras\n",
        "* Solve a simple Deep Learning problem\n",
        "* Compare regular Neural Network output with Deep Learning output\n",
        "\n",
        "To get started, please watch this instructor video:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "zHb5bE3czqhH",
        "outputId": "ae8fdfa8-3a4f-4a43-9642-3f781efe738d"
      },
      "source": [
        "from IPython.display import IFrame  # This is just for me so I can embed videos\n",
        "IFrame(src=\"https://www.youtube.com/embed/RkiTL_T8VsY\", width=560, height=315)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fd37990f610>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"560\"\n",
              "            height=\"315\"\n",
              "            src=\"https://www.youtube.com/embed/RkiTL_T8VsY\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzT4W7L3zg5E"
      },
      "source": [
        "#**What is Tensorflow?**\n",
        "Essentially, TensorFlow (a Google product) is an end-to-end open source machine learning **platform**. As a platform, it contains a number of libraries, or packages, the most well-known of which is Keras.\n",
        "\n",
        "The **GOAL** of TensorFlow is to train and run deep neural networks for handwritten digit classification, image recognition, word embeddings, recurrent neural networks, sequence-to-sequence models for machine translation, natural language processing, and PDE (partial differential equation) based simulations.\n",
        "\n",
        "##**Sooooo ... what is a Tensor?**\n",
        "The vocabulary here may sound technical and daunting, but there are a few very simple concepts hiding behind it. A Tensor is simply a multidimensional array:\n",
        "\n",
        "* Scalar = 0D Tensor\n",
        "* Vector = 1D Tensor\n",
        "* Matrix = 2D Tensor\n",
        "* Cube = 3D Tensor\n",
        "\n",
        "Thereby, we can deduce that a 4-D tensor is a vector of cubes, 5-D tensor is a matrix of cubes, 6-D tensor is a cube of cubes, etc. Take a look at the graphic below:\n",
        "\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/tensor.png\" width=\"600\">\n",
        "</div>\n",
        "\n",
        "##**And How does it Work?**\n",
        "Working with TensorFlow is  basically like setting up any other Classification:\n",
        "0. You set up your libraries and load your data\n",
        "1. You do your EDA (Exploratory Data Analysis) to see how the data is distributed and to determine what the class attribute in the dataset should be.\n",
        "2. Preprocess the data (remove n/a, transform data types as needed, deal with missing data) and THEN normalize the data so we can apply the model weights without problems.\n",
        "3. Split the data into a training set and a test set\n",
        "4. Build the model based on the training set\n",
        "5. Test the model on the test set\n",
        "6. Determine the quality of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x_5G4kA5BcJ"
      },
      "source": [
        "#**0. Preparation and Setup**\n",
        "To wrap our head around the process of setting up a Deep Learning model, we will work with a dataset with which we are already familiar: The adult dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUbUl6Mg5RH_"
      },
      "source": [
        "import tensorflow as tf # This tells Colab that we are using TensorFlow\n",
        "\n",
        "from tensorflow import keras # This is the main TensorFlow library\n",
        "from tensorflow.keras import layers # We are building a Neural Network with several hidden layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "print(\"Current TensorFlow version is\", tf.__version__)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # for visualization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)\n",
        "\n",
        "#Reading in the data as adult dataframe\n",
        "adult = pd.read_csv(\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/data/adult.data.simplified25.csv\")\n",
        "adult.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svkCY-1t54q2"
      },
      "source": [
        "#**1. Exploratory Data Analysis**\n",
        "How many rows does the adult dataset have? What are the attribute types? What is the mean, median, and mode of the incomeUSD attribute?\n",
        "\n",
        "These are all questions to solve. Use the code rows below to find the answers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TXu6Ud8y0Ro"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfx-mqha6c9z"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmMYxX7x6dzF"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwar0tYH0EAC"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYNfxVhB6efh"
      },
      "source": [
        "# **A2. Regression**\n",
        "As with the simple Neural Networks you have encountered already, preprocessing is a bit more involved than with, say, a Random Forest algorithm.\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/TF_Process1.png\" width=\"600\">\n",
        "</div>\n",
        "\n",
        "In our first case study, we will be working on Regression, which means we will be predicting ONE NUMBER, i.e. the expected IncomeUSD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uNkowDWq2ST"
      },
      "source": [
        "##**2.1 Reducing the Data**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFNTyz36EXjJ"
      },
      "source": [
        "##Your Turn\n",
        "You have done this before with the insurance dataset: Build an adult_dl dataset consisting of age, educationyears, race, hoursperweek, and incomeUSD as the class attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BM-pAfg72wF"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObVGq_Sg8C5N"
      },
      "source": [
        "##**2.2 Preparing the Data for use with TensorFlow**\n",
        "In this section, you will see that preparing data to work with a Deep Learning Neural Network requires the same kind of preprocessing that you have already encountered:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P5EQ9M1w_7j"
      },
      "source": [
        "### **2.2.1 Encoding Categorical Variables**\n",
        "Did you remember that Neural Networks (regular **and** in TensorFlow) require only numeric data? Well ... \"race\" is quite obviously categorical, so we need to convert it to one-hot format. We do this with pd.get_dummies()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsXwA597zKY4"
      },
      "source": [
        "adult_dl = pd.get_dummies(adult_dl, columns=['race'], prefix='', prefix_sep='')\n",
        "adult_dl.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEmkap4x2HKb"
      },
      "source": [
        "### **2.2.2 Splitting into Training and Test Set**\n",
        "We will do this first since we will want to reduce the amount of data that we will have to normalize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDuoUaeU2Tfm"
      },
      "source": [
        "train_dataset = adult_dl.sample(frac=0.8, random_state=0)\n",
        "test_dataset = adult_dl.drop(train_dataset.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PefhBuEj2nHW"
      },
      "source": [
        "### **2.2.3 Splitting Features from Labels**\n",
        "Separate the target value, the \"label\", from the features. This label is the value that you will train the model to predict--in our case, we want to predict incomeUSD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOaQmB3L2vKj"
      },
      "source": [
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "\n",
        "train_labels = train_features.pop('incomeUSD')\n",
        "test_labels = test_features.pop('incomeUSD')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXou63hL1Gg_"
      },
      "source": [
        "### **2.2.4 Normalizing**\n",
        "Except for age and educationyears, which are both measured in years, all other variables are measured in **different units**. This also puts them on **different scales**. Since, in a Neural Network, we need all our values on the **same scale**, so that the weights can be applied uniformly, we need to take action! One reason this is important is because the features are multiplied by the model weights. So the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.\n",
        "\n",
        "In the previous module, you've learned how mean-normalization with StandardScaler() works. This time, we are going to do this differently by setting up an actual layer INSIDE our neural network.\n",
        "\n",
        "The [**preprocessing.Normalization layer**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Normalization) is a clean and simple way to build this preprocessing into your model. And YES--by creating the normalization layer, you effectively just started building your TensorFlow model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3zzDTud1vxI"
      },
      "source": [
        "normalizer = preprocessing.Normalization(axis=-1) # axis=-1 means that normalization should be applied to the last axis (i.e., each feature independently)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1Mw1Ptl4KTp"
      },
      "source": [
        "Now we apply the normalizer to the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFxkYAQs4N6V"
      },
      "source": [
        "train_features = train_features.astype(np.float32) # Convert the data type from int to float32 so the normalizer can recognize them easily\n",
        "\n",
        "normalizer.adapt(train_features) # Adapt the normalizer to the train_features data\n",
        "\n",
        "normalized_features = normalizer(train_features) # Normalize the train_features\n",
        "\n",
        "first = np.array(train_features[:1]) # Get the first sample from train_features for demonstration\n",
        "\n",
        "with np.printoptions(precision=2, suppress=True): # Print the original and normalized data with specified precision\n",
        "    print('Original data:', first)\n",
        "    print()\n",
        "    print('Normalized data:', normalizer(first).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twvUHFO_4hC3"
      },
      "source": [
        "Above, you can see what happens inside the normalization layer: It returns the input data, with each feature independently normalized.\n",
        "\n",
        "###**WAIT! Some of the normalized data is negative. Is that correct?**\n",
        "Yes, it is correct for normalized data to sometimes have negative values. Here's why:\n",
        "\n",
        "When using the Normalization layer from TensorFlow (or other standardization techniques), the normalization process typically involves centering the data around a mean of zero and scaling it based on the standard deviation. This process is often referred to as Z-score normalization or standardization.\n",
        "\n",
        "**Explanation**\n",
        "\n",
        "The formula for standardization is:\n",
        "\n",
        "> standardized_value = (original_value − mean)/ standard_deviation\n",
        "\n",
        "* Mean (average): The center of the data.\n",
        "* Standard Deviation: How spread out the data is from the mean.\n",
        "\n",
        "By subtracting the mean from each data point and then dividing by the standard deviation, the resulting data will have:\n",
        "\n",
        "* A mean of 0.\n",
        "* A standard deviation of 1.\n",
        "\n",
        "This transformation means that data points that were below the mean in the original dataset will have negative standardized values, and those above the mean will have positive standardized values.\n",
        "<center>\n",
        "<img src = \"https://static9.depositphotos.com/1007989/1112/i/450/depositphotos_11129291-stock-illustration-nerdy-smiley.jpg\" height = 200>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS323LJk40_b"
      },
      "source": [
        "# **A3. Building the Models**\n",
        "There is always a specific process with which to build a TensorFlow model:\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/TF_Process2.png\" width=\"600\">\n",
        "</div>\n",
        "\n",
        "1. First, we set up the **keras SEQUENTIAL MODEL**. This is the framework inside of which we are going to define the layers. Sequential = layers are sequentially next to each other (either “stacked” or left-to-right, depending on how you draw them).\n",
        "---\n",
        "2. Inside the Sequential model, we define the **LAYERS**. To do this, we need to know the following:\n",
        "* **Shape**: This is the number of attributes we use as input for the model\n",
        "---\n",
        "3. In the next step, we define HOW we want the model to run, that is to **COMPILE**, with model.compile(). To do this, we need to know the following:\n",
        "* **Optimizer** = gradient descent function (i.e. which function we use to optimize the step-down of the weights); adam = adaptive learning rate optimization algorithm\n",
        "* **Loss Function**= evaluation of the ŷ vs the ground truth\n",
        "* **Metrics** = evaluation criterion, here accuracy.\n",
        "---\n",
        "4. Then, we **FIT** the model to the training set with model.fit(). To do this, we need to know the following:\n",
        "* **Epoch**: One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE. If one epoch is too big to feed to the computer at once we can divide it in several smaller batches\n",
        "* **Batch size**: Depending on the number of needed features in your dataset (you should reduce these to NO MORE THAN 6), the computing effort can be too intense. Just like you would not each a whole sandwich in one bite, the machine does better when processing the data in smaller bites called batches. The standard batch size is 32.\n",
        "---\n",
        "5. Lastly (and in purple!), we use our model to **PREDICT** the values for the test set with model.predict()\n",
        "---\n",
        "**How we choose the LOSS FUNCTION** for step 3 depends on the type of calculation we need our Neural Network to perform:\n",
        "* If the output variable is **continuous**, we are performing a regression, so the loss function is **mean squared error or MSE**\n",
        "* If the output variable is **binary**, we are performing a classification, so the loss function is **binary_crossentropy**\n",
        "* If the output variable is **categorical** with more than two labels, we are still performing a classification, but now the loss function is **categorical_crossentropy**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi1HyF7z44HR"
      },
      "source": [
        "##**3.1 REGRESSION: Using one Variable to predict incomeUSD in a Simple Neural Network**\n",
        "We will use age to predict incomeUSD. To do so, we will use the typical keras.Sequential model. This model represents a sequence of steps. In this case there are two steps:\n",
        "\n",
        "1. Normalize the input ['age'].\n",
        "2. Apply a linear transformation () to produce one output using layers.Dense.\n",
        "\n",
        "The number of inputs can either be set by the input_shape argument, or automatically when the model is run for the first time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-FrbA9NBRgK"
      },
      "source": [
        "# First, we build the normalization layer:\n",
        "age_new = np.array(train_features['age'])\n",
        "\n",
        "age_normalizer = preprocessing.Normalization(input_shape=[1,], axis=None)\n",
        "age_normalizer.adapt(age_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBgORl4A_M-3"
      },
      "source": [
        "# Now we build the framework that holds all the models:\n",
        "age_model = tf.keras.Sequential([\n",
        "    age_normalizer,\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "age_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc5_BniXDdSU"
      },
      "source": [
        "Do you see how the Output Shape contains 1? That's only one number in one layer, so a very basic Neural Network.\n",
        "\n",
        "Now we can configure the training procedure using the Model.compile() method. The most important arguments to compile are the loss and the optimizer since these define what will be optimized (mean_absolute_error) and how (using the [optimizers.Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zL3vNLLDiO3"
      },
      "source": [
        "age_model.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1AxoAvHDtqa"
      },
      "source": [
        "Once the model is configured, we use Model.fit() to train it over 100 epochs (give this about 1-2 minutes):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCFlvfrMDiWs"
      },
      "source": [
        "%%time\n",
        "# The %%time magic command measures and shows the time it takes to execute the entire cell. This is helpful for performance monitoring and optimization.\n",
        "history = age_model.fit(\n",
        "    train_features['age'], train_labels,\n",
        "    epochs=100,\n",
        "    # suppress logging\n",
        "    verbose=1,\n",
        "    # Calculate validation results on 20% of the training data. Validation means that we test as we go, on a 20% subset of the training data\n",
        "    validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output looks something like this:\n",
        "\n",
        "```\n",
        "Epoch 1/100\n",
        "500/500 [==============================] - 1s 2ms/step - loss: 56370.8867 - val_loss: 55092.4062\n",
        "Epoch 2/100\n",
        "500/500 [==============================] - 1s 2ms/step - loss: 55920.8945 - val_loss: 54642.5195\n",
        "Epoch 3/100\n",
        "500/500 [==============================] - 1s 2ms/step - loss: 55470.8594 - val_loss: 54192.6094\n",
        "Epoch 4/100\n",
        "500/500 [==============================] - 1s 2ms/step - loss: 55020.7891 - val_loss: 53742.7422\n",
        "```\n",
        "Here, you are seeing TWO loss outputs: loss and val_loss.\n",
        "\n",
        "In the output of a machine learning training process, loss and val_loss are key metrics used to monitor the model's performance.\n",
        "\n",
        "* **loss**: The training loss is the error calculated on the training dataset after each epoch. It shows how well the model is fitting the training data. This loss is minimized during the training process.\n",
        "* **val_loss**: The validation loss is the error calculated on the validation dataset after each epoch. It provides an estimate of the model's performance on *unseen data* (the validation set), which helps in detecting overfitting. Overfitting occurs when the model performs well on training data but poorly on validation data.\n",
        "\n",
        "###**But the numbers are really large!**\n",
        "\n",
        "Yes, they are. When the loss numbers are very large, as seen in our example, it typically indicates that the model's predictions are significantly off from the actual values. Here are some potential reasons and implications of large loss values:\n",
        "\n",
        "1. **Scale of Target Variable:**\n",
        "The target variable (output) might be on a very large scale. For example, if you are predicting monetary values that range in the tens of thousands, the loss values can naturally be large if they are not normalized or scaled appropriately.\n",
        "2. **Initial Model Parameters:** The initial weights of the model might be poorly initialized, leading to poor predictions in the initial epochs.\n",
        "3. **Learning Rate:** If the learning rate is too high, the model may be making large updates to the weights, which can result in large error values and unstable training.\n",
        "4. **Model Complexity:** The model might be too simple to capture the complexity of the data, resulting in poor predictions and thus high loss values.\n",
        "5. **Data Issues:** There could be outliers or errors in the data, which can cause high loss values. For example, extreme values in the target variable can skew the loss.\n",
        "6. **Inappropriate Loss Function:** Using a loss function that does not suit the problem can lead to high loss values. For example, using mean squared error (MSE) for a classification problem instead of cross-entropy loss.\n",
        "\n",
        "**HOWEVER**, despite the huge numbers, both loss and val_loss are decreasing, which is a good sign indicating that the model is learning and improving on both the training and validation datasets. Monitoring both loss and val_loss during training helps ensure that our model is learning properly and generalizing well to new data, which is critical for building robust machine learning models."
      ],
      "metadata": {
        "id": "LCkGHQrNVZue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features['age']"
      ],
      "metadata": {
        "id": "Y8ueGxNLwwiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eJIetubEUEY"
      },
      "source": [
        "Visualize the model's training results (you must run the code fields to see the graph):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIOA_tsOAS_W"
      },
      "source": [
        "x = tf.linspace(0.0, 250, 251)\n",
        "y = age_model.predict(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_IVJUMrAn0a"
      },
      "source": [
        "def plot_age(x, y):\n",
        "  plt.scatter(train_features['age'], train_labels, label='Data')\n",
        "  plt.plot(x, y, color='k', label='Predictions')\n",
        "  plt.xlabel('age')\n",
        "  plt.ylabel('incomeUSD')\n",
        "  plt.legend()\n",
        "plot_age(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-PrtMxmBUTj"
      },
      "source": [
        "##Your Turn\n",
        "\n",
        "Look at the scatter plot. What does the data distribution vs. the predictor line tell you? Write your answer in the field below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lOO05z3TiFZ1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc1f9q1dEDAV"
      },
      "source": [
        "Now let's test this on the test set and collect the results so we can inspect them at the end of this file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyAVqluWugku"
      },
      "source": [
        "test_results = {}\n",
        "\n",
        "test_results['age_model'] = age_model.evaluate(\n",
        "    test_features['age'],\n",
        "    test_labels, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGckOL4OuqAH"
      },
      "source": [
        "This is a regression with a single variable--way too simplistic for our purposes! On to bigger and better things!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyAIVER5vfNS"
      },
      "source": [
        "## **3.2 REGRESSION: Using Multiple Variables to Predict incomeUSD in a Simple Neural Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbt4A31Yvrv-"
      },
      "source": [
        "You can use an almost identical setup to make predictions based on multiple inputs. This model still does the same y - mx + b except that m is a matrix and b is a vector.\n",
        "\n",
        "This time, we use the Normalization layer that was adapted to the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqIVUbsKv7Yu"
      },
      "source": [
        "linear_model = tf.keras.Sequential([\n",
        "    normalizer,\n",
        "    layers.Dense(units=1)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baZH3g_CwK5K"
      },
      "source": [
        "When you call the model, its weight matrices will be built. You can see that the kernel (the m in y = mx + b) has a shape of (8,1). The shape (8, 1) suggests that the dense layer has 8 input features and 1 output unit. Each input feature has a corresponding weight, and there is one set of these weights for the single output unit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PstTgNeawWRU"
      },
      "source": [
        "linear_model.layers[1].kernel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diKM-dKTwdho"
      },
      "source": [
        "Now, we configure the model's runtime execution with the same compile and fit calls as for the single input age model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wO8dfkkwhvI"
      },
      "source": [
        "linear_model.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKKIMHS9wput"
      },
      "source": [
        "%%time\n",
        "history = linear_model.fit(\n",
        "    train_features, train_labels,\n",
        "    epochs=100,\n",
        "    # 100 epochs, so we turn on logging\n",
        "    verbose=1,\n",
        "    # Calculate validation results on 20% of the training data\n",
        "    validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0myiyL-ydWy"
      },
      "source": [
        "Just like before, we want to be sure the datatype is float32 and collect the results in the test_results variable again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FlEqQxSyh9v"
      },
      "source": [
        "test_features = test_features.astype(np.float32) # Ensuring that the datatype is float\n",
        "test_labels = test_labels.astype(np.float32)\n",
        "\n",
        "test_results['linear_model'] = linear_model.evaluate(\n",
        "      test_features, test_labels, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj1wYh_qlsdS"
      },
      "source": [
        "## Your Turn\n",
        "What is the command to display the current contents of the test_results variable? Type it below and inspect the results! What do you see?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2KvxzFJluL1"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KWik3H_ys0G"
      },
      "source": [
        "## **3.3 REGRESSION: Deep Neural Network (DNN)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fBO1pEHzCEQ"
      },
      "source": [
        "The previous section implemented linear models for single and multiple inputs in a **Simple Neural Network.**\n",
        "\n",
        "\n",
        "\n",
        "This section implements single-input and multiple-input DNN models. The code is basically the same except **the model is expanded to include some \"hidden\" non-linear layers**. The name \"hidden\" here just means not directly connected to the inputs or outputs.\n",
        "\n",
        "These models will contain a few more layers than the linear model:\n",
        "\n",
        "1. The normalization layer (imagine this as hidden layer 1 in the graphic below)\n",
        "2. Two hidden, nonlinear, Dense layers using the relu nonlinearity (hidden layers 2 and 3 in the graphic below)\n",
        "3. A linear single-output layer because we are calculating ONE regression output.\n",
        "\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/deepnn_regression.png\" width=\"600\">\n",
        "</div>\n",
        "\n",
        "Both will use the same training procedure so the compile method is included in the build_and_compile_model function below.\n",
        "\n",
        "###**Deep Neural Network Model Code Below**\n",
        "Here it is! NOTE that now, we are building and configuring the layers!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RUfA-v7zNCQ"
      },
      "source": [
        "def build_and_compile_model(norm):           # The input for build and compile is the norm\n",
        "  model = keras.Sequential([                 # Here you are setting up the keras.Sequential architecture\n",
        "      norm,                                  # This is the normalizer function we built before\n",
        "      layers.Dense(64, activation='relu'),   # Here is the first hidden layer--64 nodes, built with the relu function\n",
        "      layers.Dense(64, activation='relu'),   # Here is the second hidden layer, also built with the relu function\n",
        "      layers.Dense(1)                        # Here is the Dense layer for a single output because we are working on a REGRESSION\n",
        "  ])                                         # AND WE'RE DONE WITH BUILDING THE MODEL!\n",
        "\n",
        "  model.compile(loss='mean_absolute_error',   # Now we configure the runtime for our model; the loss function is mean absolute error--makes sense for a regression!\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNIsFYij0Kyd"
      },
      "source": [
        "### **3.3.1 REGRESSION--One Variable**\n",
        "As before, we use the age variable to predict incomeUSD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwTGLgmn0VLO"
      },
      "source": [
        "dnn_age_model = build_and_compile_model(age_normalizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZZKGCBX0d3U"
      },
      "source": [
        "This model has quite a few more trainable parameters than the linear models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQUyjuXU0e0_"
      },
      "source": [
        "dnn_age_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTVf1W8-0m7s"
      },
      "source": [
        "Now, we train the model with again 100 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdNzp5h50tMO"
      },
      "source": [
        "%%time\n",
        "history = dnn_age_model.fit(\n",
        "    train_features['age'], train_labels,\n",
        "    validation_split=0.2,\n",
        "    verbose=1, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels"
      ],
      "metadata": {
        "id": "taPL4VmJ_CwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiwtPeLJB7lQ"
      },
      "source": [
        "And we plot the outcomes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kuceR4fB-ig"
      },
      "source": [
        "x = tf.linspace(0.0, 250, 251)\n",
        "y = dnn_age_model.predict(x)\n",
        "\n",
        "plot_age(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7V2XJz7CMDO"
      },
      "source": [
        "Wow! Our model does a better job! But is it a good job? Not yet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUpPmDiK0-JQ"
      },
      "source": [
        "And we collect the test results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To9dY4na1Bbt"
      },
      "source": [
        "test_results['dnn_age_model'] = dnn_age_model.evaluate(\n",
        "    test_features['age'], test_labels,\n",
        "    verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-Gwu9xw1Qz7"
      },
      "source": [
        "### **3.3.2 REGRESSION: Multiple Variables**\n",
        "If you repeat this process using all the inputs it slightly improves the performance on the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVS07zOT1efm"
      },
      "source": [
        "dnn_model = build_and_compile_model(normalizer)\n",
        "dnn_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ob1LsbR1oZx"
      },
      "source": [
        "%%time\n",
        "history = dnn_model.fit(\n",
        "    train_features, train_labels,\n",
        "    validation_split=0.2,\n",
        "    verbose=1, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6T-OKdlCkU5"
      },
      "source": [
        "Let's see if our predictions come closer to the actual data distribution (run the code below to see the graph):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model.fit(\n",
        "    train_features, train_labels,\n",
        "    validation_split=0.2,\n",
        "    verbose=1, epochs=0)"
      ],
      "metadata": {
        "id": "QHlfTHh9u3Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj4ILEYiCplW"
      },
      "source": [
        "test_predictions = dnn_model.predict(test_features).flatten()\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [incomeUSD]')\n",
        "plt.ylabel('Predictions [incomeUSD]')\n",
        "lims = [0, 50]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims, lims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJXY2BDz5Qle"
      },
      "source": [
        "Now we can predict our test values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlEGGb5Z5Re0"
      },
      "source": [
        "test_predictions = dnn_model.predict(test_features).flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTOKbKxG128Z"
      },
      "source": [
        "As before, we collect the results on the test set in our test_results variable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6RZVlWq15-d"
      },
      "source": [
        "test_results['dnn_model'] = dnn_model.evaluate(test_features, test_labels, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irE3Vbjb2DD7"
      },
      "source": [
        "# **A4. Comparing all 4 models**\n",
        "Now that all the models are trained check the test-set performance and see how they did:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6t4bb2s5wtr"
      },
      "source": [
        "pd.DataFrame(test_results, index=['Mean absolute error [incomeUSD]']).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmV-zMhE2XUw"
      },
      "source": [
        "## Your Turn\n",
        "\n",
        "Which model performs the best? If you think about how each of the 4 models was built, why does your chosen model perform best?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0c5cHd9zEhQ"
      },
      "source": [
        "#**B2. Classification**\n",
        "In the previous section, you have learned how a Deep Learning Network works and how to build one that will predict one numeric value.\n",
        "\n",
        "**A Deep Neural Network--Regression**\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/deepnn_regression.png\" width=\"600\">\n",
        "</div>\n",
        "\n",
        "In this module, we will start with a Deep Learning NN for Classification purposes. Instead of producing ONE numeric value, we will be configuring it to yield categorical output. This means that you'll see as many nodes in the output layer as there are level to your target categories, like the two below.\n",
        "\n",
        "**A Deep Neural Network--Classification**\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/deepnn_classification2layers.png\" width=\"600\">\n",
        "</div>\n",
        "\n",
        "At the end of this section, you will be able to:\n",
        "\n",
        "* Configure a deep learning Classification Network\n",
        "* Distinguish the activation functions in the output layer for classifications from those for regression\n",
        "* Apply regular data classification techniques to image classification\n",
        "* Describe the special cases of Convolutional Neural Networks\n",
        "* Solve a simple classification problem\n",
        "\n",
        "To get started, please watch this great video that shows you where we are going with this (and if you don't remember the content from the instructor video any more, please [review that](https://youtu.be/RkiTL_T8VsY), as well):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "i4D1dLxTysG2",
        "outputId": "134254c9-9cfc-4632-b15b-b15f8251397e"
      },
      "source": [
        "from IPython.display import IFrame  # This is just for me so I can embed videos\n",
        "IFrame(src=\"https://www.youtube.com/embed/vF21cC-8G1U\", width=560, height=315)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fd37990f490>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"560\"\n",
              "            height=\"315\"\n",
              "            src=\"https://www.youtube.com/embed/vF21cC-8G1U\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWgxDP65GE8U"
      },
      "source": [
        "###**The Problem**\n",
        "In the previous section, we used a regression Deep Learning network to predict incomeUSD and age. We will start this section with predicting a categorical variable in our adult dataset: Race.\n",
        "\n",
        "You will see that setting up a Deep Learning model for a simple classification is very similar to building a regression model. In fact, the only difference, really, is in the output layer. For preprocessing, there is no difference from what we did before; in fact, in the previous model, we already prepared the \"race\" variable for the output layer by one-hot encoding it. Below is a summary of the code with comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAuzLW3mhcsH"
      },
      "source": [
        "#Reading in the data as a fresh adult dataframe\n",
        "adult = pd.read_csv(\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/data/adult.data.simplified25.csv\")\n",
        "adult.head()\n",
        "\n",
        "# Downsizing the Dataset to just the numeric attributes\n",
        "adult_dl = pd.DataFrame(adult, columns = ['age', 'educationyears', 'race','hoursperweek','incomeUSD'])\n",
        "\n",
        "# Splitting into Training and Test Set\n",
        "train_dataset = adult_dl.sample(frac=0.8, random_state=0)\n",
        "test_dataset = adult_dl.drop(train_dataset.index)\n",
        "\n",
        "# Splitting Features from Labels\n",
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "\n",
        "train_labels = train_features.pop('race')\n",
        "test_labels = test_features.pop('race')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2lu51Mekv6V"
      },
      "source": [
        "train_labels.head() # Let's see what the training labels look like now"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbG67ol9keV2"
      },
      "source": [
        "# Encoding the labels with pd.get_dummies\n",
        "train_labels1 = pd.get_dummies(train_labels, columns=['race'], prefix='', prefix_sep='')\n",
        "test_labels1 = pd.get_dummies(test_labels, columns=['race'], prefix='', prefix_sep='')\n",
        "\n",
        "# Normalizing the input variables\n",
        "normalizer = preprocessing.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(train_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlM8zPH4h9tf"
      },
      "source": [
        "train_labels1.head() # Let's see what the training labels look like encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inaLuLY-hJxH"
      },
      "source": [
        "#**B3. Build the Keras Model**\n",
        "Now, we can build the Sequential model and add layers one at a time until we are happy with our network architecture.\n",
        "\n",
        "* To build the **input layer**, we need to define the number of input features. We use the **input_dim** argument and set it to 4 for the 4 input variables ('age', 'educationyears','hoursperweek','incomeUSD').\n",
        "* The **output layer** will be our race attribute with 5 levels (Amer-Indian-Eskimo,\tAsian-Pac-Islander,\tBlack,\tOther,\tWhite)\n",
        "\n",
        "\n",
        "###**How do we know the number and architecture of layers in the middle?**\n",
        "\n",
        "The short answer is: We don't. The longer answer is: We experiment until we get the best output the fastest. The even longer answer is: We can use various optimization strategies that can help us out somewhat. So, let's assume that trial and error has shown us that three layers is optimal. Furthermore, let's assume that we are going to build a **Dense Network**, aka a **fully connected** network structure, in which every node is connected with every node in the next layer.\n",
        "\n",
        "To define this architecture, we use the Dense class. We will specify the number of neurons or nodes in the layer as the first argument, and set up the activation function with the activation argument.\n",
        "\n",
        "Speaking of **activation function**, we will use the **rectified linear unit** or ReLU activation function on the first two layers and the Softmax function in the output layer (if our output were between 0 and 1, we would use the Sigmoid function).\n",
        "\n",
        "###**Model Design**\n",
        "So, our model looks like this:\n",
        "\n",
        "* The model expects rows of data with 4 variables ('age', 'educationyears', 'hoursperweek', and 'incomeUSD' = the input_dim=4 argument)\n",
        "* The first hidden layer has 12 nodes and uses the relu activation function.\n",
        "* The second hidden layer has 8 nodes and uses the relu activation function.\n",
        "* The output layer has five nodes and uses the Softmax activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb1mgzo3ubLv"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=4, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY-qTO0r0NVO"
      },
      "source": [
        "#**B4. Compile the Keras Model**\n",
        "Now that the model is defined, we can compile it. To do so, we must specify\n",
        "* the **loss function** to use to evaluate a set of weights. In our case, we will use **categorical_crossentropy** because we have multiple nodes (categories) in the output layer.\n",
        "* the **optimizer** searches through different weights for the network and any optional metrics we would like to collect and report during training. In our case, we will define the optimizer as the efficient stochastic gradient descent algorithm “**adam**“. This is a popular version of gradient descent because it automatically tunes itself and gives good results in a wide range of problems.\n",
        "* Finally, because this is a classification problem, we will collect and report the **classification accuracy**, defined via the **metrics** argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDnw6qn1z6EO"
      },
      "source": [
        "# compile the Keras model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl1YNORN1mru"
      },
      "source": [
        "#**B5. Train the Model**\n",
        "Now that we have defined our model and compiled it, it is time to train the model on some data. We use the fit() function for this purpose. Training occurs over **epochs** and **each epoch is split into batches**.\n",
        "\n",
        "* **Epoch**: One pass through all of the rows in the training dataset. The training process will perform a set number of iterations through the dataset  that we must specify using the 'epochs' argument.\n",
        "\n",
        "* **Batch**: The number of dataset rows that are considered before the model weights are updated within each epoch. One epoch contains one or more batches, based on the defined 'batch_size' argument.\n",
        "\n",
        "For this problem, we will run for a small number of epochs (150) and use a relatively small batch size of 10.\n",
        "\n",
        "###**How do we know the number of epochs and the batch size?**\n",
        "Three words: Trial and error. Again. That's because we will be revising the model until we get the smallest loss function (aka the smallest error). Now, the model will always have **some** error, but the amount of error will level out after some point for a given model configuration. This is called model convergence.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUNgj2mZ3vUl"
      },
      "source": [
        "# fit the Keras model on the dataset\n",
        "model.fit(train_features, epochs=10, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd1m1CIV4u9-"
      },
      "source": [
        "#**B6. Evaluate the Model**\n",
        "We have trained our neural network and we can now evaluate the performance of the network on the test dataset. To evaluate your model on your training dataset, use the evaluate() function on your model and pass it the test data.\n",
        "\n",
        "This will generate a prediction for each input and output pair and collect scores, including the average loss and any metrics you have configured, such as accuracy.\n",
        "\n",
        "The evaluate() function will return a list with two values. The first will be the loss of the model on the dataset and the second will be the accuracy of the model on the dataset. We are only interested in reporting the accuracy, so we will ignore the loss value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WampDIhW5RpV"
      },
      "source": [
        "# evaluate the keras model\n",
        "loss, accuracy = model.evaluate(test_features, test_labels1)\n",
        "print('Accuracy: %.2f' % (accuracy*100),'%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opT9phxWAX6E"
      },
      "source": [
        "#Your Turn\n",
        "As you can tell, steps 5 and 6 above don't work (yet!), and there is also a CRITICAL error in building the Keras model in Section B3.\n",
        "\n",
        "This exercise tests your ability to **research** and **debug**. You already know a lot about running this same network on the same data, but as a regression problem, so NOW your job is to translate the regression problem into a classification problem for 'race'. All the building blocks you need are in this workbook.\n",
        "\n",
        "**Here is your job:**\n",
        "1. Fix the Keras model in section B3.\n",
        "2. Fix the code in section B5 so that the model will run\n",
        "4. Research how to use the predict() function to run the model on the test_features and test_labels1. Are the test_labels1 correctly encoded?\n",
        "\n",
        "Use the fields below to work on your code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Leb73W_BbhD"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3FOFCIbBbzS"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KAQCAxWBb-C"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoTgQPR62hlR"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q6RPwex77sa"
      },
      "source": [
        "# If You Get Stuck\n",
        "To help you get unstuck:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Task 2.1\n",
        "adult_dl = pd.DataFrame(adult, columns = ['age', 'educationyears', 'race','hoursperweek','incomeUSD'])\n",
        "adult_dl.head()\n",
        "```"
      ],
      "metadata": {
        "id": "jcBQL9IrtuN8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Task 3.2\n",
        "test_results\n",
        "\n",
        "or\n",
        "\n",
        "print(test_results)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Ufucj_I4uvHT"
      }
    }
  ]
}