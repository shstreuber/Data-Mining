{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"Module7_kNN_NaiveBayes.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"Bt6qwtGlIJ6G"},"source":["# **Module 7: CLASSIFICATION: k Nearest Neighbor and Naive Bayes**\n","This week, we will step into machine learning with the k Nearest Neighbor and Naive Bayes algorithms. At the end of this week you will be able to:\n","* Outline the concepts of Supervised Learning\n","* Explain what classification is\n","* Describe how k Nearest Neighbor works\n","* Describe how Naive Bayes works\n","* Write code to execute both, k Nearest Neighbor and Naive Bayes"]},{"cell_type":"markdown","metadata":{"id":"pkWv-q8OiWG6"},"source":["##**Supervised Learning**\n","This week, we are going to take the whole idea of forecasting a step further. With last week's logistic regression, we learned how to build a model that sorts data into 1 of two factors in a target attribute; in other words: A model with binary class outcome. This week, we are going to work with a categorical class attribute and two different machine learning mechanisms: The empirical classifier k Nearest Neighbor and the statistical classifier Naive Bayes.\n","\n","Both are part of **SUPERVISED LEARNING**. A supervised machine learning algorithm relies on labeled input data to learn a function that produces an appropriate output when given new unlabeled data. Imagine a computer is a child, we are its supervisor (e.g. parent, guardian, or teacher), and we want the child (computer) to learn what a book looks like. We will show the child several different pictures, some of which are books and the rest could be pictures of anything (cats, coffee cups, computers, etc).\n","When we see a book, we shout \"book!” When it’s not a pig, we shout “no, not book!” After doing this several times with the child, we show them a picture and ask “book?” and they will correctly (most of the time) say “book!” or “no, not book!” depending on what the picture is. That is supervised learning.\n","\n","Now, please watch the video below. It's a great introduction to Supervised Learning."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":335},"id":"Lj5-hiXkYzzI","executionInfo":{"status":"ok","timestamp":1624178467842,"user_tz":300,"elapsed":163,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"300cb7fe-ab64-435d-a8ff-12d77f16a26f"},"source":["from IPython.display import HTML # This is just for me so I can embed videos\n","from IPython.display import Image # This is just for me so I can embed images\n","HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/kE5QZ8G_78c\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/kE5QZ8G_78c\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"U0xAsvw1bDCH"},"source":["##**Classification**\n","Classification is the problem of identifying which of a set of categories an observation belongs to. For example: Classification helps us assign a given email to the \"spam\" or \"non-spam\" class, or a diagnosis to a given patient based on observed characteristics of the patient. \n","\n","In machine learning, classification has these steps:\n","1. Determine what the class attribute in the dataset should be. This will be the attribute you'll predict later on\n","2. Preprocess the data (remove n/a, transform data types as needed, deal with missing data) and ensure that the dependent attribute is CATEGORICAL\n","3. Split the data into a training set and a test set\n","4. Build the model based on the training set\n","5. Test the model on the test set and compare the calculated class values to the actual class values shown in the test set.\n","6. Determine the quality of the model\n","\n","Ready? Let's go.\n"]},{"cell_type":"markdown","metadata":{"id":"KdkVv9lncnIq"},"source":["#**0. Preparation and Setup**\n","For these explanations, we will need a model with a dependent attribute that is categorical. The typical explanation uses the famous [iris flower dataset](https://github.com/shstreuber/Data-Mining/blob/master/data/iris.csv), which even has [its own wikipedia page](https://en.wikipedia.org/wiki/Iris_flower_data_set). However, we will use the insurance dataset because it allows us to tackle actual real-world problems. Since we will be working with two different types of classification, the first one called k Nearest Neighbor, and the second on called Naive Bayes, we will import all the libraries upfront."]},{"cell_type":"code","metadata":{"id":"F_OxV6K4YV3B"},"source":["import numpy as np\n","import pandas as pd \n","import matplotlib.pyplot as plt\n","from scipy import spatial\n","import statsmodels.api as sm\n","\n","from IPython.display import HTML # This is just for me so I can embed videos\n","from IPython.display import Image # This is just for me so I can embed images\n","\n","#Reading in the data as insurance dataframe\n","insurance = pd.read_csv(\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/data/insurance_with_categories.csv\")\n","\n","#Verifying that we can see the data\n","insurance.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e_glAv3NIJ6h"},"source":["# **1. k Nearest Neighbor**\n","The concept of the k-nearest neighbor classifier is part of our everyday life: Imagine you meet a group of young, stylish, and sportive people. They talk about their friend Ben, who isn't with them. How do you imagine Ben? Young, stylish, and sportive? Bingo! Then you learn that Ben lives in a neighborhood where people have an average income above 200000 dollars a year and drive fancy European cars. Both his neighbors make even more than 300,000 dollars per year! Most likely, you imagine Ben to drive an Audi or a Porsche, as well, right?\n","\n","The principle behind nearest neighbor classification consists in finding a predefined number, i.e. the 'k' - of training samples closest in (Euclidian) distance to the new sample that we want to classify. The label of the new sample will be defined based on these neighbors. \n","\n","Would you like a more in-depth explanation? The video below gives you all the detail you will want to know as you proceed.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":335},"id":"IaUy_ZDBfXKP","executionInfo":{"status":"ok","timestamp":1624178469558,"user_tz":300,"elapsed":21,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"18b61af8-bd74-4941-b04c-53797a0a2efa"},"source":["HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/4HKqjENq9OU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/4HKqjENq9OU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"Ice_fMBvIJ6t"},"source":["##**1.1 Nearest Neighbor Algorithm**\n","So, in a nutshell, the k Nearest Neighbor Algorithm works like this:\n","1. Load the data\n","2. Initialize K to your chosen number of neighbors\n","3. For each example in the data: \n","  \n","  3.1 Calculate the distance between the query example and the current example from the data.\n","  \n","  3.2 Add the distance and the index of the example to an ordered collection\n","4. Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by the distances\n","5. Pick the first K entries from the sorted collection\n","6. Get the labels of the selected K entries\n","7. If classification, return the mode of the K labels\n","\n","To work with the k Nearest Neighbor algorithm, we use its library from the scikit learn package. We will also learn a new way to build training and test sets (with a process called cross-validation), so we are importing that package, too. Lastly, we will be generating \"pretty pictures\"--so, matplotlib is going to help us out with that.\n","\n"]},{"cell_type":"code","metadata":{"id":"Ds7QcWXVIJ7m"},"source":["# We import all the kNN libraries\n","\n","import matplotlib.patches as mpatches\n","import matplotlib.pyplot as plt\n","\n","from sklearn import neighbors, datasets\n","from sklearn.model_selection import cross_val_score, train_test_split\n","%matplotlib inline "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nxOsBveiIJ8r"},"source":["##**1.2 Exploratory Data Analysis**"]},{"cell_type":"markdown","metadata":{"id":"qdUIGgK4s7wz"},"source":["1. Let's investigate the features (= attributes or dimensions)"]},{"cell_type":"code","metadata":{"id":"B3jvuK-vIJ-_"},"source":["insurance.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4jh0Uka9kLO2"},"source":["We have 4 numeric attributes and 3 categorical ones. \n","\n","2. Let's rearrange the numeric features into a dataframe and use them to predict what region a person comes from."]},{"cell_type":"code","metadata":{"id":"wzYzLG4UlDvR"},"source":["insurance2 = pd.DataFrame(insurance, columns = ['age', 'bmi', 'children','charges','region'])\n","insurance2.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZH1a8QAYIKAV"},"source":["# Let's check what the levels of region are!\n","insurance2.region.unique()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wudd37G_nQOX"},"source":["And now for some fun stuff! Pandas has a great package that makes EDA much much easier: The pandas profiling package. It's not a standard package, so you'll need to install it with pip, then load it, and finally call it. Click around the report to learn more about the dataset we just built!"]},{"cell_type":"code","metadata":{"id":"fuJIng85nZ1p"},"source":["! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip # Installing the package first"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXJoeyJ6o_KR"},"source":["import pandas_profiling # Now we load the package\n","insurance2.profile_report() # And we call the package. This will take a moment. Prepare to be amazed!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UKl7xoQD3dKV"},"source":["##**1.3 Setting up Training and Test Sets**\n","To set up our training and test sets, we first split the independent variables (which we are assuming are age, bmi, children, and charges) and the class attribute (region), which contains the labels that we want to assign to the \"unknown\" data.\n","\n"]},{"cell_type":"code","metadata":{"id":"8PVH8GKCIKEP"},"source":["x=insurance2.iloc[:,:4] # all parameters\n","y=insurance2['region'] # class labels 'southwest', 'southeast', 'northwest', 'northeast'\n","\n","#print(x) # Uncomment this line to verify your parameters/ independent variables/ attributes/ features\n","#print(y) # Uncomment this line to verify your class labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nuzsSUvVHJsM"},"source":["Now that we have separated the x attributes (independent variables) and the y attribute (class attribute, dependent variable), we build our training and test sets! \n","\n","NOTE that we are not allocating any sizes for the train_test_split below. This will invoke the default, which is a 75% training/ 25% test split."]},{"cell_type":"code","metadata":{"id":"X0xJHSiTIKQa"},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(x, y, random_state = 0)\n","\n","# So, what training data do we have?\n","print(\"X_train shape: {}\".format(X_train.shape))\n","print(\"y_train shape: {}\".format(y_train.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VdFwC9M86UqH"},"source":["##Your Turn\n","How many rows and columns do you have in the test data set? Write the command below and run it!"]},{"cell_type":"code","metadata":{"id":"-_xfs9j-IKRH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vMdKX2sRIKPZ"},"source":["##**1.4 Building the Simplest Model with k=1**\n","\n","Remember that, in kNN classification, the output is class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n","\n","We'll try that out first.[link text](https://)"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Q-QH9fAgIKRv"},"source":["# This is the model with the one nearest neighbor. Note the output shows that the metric is set to minkowski, \n","# but the p is set to 2, which means that we are using Euclidian distance (1 = Manhattan distance)\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","model1 = KNeighborsClassifier(n_neighbors = 1)\n","model1.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a7a73cpU_iQL"},"source":["###**1.4.1 Testing the Model**\n","\n","As you can see, we have built model1, which is the kNN model with just 1 nearest neighbor. Next, we test it.\n","\n","We use y_pred to store the calculated y values (remember y hat?) that the model gives us. Then we can compare them with the actual y values that we know and see what percentage the model identified correctly."]},{"cell_type":"code","metadata":{"id":"fXHRpANIIKSd"},"source":["y_pred = model1.predict(X_test)\n","print(\"Test set predictions: \\n {}\".format(y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D5zUjMFvOcK9"},"source":["###**1.4.2 Evaluating the model**\n","Here, we use three critical methods to get an idea of how \"good\" our model really is.\n"]},{"cell_type":"markdown","metadata":{"id":"PPQZ0nMBJ2DQ"},"source":["####1.4.2.1 The Accuracy Score"]},{"cell_type":"code","metadata":{"id":"Z6gCBHPUIKTS"},"source":["# Accuracy score\n","print(\"Test set score: {:.2f}\".format(model1.score(X_test, y_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NxZ8G0g0KL8o"},"source":["Oh boy! That number could be better. Let's see if the other methods indicate the same thing."]},{"cell_type":"markdown","metadata":{"id":"25XhYJpOX1dq"},"source":["####1.4.2.2 Data Inspection Calculated vs. Actual values"]},{"cell_type":"code","metadata":{"id":"HbvxJdDGJ6Do"},"source":["# Let's compare the actual y and the predicted y\n","\n","realvsmodel1 = pd.DataFrame(y_pred,y_test)\n","realvsmodel1 = pd.DataFrame({'predicted':y_pred,'original':y_test})\n","realvsmodel1.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vcu5LeZcKUse"},"source":["Well, that doesn't look too exciting, does it? Let's go to the last and MOST IMPORTANT way of evaluating the quality of our model: **The Confusion Matrix.** "]},{"cell_type":"markdown","metadata":{"id":"JhN3LKPrMAKc"},"source":["####1.4.2.3 The Confusion Matrix\n","\n","A confusion matrix compares the calculated or predicted values for all labels in the class attribute with the actual, true values that we know. In other words, we check which true values were predicted correctly and which were predicted incorrecty. For this purpose, we will use the plot_confusion_matrix group from the scikit-learn.metrics library.\n","\n","A longer, more mathematical, explanation is [here](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)."]},{"cell_type":"code","metadata":{"id":"nw-3P8PeZRtN"},"source":["from sklearn.metrics import plot_confusion_matrix\n","plot_confusion_matrix(model1, X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DXbUuwkANB42"},"source":["Alrighty. What are we seeing here? \n","\n","The true values appear in rows (label on the left side, down), the predicted values appear in columns (label across the top). Here is how to explain **the first row**: \n","* All the items should have been in the northeast region because the True Label is 'northeast'. That is clearly not the case; otherwise, we would see 96 in the green box in the upper left and zeros across the rest of the row.\n","* Of the actual true 'northeast' region, 35 were predicted correctly (green); 18 were incorrectly predicted as 'northwest', 17 were incorrectly predicted as 'southeast', and 26 were incorrectly predicted as 'southwest'. So, out of 96 actual 'northeast' rows, only 35 were predicted correctly. 18+17+26 (= 61) were predicted incorrectly."]},{"cell_type":"markdown","metadata":{"id":"Kv0cINFSOwSp"},"source":["###1.4.2.4 The Classification Report\n","\n","The Classification Report gives us even more insights into how well (or, in our case, badly) our model performs. To read it correctly, we first have to define a few terms:\n","1. **precision** (also called positive predictive value) is the number of correctly identified positive results divided by the number of all positive results, including those not identified correctly ((true positives) / (true positives + false positives)). Said another way, “for all instances classified positive, what percent was correct?”\n","2. **recall** (also known as sensitivity) is the number of correctly identified positive results divided by the number of all samples that should have been identified as positive ((true positives) / (true positives + false negatives)). Said another way, “for all instances that were actually positive, what percent was classified correctly?\n","3. **f-1 score** is the harmonic mean of the precision and recall. The highest possible value of F1 is 1, indicating perfect precision and recall, and the lowest possible value is 0, if either the precision or the recall is zero. As a rule of thumb, the weighted average of F1 should be used to compare classifier models, not global accuracy.\n","3. **support** is the number of actual occurrences of the class in the specified dataset."]},{"cell_type":"code","metadata":{"id":"C0S9XzqdM_1u"},"source":["from sklearn.metrics import classification_report\n","print(classification_report(y_test, y_pred, labels=['southwest', 'southeast', 'northwest', 'northeast']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G6yUWu2GPzeK"},"source":["Look at these values (note that \"support\" is the number of all true label quantities)--would you accept this quality from, say, a dentist? Or from your car, which (in the case of southwest), engages the brakes in 18% of all cases when you step on the pedal? \n","\n","Yeah, I thought so, too. Not good."]},{"cell_type":"markdown","metadata":{"id":"ffwJ5JBqIKUD"},"source":["## **1.5 Building kNN with 5 nearest neighbors (and a 2/3 to 1/3 train/ test split)**\n","Given our less-than fabulous results above, let's see if instead of assigning the class label from only 1 nearest neighbor, we can increase the accuracy of our predictions by looking at the class labels for the 5 nearest neighbors!"]},{"cell_type":"markdown","metadata":{"id":"Ft7hoUyhRMIb"},"source":["###1.5.1 Setting up Training and Test Set\n","Note how here, we use the test_size parameter to split 2/3 of the data into the training set and 1/3 into the test set."]},{"cell_type":"code","metadata":{"id":"vcy-VXSTIKUK"},"source":["X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.33)\n","\n","print(\"X_train shape: {}\".format(X_train.shape))\n","print(\"y_train shape: {}\".format(y_train.shape))\n","print(\"X_test shape: {}\".format(X_test.shape))\n","print(\"y_test shape: {}\".format(y_test.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nfpXDfqyRcO7"},"source":["###1.5.2 Building the model with k=5"]},{"cell_type":"code","metadata":{"id":"Q1qY2KmcIKU7"},"source":["model5 = neighbors.KNeighborsClassifier(n_neighbors=5)\n","\n","model5.fit(X_train, y_train)\n","\n","y_pred = model5.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gGCYWn-aVSc6"},"source":["Alright, we've got the model built. Everyone ready for the Confusion Matrix and the Classification Report?"]},{"cell_type":"code","metadata":{"id":"VbgDfHDIVher"},"source":["plot_confusion_matrix(model5, X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1XKnQIxuVroI"},"source":["print(classification_report(y_test, y_pred, labels=['southwest', 'southeast', 'northwest', 'northeast']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J6BE4vI1IKVo"},"source":["Wait, seriously? Why are we getting a result that is only a little bit better, but we're spending much more processing effort? There's got to be a way to find the optimal number for k!\n","\n","And there is."]},{"cell_type":"markdown","metadata":{"id":"4D_fXddDSLUq"},"source":["##**1.6 Optimizing k with Cross-Validation**\n","We could spend entire days re-running the kNN and increasing k by 1 until we've found the best value for k. But that would be 1. boring, 2. too much work, 3. not efficient, given that we could instead just cycle through a list of values until we've found the best one. \n","\n","To achieve this in sa most efficient way, we will need to use another trick (aka a preprocessing method) that we haven't encountered yet: **Cross-validation.** Find out [in this detail description](https://machinelearningmastery.com/k-fold-cross-validation/) how cross-validation works. [This graphic](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png) will also help you understand. Or watch this 5-minute video:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":335},"id":"YQXEtCxrZzkp","executionInfo":{"status":"ok","timestamp":1624178502242,"user_tz":300,"elapsed":42,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"6434451d-ce3f-4f6f-8486-c51d0b038c43"},"source":["HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/fSytzGwwBVw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/fSytzGwwBVw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"7c6x3eH_aUYV"},"source":["This what we'll do now. Let's go!\n","\n","First, we build a list of potential k values. Then we create an empty list that will hold cross-validation scores."]},{"cell_type":"code","metadata":{"id":"5t0e0_mVIKVt"},"source":["# To determine how to pick k, we are first creating a list of potential k values\n","klist = list(range(1,50,2)) # Our list goes from 1 to 50 in increments of 2\n","\n","# Then we create an empty list that will hold cross-validation scores\n","cv_scores = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NnoHxO2uatiV"},"source":["Now we can build our cross-validation. We will cycle through our k values from the k-value list and store the accuracy scores in the cv_scores list. To make things easier on us, we will convert the accuracy score into its opposite--the misclassification error. This misclassification error is really the average of all the misclassifications for one run of k."]},{"cell_type":"code","metadata":{"id":"dzqf4YaFaxOv"},"source":["# Perform 10-fold cross validation for each k value (we have a small dataset, so we can do this)\n","for k in klist:\n","    model10 = neighbors.KNeighborsClassifier(n_neighbors=k)\n","    scores = cross_val_score(model10, x, y, cv=10, scoring='accuracy')\n","    cv_scores.append(scores.mean())\n","\n","# Changing to misclassification error\n","errors = 1- np.array(cv_scores)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KUDbFgn3bIbf"},"source":["NOW we can use the error number to determine the optimal k! To do so, we look at our errors and pick the row with the k value that produced the smallest error. \n","\n","To make things easier to understand, we plot the misclassification errors in comparison to k so we can see our results."]},{"cell_type":"code","metadata":{"id":"cIRA1hbGbJRp"},"source":["optimal_k = klist[np.argmin(errors)]\n","print(\"The optimal number of neighbors is {}\".format(optimal_k))\n","\n","# plot misclassification error vs k\n","plt.plot(klist, errors)\n","plt.xlabel('Number of Neighbors K')\n","plt.ylabel('Misclassification Error')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Ho8UgYSStbv"},"source":["##Your Turn\n","Re-run this model with k=10 and k=50; for each of these, build and run a confusion matrix and a classification report. What changes? What do your results say about the data?\n","\n","Use the lines below for your code"]},{"cell_type":"code","metadata":{"id":"2pK5Kjhuc6q5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hd1cXY_0c235"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r9YkRp3gIKig"},"source":["# **2. Naive Bayes**\n","Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. \n","\n","Here is a [great explanation](https://www.datacamp.com/community/tutorials/naive-bayes-scikit-learn) of the principle behind the Bayes Theorem. Read the webpage so you understand what we're up to--if you skip it, nothing below will make sense to you.\n","\n","**WAIT!**\n","Read that first paragraph again. Then go all the way back to the Exploratory Data Analysis (EDA) section of this file. Look closely at the output of the pandas_profile analysis. Read the first paragraph another time. \n"]},{"cell_type":"markdown","metadata":{"id":"4r8HEjAvrLhI"},"source":["##Your Turn\n","Given the results from the EDA, should we conduct a Naive Bayes Analysis, at all? What condition does the insurance2 dataset violate? Type your answer below.\n"]},{"cell_type":"markdown","metadata":{"id":"4UwWwpierHF1"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"QXTUHXNvnpG7"},"source":["##**2.1 Setting up the Environment**\n","We will be working with Scikit-Learn again. More specifically, we will be working with the Gaussian Naive Bayes algorithm. Are there other Naive Bayes algorithms? [Absolutely](https://scikit-learn.org/stable/modules/naive_bayes.html). "]},{"cell_type":"code","metadata":{"id":"Cx7DPgs_IKjm"},"source":["from sklearn.naive_bayes import GaussianNB"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"u0BzHbFflwzU","executionInfo":{"status":"ok","timestamp":1624178650730,"user_tz":300,"elapsed":120,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"8a25e0e6-fefc-4d77-a2fb-14d588de3b3e"},"source":["#Let's verify that the dataset is still what it needs to be:\n","insurance2.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>bmi</th>\n","      <th>children</th>\n","      <th>charges</th>\n","      <th>region</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>19</td>\n","      <td>27.900</td>\n","      <td>0</td>\n","      <td>16884.92400</td>\n","      <td>southwest</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>18</td>\n","      <td>33.770</td>\n","      <td>1</td>\n","      <td>1725.55230</td>\n","      <td>southeast</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>28</td>\n","      <td>33.000</td>\n","      <td>3</td>\n","      <td>4449.46200</td>\n","      <td>southeast</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>33</td>\n","      <td>22.705</td>\n","      <td>0</td>\n","      <td>21984.47061</td>\n","      <td>northwest</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>32</td>\n","      <td>28.880</td>\n","      <td>0</td>\n","      <td>3866.85520</td>\n","      <td>northwest</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   age     bmi  children      charges     region\n","0   19  27.900         0  16884.92400  southwest\n","1   18  33.770         1   1725.55230  southeast\n","2   28  33.000         3   4449.46200  southeast\n","3   33  22.705         0  21984.47061  northwest\n","4   32  28.880         0   3866.85520  northwest"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A5FZEpfBq6f-","executionInfo":{"status":"ok","timestamp":1624180512076,"user_tz":300,"elapsed":119,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"0bce0be3-4a76-4da1-c38a-5434be3da7b3"},"source":["insurance2.dtypes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["age           int64\n","bmi         float64\n","children      int64\n","charges     float64\n","region       object\n","dtype: object"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"bQcXqsp6IKlR"},"source":["##**2.2 Setting up the training and test sets**"]},{"cell_type":"code","metadata":{"id":"-IC9jLLNIKlb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624180628437,"user_tz":300,"elapsed":134,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"ece512da-f2b5-4381-87ea-749df2876c76"},"source":["ins_train, ins_test = train_test_split(insurance2, test_size = 0.2)\n","print(ins_train)\n","print(ins_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["      age     bmi  children      charges     region\n","446    60  29.640         0  12730.99960  northeast\n","447    56  25.650         0  11454.02150  northwest\n","1112   48  25.850         3  24180.93350  southeast\n","42     41  21.780         1   6272.47720  southeast\n","610    47  29.370         1   8547.69130  southeast\n","...   ...     ...       ...          ...        ...\n","481    49  37.510         2   9304.70190  southeast\n","327    45  36.480         2  42760.50220  northwest\n","657    27  33.155         2   4058.71245  northwest\n","542    63  36.300         0  13887.20400  southeast\n","795    27  28.500         0  18310.74200  northwest\n","\n","[1070 rows x 5 columns]\n","      age     bmi  children      charges     region\n","99     38  19.300         0  15820.69900  southwest\n","35     19  20.425         0   1625.43375  northwest\n","1227   42  37.180         2   7162.01220  southeast\n","627    33  42.460         1  11326.71487  southeast\n","148    53  37.430         1  10959.69470  northwest\n","...   ...     ...       ...          ...        ...\n","214    45  30.900         2   8520.02600  southwest\n","1122   53  36.860         3  46661.44240  northwest\n","897    19  25.555         1   2221.56445  northwest\n","72     53  28.100         3  11741.72600  southwest\n","660    37  46.530         3   6435.62370  southeast\n","\n","[268 rows x 5 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iBEgLbJqt6nX"},"source":["ins_train_np = np.array([ins_train])\n","ins_test_np = np.array([ins_test])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7h8x3z3sjrn9"},"source":["##**2.3 Building the model with GaussianNB()**"]},{"cell_type":"code","metadata":{"id":"tdkRBdqljpcy"},"source":["ins_naivebayes = GaussianNB()\n","ins_naivebayes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8hq_3Bkpqj4I"},"source":["ins_naivebayes.fit(ins_train[['southwest', 'southeast', 'northwest', 'northeast']], ins_train['region'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_gdUlh18jy8d"},"source":["##**2.4 Testing the model and calculating the accuracy score**"]},{"cell_type":"code","metadata":{"id":"78LcEwVPIKmj"},"source":["ins_predictions = ins_naivebayes.predict(iris_test[['southwest', 'southeast', 'northwest', 'northeast']])\n","accuracy_score(ins_test['region'], ins_predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g7_xFhTIIKnm"},"source":["##**2.5 Comparing the predicted and the original values**"]},{"cell_type":"code","metadata":{"id":"VrWTCzRQkPAE"},"source":["realvsmodel2 = pd.DataFrame(ins_predictions,ins_test)\n","realvsmodel2 = pd.DataFrame({'predicted':ins_predictions,'original':ins_test['region']})\n","realvsmodel2.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HEEX_roRTRU-"},"source":["##Your Turn\n","Plot a confusion matrix. For your code, refer to secion 1.4.2.3 above:"]},{"cell_type":"code","metadata":{"id":"r4fD9ZZxnSSR"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PiLLUOJKnTGu"},"source":["## Your Turn\n","Build a classification report. For your code, refer to section 1.4.2.4 above:"]},{"cell_type":"code","metadata":{"id":"-6iQSgRjndmw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iIWl6D3vneew"},"source":["##Your Turn\n","Compare the results from the confusion matrix and the classification report for k Nearest Neighbor and Naive Bayes. Which model produces better results? Write your answer into the text field below."]},{"cell_type":"markdown","metadata":{"id":"MU-nAxchn3YW"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"g22TRE4mTSLE"},"source":["##Your Turn\n","How valid are the results for the Naive Bayes classification really? Review the assumptions for work with Naive Bayes, especially regarding dependency among independent attribtues, and then look again at the results of the pandas_profiling output above. Write your answer into the text field below.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hCiVwilzrles"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"IVMlu10DoYof"},"source":["##Your Turn\n","So ... given the data that we have, can we reliably predict the region someone lives in based on their age, bmi, number of children, and the $ amount of their insurance claims? Explain in the field below."]},{"cell_type":"markdown","metadata":{"id":"uU1Up9ujr8jh"},"source":[""]}]}