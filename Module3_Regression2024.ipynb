{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "BFixOSg8PuwT"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shstreuber/Data-Mining/blob/master/Module3_Regression2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQYet9gT5E7c"
      },
      "source": [
        "# **Module 3: Regression**\n",
        "In our last module, we saw that we can numerically relate two variables--either by evaluating how and when they vary together (covariance) or how variation in one variable leads to variation in another (correlation). It's now time for us to start thinking about how to determine more than just the mere **existence** of a relationship--we want to determine exactly **how** two variables relate to one another.\n",
        "\n",
        "In this module, you will learn\n",
        "* How to set up a simple linear regression\n",
        "* How to set up a multiple regression\n",
        "* How to set up a logistic regression\n",
        "* How to interpret the output from a regression\n",
        "\n",
        "**Be sure to expand all the hidden cells, run all the code, and do all the exercises--you will need the techniques for the lesson lab!**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**What is Regression?**\n",
        "Regression is one of the most valuable strategies in a data analyst's toolbox. It's widely used in the industry because it's quick to compute (unlike later models in this course) and therefore efficient on computing resources.\n",
        "\n",
        "And it's not as hard as it may seem.\n",
        "\n",
        "Did you know that there are at least **SEVEN most common types of regression**? Yes, seven.\n",
        "\n",
        "<img src=\"https://theaxisofego.com/wp-content/uploads/2012/02/sevendwarfs.jpg?w=288&h=275\">\n",
        "\n",
        "As you read [the blog post under this link](https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/), focus on the **linear** and the **logistic** kind--that's what we will use this week.\n",
        "\n"
      ],
      "metadata": {
        "id": "B0hDiFVJfW6s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BFlIE4kcf5J"
      },
      "source": [
        "#**0. Preparation and Setup**\n",
        "We are working with our adult dataset again, so we're loading our libraries and our dataset just like last time, only that we won't need chisquare. This time around, we will use the following to allow us to use more statistics:\n",
        "* **scipy.stats** -- all sorts of [statistical functions](https://docs.scipy.org/doc/scipy/reference/stats.html) that could potentially be useful\n",
        "* **statsmodels** -- this is your typical [Python statistics package](https://www.statsmodels.org/stable/index.html). It is used for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration.\n",
        "* **ols** -- this is the Ordinary Least Squares function within statsmodels. It is located in the [formula class](https://www.statsmodels.org/stable/api.html#statsmodels-formula-api) and will help us set up our regression models.\n",
        "* **Scikit Learn** -- we already met [this package](https://scikit-learn.org/) in this course. It is one of the most popular machine learning packages.\n",
        "\n",
        "**The Dataset**\n",
        "\n",
        "For this purpose, we will use the insurance prediction dataset. This dataset exists in TWO forms; the first one is the original, with [numeric and categorical attributes mixed](https://raw.githubusercontent.com/shstreuber/Data-Mining/master/data/insurance_with_categories.csv). This dataset contains 1338 observations (rows) and 7 features (columns). The dataset contains 4 numerical features (age, bmi, children and charges) and 3 categorical features (sex, smoker and region). Later, we will encounter another version of this dataset where [the categories are converted](https://raw.githubusercontent.com/shstreuber/Data-Mining/master/data/insurance.csv) into factors with numerical value designated for each level. Lastly, the **dependent attribute**, i.e. the **class attribute** in this datasetis charges, which tells us how high a filed claim was.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ot9JXypGcwA-"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import spatial\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "from IPython.display import IFrame  # This is just for me so I can embed videos\n",
        "\n",
        "#Reading in the data as adult dataframe\n",
        "insurance = pd.read_csv(\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/data/insurance_with_categories.csv\")\n",
        "\n",
        "#Verifying that we can see the data\n",
        "insurance.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07GiJ04g5E-S"
      },
      "source": [
        "# And let's verify the datatypes before we go off into analysis land:\n",
        "insurance.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX7r0GePHlYm"
      },
      "source": [
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_jgLSAL5E_s"
      },
      "source": [
        "#**1. Exploratory Data Analysis**\n",
        " Let's get a sense of how some of the independent numeric variables relate to our class attribute (i.e. the one attribute most likely to change when the independent attributes change)--in this case, our **class attribute** is charges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzcmBQl85FAQ"
      },
      "source": [
        "plt.scatter(insurance['age'], insurance['charges'], alpha=0.5) # alpha=0.5 is added to make the points slightly transparent, making it easier to see overlapping points.\n",
        "plt.title('Scatter plot age vs. charges')\n",
        "plt.xlabel('age')\n",
        "plt.ylabel('charges')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw4CT3noAEps"
      },
      "source": [
        "Hm. Interesting. We see three somewhat defined bands here. The solid band at the bottom, followed by a less solid band starting between 10,000 and 20,000, and then, there's the band starting between 30 and 40k. So, the older a person gets, the more likely they are to incur higher insurance claims. Given what we know about getting older, that seems about right.\n",
        "\n",
        "But what about the dependency of [bmi](https://www.nhlbi.nih.gov/health/educational/lose_wt/BMI/bmicalc.htm) and charges? Might it be reasonable to assume that higher bmi = higher charges?  Well, let's see ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZwHyoNS5E_w"
      },
      "source": [
        "plt.scatter(insurance['bmi'], insurance['charges'], alpha=0.5)\n",
        "plt.title('Scatter plot bmi vs. charges')\n",
        "plt.xlabel('bmi')\n",
        "plt.ylabel('charges')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs9kyHSHA0hI"
      },
      "source": [
        "**SURPRISE!** Higher bmi does not always mean higher insurance charges! We see plenty of data in the 40+ ranges that seem to have identical charges to data in the 20 to 25 range. HOWEVER, we also see more data breaking the 30,000 barrier starting at a bmi of 30+. So ... two relationships, really?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hJsveUVXPnP"
      },
      "source": [
        "Next, let's check the correlations between all the attributes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJWVCI6AXTsM"
      },
      "source": [
        "insurance.corr(numeric_only=True) # Need to specify numeric_only=True because the default has changed with version 2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRnvaBoVXdUm"
      },
      "source": [
        "## Your Turn\n",
        "Look at the table above. Which attribute has the best correlation to charges? **Display just this attribute** in this field:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfuImXt2Xxnw"
      },
      "source": [],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddpDPqq0CaOV"
      },
      "source": [
        "##Your Turn\n",
        "Now, does having more children mean higher charges? Let's see!\n",
        "\n",
        "Use the field below to **build a scatterplot of children and charges**! What does the relationship reveal?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQUujPm35FA4"
      },
      "source": [],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjtNo08E5FBY"
      },
      "source": [
        "Really think about this--wouldn't it be interesting if we could find the rule that connects these variables?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuxMm6CI5FDZ"
      },
      "source": [
        "# **2. Simple Regression**\n",
        "In the previous module, we saw how Correlation and the Pearson Correlation Coefficient, r, give us a first insight into the relationship between two variables. But that is only 1 number which doesn't tell us anything about the relationship of each single datapoint with the next--and it certainly doesn't allow us to predict any future values.\n",
        "\n",
        "That is why Regression is a better aproach.\n",
        "\n",
        "To review the **concepts and vocabulary** of simple regression, watch this video:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "QD1bgPR5FMX7",
        "outputId": "ff43f3ea-273f-4622-f4c4-6ffa4f2f5b26"
      },
      "source": [
        "IFrame(src=\"https://www.youtube.com/embed/aq8VU5KLmkY\", width=560, height=315)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7889aa794eb0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"560\"\n",
              "            height=\"315\"\n",
              "            src=\"https://www.youtube.com/embed/aq8VU5KLmkY\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlLQYVED-rAr"
      },
      "source": [
        "\n",
        "##**2.1 Simple Linear Regression with statsmodels**\n",
        "The simplest regression comes in the form of\n",
        "\n",
        "                    `y = f(x) + b`\n",
        "\n",
        "where x is the independent variable and y the dependent variable (aka the class attribute). The idea is that you can find a coefficient for x (here named f) that, when paired with a constant value (here b), will allow you to calculate any dependent value y. Since there is no exponential math going on here, the result will be a straight line. That's also why we call this \"linear regression.\"\n",
        "\n",
        "So, how does simple linear regression work in Python? Well, you've already taken the first steps by plotting the independent variable x on the dependent variable y above. The rest is explained in the instructor video below:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IFrame(src=\"https://www.youtube.com/embed/jC1z8pIAsp8\", width=560, height=315)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "oY0v5T7blchP",
        "outputId": "5c770e62-1a83-42f8-a1db-30d012cc34b0"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7889a6b92e90>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"560\"\n",
              "            height=\"315\"\n",
              "            src=\"https://www.youtube.com/embed/jC1z8pIAsp8\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxuPJDEhRQuM"
      },
      "source": [
        "###**2.1.1 Simple Linear Regression with statsmodels**\n",
        "Below, we will use Ordinary Least Squares to calculate our regression equation. As the video above explains, Ordinary Least Squares, or linear least squares, estimates the parameters in a regression model by minimizing the sum of the squared residuals. This method draws a line through the data points that minimizes the sum of the squared differences between the observed values and the corresponding fitted values.\n",
        "\n",
        "**If you can't remember what residuals are**, check out [this blog post](https://www.statisticshowto.com/residual/).\n",
        "\n",
        "So, let's go back to age and insurance charges! Note that, in order to format our output as\n",
        "\n",
        "                          `y = f(x) + b`\n",
        "\n",
        "We need to make sure that the calculation considers the constant b. That's why we use add_constant below. Otherwise, all we would get as output is the coefficient f."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4RVz3U4O5FDb"
      },
      "source": [
        "X = insurance['age']\n",
        "y = insurance['charges']\n",
        "X = sm.add_constant(X)\n",
        "insurance.mod1 = sm.OLS(y, X).fit()  ## sm.OLS(output, i.e. dependent variable, input, i.e. independent variable)\n",
        "insurance.mod1_summary = insurance.mod1.summary()\n",
        "print(insurance.mod1_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuZR7vmS5FDy"
      },
      "source": [
        "### **WHOA! T.M.I.!--Understanding the Output**\n",
        "\n",
        "We're seeing a lot of information here.\n",
        "\n",
        "**Most important**: Point your eyes a little below the middle of this output. That's where you'll see the following:\n",
        "```\n",
        "         coef\t    std err\t t\t    P>|t|\t[0.025\t  0.975]\n",
        "const\t3165.8850\t937.149\t3.378\t0.001\t1327.440\t5004.330\n",
        "age\t   257.7226\t22.502\t11.453\t0.000\t 213.579\t301.866\n",
        "```\n",
        "**What does this mean?**\n",
        "\n",
        "The first number here is const coef, that is, the constant in our linear equation, here 3165.8850. It is the value for our letter b in\n",
        "\n",
        "                               `y = f(x) + b`\n",
        "\n",
        "The second important number here is age coef, that is, the factor by which we multiply the independent variable, here 257.7226. It is the value for f. So, the equation that OLS has built for us is\n",
        "\n",
        "                              `y = (257.7226 * x) + 3165.8850`\n",
        "\n",
        "\n",
        "The coefficient of 257.7226 means that as the age variable increases by 1, the predicted value of charges increases by 257.7226 plus 3165.8850.  \n",
        "\n",
        "We also meet an \"old friend\" again--the p value in the P>|t| column. Since the initial assumption, that is, the H0, of a Regression is always that all variables are independent from each other, the p value shows us how many %  of the result are really due to random chance. Remember how we learned that the magic threshold is 5%, and \"if p is low, H0 must go\"? With p being at 0.000, we're seeing here that age and charges are obviously dependent.\n",
        "\n",
        "**Now, let's look at this:**\n",
        "```\n",
        "                         OLS Regression Results                            \n",
        "==============================================================================\n",
        "Dep. Variable:                charges   R-squared:                       0.089\n",
        "Model:                            OLS   Adj. R-squared:                  0.089\n",
        "....\n",
        "No. Observations:                1338   AIC:                         2.883e+04\n",
        "Df Residuals:                    1336   BIC:                         2.884e+04                                         \n",
        "==============================================================================\n",
        "\n",
        "```\n",
        "Let's start at the bottom: The number of observations is 1338, which, since we are looking at 2 attributes, makes the degrees of freedom 1338 - 2 = 1336. No problem here. Let's move up. The R-squared value of 0.089 or 8.9% means that 8.9% of the variation in charges can be explained by using age as the predictor.\n",
        "\n",
        "To learn more about how to interpret the entire output, check out [this excellent video](https://www.youtube.com/watch?v=VvlqA-iO2HA).\n",
        "\n",
        "**Wait, what?**\n",
        "\n",
        "Let's look at these numbers again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V_WzHe2llBm"
      },
      "source": [
        "print('Parameters: ', insurance.mod1.params)\n",
        "print('R2: ', insurance.mod1.rsquared)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKu3Nskwmwuk"
      },
      "source": [
        "The R-squared value is 8.9%! If we compare this to our initial impression from the scatter diagram and the p-value interpretation above, namely that age could well be closely related to charges, th R-squared value makes age a lousy predictor of most of the charges. **LOUSY**!\n",
        "\n",
        "Let's take a look at the calculated values for x (the video called them y \"hat\") and then plot them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "F6tgl-pl5FD4"
      },
      "source": [
        "predictions = insurance.mod1.predict(X) # Here, you'll see the CALCULATED values with the formula\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r-t1YjSCeAN"
      },
      "source": [
        "###**2.1.2 Plotting the results**\n",
        "Now let's build all four regression plots. We pass in the model as the first parameter, then specify the predictor variable we want to analyze."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "bO66e2i45FE6"
      },
      "source": [
        "fig = plt.figure(figsize=(15,8))\n",
        "fig = sm.graphics.plot_regress_exog(insurance.mod1, \"age\", fig=fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLwE_YqQECTW"
      },
      "source": [
        "**WOW**! Remember the R-squared number of 8.9%? Here you can see the distribution of values for the age variable. Look at the plot in the upper left-hand corner of the fitted, i.e. predicted vs. actual values. That fit line isn't all that representative, is it--although there's clearly some connection (so, the p-value was correct)? And the residuals, which should be centered around 0? Check out the upper right-hand corner graph--obviously not the case, is it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vD73kRkFx_r"
      },
      "source": [
        "## **2.2. Multiple Linear Regression with scikit learn**--Numeric Attributes\n",
        "\n",
        "Well. We've seen where looking at just ONE variable in connection with our insurance charges got us--not too far. So ... what if we look at a few variables together as joint influencers of the charge variable? Perhaps that will increase the R-squared value. When we run a regression with more than one independent attribute in the X variable, that is called Multiple Regression. Here, we are using the linear model, so we are performing Multiple Linear Regression.\n",
        "\n",
        "Now, we could do this in statsmodels--but that'll be your job later. To shake things up, we will use another very popular and versatile package for this purpose: Scikit Learn. In our preparation and setup section, we already imported its linear_model subpackage.\n",
        "\n",
        "We will focus our Multiple Regression on the **joint influence of age, bmi, and number of children on charges**. You have already encountered these three in section 1 of this workbook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jpbpoNmIyxk"
      },
      "source": [
        "# Let's use the three \"big\" predictor variables from above: age, bmi, and number of children\n",
        "\n",
        "X = insurance[['age','bmi','children']] # Now we have values from three different independent attributes stored in X\n",
        "y = insurance['charges'] # This is, again, our dependent attribute\n",
        "print(X) # This is the content of our X variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vMb3aLKJOiA"
      },
      "source": [
        "# Defining the model for a multiple regression using the lm.fit() function:\n",
        "\n",
        "lm = linear_model.LinearRegression() # Here, we are using the linear_model subpackage from scikit-learn that we imported above\n",
        "model = lm.fit(X,y) # And here, we are building the model"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDZ6CofiJvzV"
      },
      "source": [
        "You will have noticed that, when you clicked on the previous code cell, the output was ... nothing. When we run a linear regression with SKLearn, we don’t get a pretty table like in Statsmodels. But then, the model is much easier to build. What we can do is use built-in functions to return the score, the coefficients and the estimated intercepts. Let’s see how it works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhYmaW2oJ0CN"
      },
      "source": [
        "# To find the R-squared score:\n",
        "lm.score(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Xbe99U6J9D9"
      },
      "source": [
        "# To find the coefficients for the predictors:\n",
        "lm.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JuKU2BhKFLp"
      },
      "source": [
        "# To find the intercept:\n",
        "lm.intercept_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm2b1ahWVCRA"
      },
      "source": [
        "What does all of this mean? Well, the R-squared value has improved from 8.9% to 12%. That's something. We are seeing our coefficients and the intercept, as well. This makes our equation\n",
        "\n",
        "\n",
        "```\n",
        "charges = 239.99 * age + 332.08 * bmi + 542.86 * children - 6916.24\n",
        "\n",
        "```\n",
        "As I said: Better already. Good? Not good enough. There's got to be a better way. And there is.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMd_g_mVWPKl"
      },
      "source": [
        "## **2.3 Multiple Linear Regression--All Attributes**\n",
        "Remember the other attributes we have in this dataset? Let's take a quick look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufCSC6m3Wi-H"
      },
      "source": [
        "insurance.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nufjV8gRWrRK"
      },
      "source": [
        "We still have sex, smoker, and region! Let's see what their influence is when combined with our numeric attributes.\n",
        "\n",
        "First, we set up our X and y attributes again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqIDYS52XCZZ",
        "outputId": "60be8af0-b81b-4861-867d-628b9d8ab283"
      },
      "source": [
        "X = insurance.iloc[:, :-1].values\n",
        "y = insurance.iloc[:, 6].values\n",
        "print(X)\n",
        "print(y)"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[19 'female' 27.9 0 'yes' 'southwest']\n",
            " [18 'male' 33.77 1 'no' 'southeast']\n",
            " [28 'male' 33.0 3 'no' 'southeast']\n",
            " ...\n",
            " [18 'female' 36.85 0 'no' 'southeast']\n",
            " [21 'female' 25.8 0 'no' 'southwest']\n",
            " [61 'female' 29.07 0 'yes' 'northwest']]\n",
            "[16884.924   1725.5523  4449.462  ...  1629.8335  2007.945  29141.3603]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B35j2LintRpv"
      },
      "source": [
        "Now, we can run our Multiple Regression again--this time with a process called \"Backward Elimination.\" In short, **Backward Elimination** is the process of entering all the independent variables into the equation first and **delete them one at a time** if they do not contribute to the regression equation.\n",
        "\n",
        "**Time to get up close and personal with r-squared**! Watch the video below:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IFrame(src=\"https://www.youtube.com/embed/bMccdk8EdGo\", width=560, height=315)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "I7kdGr3LoUug",
        "outputId": "07c87d56-70ca-4fa6-ffbe-bd33a043ed36"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7889aa6e0bb0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"560\"\n",
              "            height=\"315\"\n",
              "            src=\"https://www.youtube.com/embed/bMccdk8EdGo\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OH!**\n",
        "But since regression requires only numeric variables, we will need to work with an entirely numeric dataset. Good thing we have one--otherwise, we would need to do some serious preprocessing. Let's load this dataset:"
      ],
      "metadata": {
        "id": "Be6VcPnuocOg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIsRDZekua5A"
      },
      "source": [
        "insurance2 = pd.read_csv(\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/data/insurance.csv\")\n",
        "insurance2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXL3P6g_wPmP"
      },
      "source": [
        "There is an additional column called \"insuranceclaim\". We will delete this column for now. (NOTE: If you click the \"run\" icon below more than once, you will get a runtime error because Python is trying to delete a column that has already been deleted)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUKsKfPGwhPm"
      },
      "source": [
        "del insurance2['insuranceclaim']\n",
        "insurance2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBahGGV3u0Cr"
      },
      "source": [
        "Now we can start eliminating attributes if they don't contribute to an optimal regression result (again, let's watch r-squared). We will use statsmodels this time around. First, we will set up X and y again so that we replace the strings currently in these variables with numbers:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wPiF3n_vFmN"
      },
      "source": [
        "X = insurance2.iloc[:, :-1].values\n",
        "y = insurance2.iloc[:, 6].values\n",
        "\n",
        "X = np.append(arr = np.ones((1338, 1)).astype(int), values = X, axis = 1) # We are building our numpy array\n",
        "\n",
        "X_opt = X[:, [0, 1, 2, 3, 4, 5, 6]] # Now we combine all 6 input variables into our first iteration\n",
        "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit() # Now we set up our regressor function with OLS again as before.\n",
        "print(regressor_OLS.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va2LVCq4wD3s"
      },
      "source": [
        "Wow! With all variables combined, we can achieve and r-squared value of 75%. Nice. But you never know if two variables maybe negatively impact each other. So, let's remove one and see if that will help us increase r-squared."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tnee2qiYxRZ2"
      },
      "source": [
        "# How about age? That wasn't so good before.\n",
        "\n",
        "X_opt = X[:, [0, 2, 3, 4, 5]] # Age has been removed\n",
        "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit() # Now we set up our regressor function with OLS again as before.\n",
        "print(regressor_OLS.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzuhKnWexQa_"
      },
      "source": [
        "Oh. Well, that actually decreased R-squared. How about eliminating sex?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "046pUvdayP5F"
      },
      "source": [
        "# bmi looked pretty split before. Removing bmi now.\n",
        "\n",
        "X_opt = X[:, [0, 2, 4, 5, 6]] # Age has been removed\n",
        "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit() # Now we set up our regressor function with OLS again as before.\n",
        "print(regressor_OLS.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVbGCdbm4tGP"
      },
      "source": [
        "Wow. That dropped the r-squared function even faster. How about bmi, then?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnYxeXSV41Tw"
      },
      "source": [
        "X_opt = X[:, [0, 1, 3, 4, 5, 6]] # Removed bmi\n",
        "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit() # Now we set up our regressor function with OLS again as before.\n",
        "print(regressor_OLS.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuLfrDK85LL5"
      },
      "source": [
        "Heh. Removing bmi did nothing to our r-squared--except for saving us the computing resources that would come from applying the regression to the full dataset including bmi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TKLUEeo5lEP"
      },
      "source": [
        "X_opt = X[:, [0, 1, 3, 4, 6]] # Removing bmi and region\n",
        "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit() # Now we set up our regressor function with OLS again as before.\n",
        "print(regressor_OLS.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBbDBO39595l"
      },
      "source": [
        "Well, looks like we definitely want to keep region! What about smoker?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1offbs66OaG"
      },
      "source": [
        "X_opt = X[:, [0, 1, 3, 5, 6]] # Removing bmi and smoker\n",
        "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit() # Now we set up our regressor function with OLS again as before.\n",
        "print(regressor_OLS.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZEpARD-6coY"
      },
      "source": [
        "Wow. We just saved ourselves processing 2,600 values and lost less than 1 percent in our r-squared value. That seems like a pretty good deal!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1snN5oGnk9A"
      },
      "source": [
        "##**WAIT! What was the Goal again?**\n",
        "\n",
        "We started this entire regression journey because we wanted to find a function that would allow us to **PREDICT** future values of the charges attribute. This means that we would want to combine all our X inputs (as we did above) in order to find y values based on the regression formula. This also means that we need more independent attributes with no values in the associated y attribute, so that we can actually **CALCULATE** y.\n",
        "\n",
        "But our dataset is clean, with 1338 values in each column.\n",
        "\n",
        "So ... we need to either invent more independent attribute values, OR we need to delete some of those y attribute data, so that we can actually calculate them.\n",
        "\n",
        "Inventing data is a big no-no when you're dealing with actual data (although data scientists will often create artificial datasets to prove a theory). This means that the only other thing left to us is to delete data from the y column.\n",
        "\n",
        "But how? And how much?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOW4wCWorK78"
      },
      "source": [
        "##**2.4 Training and Test Datasets**\n",
        "What we really need at this point is two different datasets. We need one dataset to build, or **TRAIN**, our regression function. Then, we can use a second dataset on which to **TEST** whether our function actually works. However, since we have only one dataset, we will have to\n",
        "* Split the dataset into a larger training dataset and a smaller testing dataset\n",
        "* Build our function on the basis of the training dataset\n",
        "* Remove the y values in the test dataset\n",
        "* Apply our function to the test dataset\n",
        "* Compare the computed values for the test dataset (the Y hat values!) with the actual values in y that we did, of course, not throw away entirely.\n",
        "\n",
        "In fact, that's what we will do with all data from hereon out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhGSu-yGspVb"
      },
      "source": [
        "###**2.4.1 Building the Training and Test Set**\n",
        "We will use the generic train_test_split function from scikit-learn for this. We will build a separate X_train and X_test frame and a y_train and a y_test frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5ghof9Cszhb"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_opt, y, test_size = 0.2, random_state = 0) # Size of test data is 20%, size of training data is 80%\n",
        "\n",
        "print('X Train', X_train.shape)\n",
        "print('X Test', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmX2RqjG7KXe"
      },
      "source": [
        "###**2.4.2 Finding the Regression Formula**\n",
        "Now we can find the regression formula in the training set!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2bK_KqTn1OB"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "print('Model accuracy score:', round(regressor.score(X_train,y_train)*100,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MZyZ2149Lgn"
      },
      "source": [
        "The model accuracy score is basically the r-squared value expressed as percentage and rounded to two decimals. Good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lktme0f9pw9"
      },
      "source": [
        "###**2.4.3 Applying the Regression Formula to the Test Set**\n",
        "Now we can plug our X_test set values into the formula and calculate the y_test values to predict. In order not to confuse the actual y_test values with the values we are going to calculate, we will call our predicted values y_pred."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyrgSN1Y9fTd"
      },
      "source": [
        "y_pred = regressor.predict(X_test)\n",
        "print(y_pred[0:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F0B6G_7_aGc"
      },
      "source": [
        "###**2.4.4 Comparing predictions and actual values in the test set**\n",
        "What good are the predictions if you can't tell how close to reality they are? That's where our y_test comes in. Now we can compare our y_pred and y_test, and we will see how good our prediction is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2-bykaQFsGk"
      },
      "source": [
        "charges_pred = pd.DataFrame({'Prediction (y_pred)':y_pred,'Actual Values (y_test)':y_test}).round(2)\n",
        "print(charges_pred.shape)\n",
        "charges_pred.head(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyIJg-MtACBm"
      },
      "source": [
        "We can see that the y_pred data are a bit more extreme on each end than the y_test data. That's why accuracy is at around 73%--not great, but certainly better than 8.9% or 12%.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*Do0lysWTXigG3F79FRRtKQ.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSjghpUTTwM1"
      },
      "source": [
        "# **3. Classification with Logistic Regression**\n",
        "You just learned how to use Simple Regression and a training/ test set split to calculate and predict **numeric output**. That's because Regression is a mathematical tool.\n",
        "\n",
        "But what if we are interested in **categorical output**? What if we want to use a student's grades to predict whether they will pass or fail a class at the end of the semester? Or use a soccer team's past number of goals in a tournament to determine if it will win or lose the World Cup? Or see if  body weight, calorie intake, fat intake, and age have an influence on the probability of having a heart attack (yes vs. no)?\n",
        "\n",
        "We will use the first insurance dataset again, but with an additional insuranceclaim dimension that contains 1 (claim valid) or 0 (claim invalid). We are setting the dataframe up from scratch again first, though."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZbUDYSqcnrF"
      },
      "source": [
        "##**3.1 What is Logistic Regression?**\n",
        "We have already seen the questions Logistic Regression tries to answer. Basically, there are THREE RULES for Logistic Regression:\n",
        "1. The dependent variable should be dichotomous (e.g., yes/ no, present/ absent, pass/ fail, and so on).\n",
        "2. There should be no outliers in the data.\n",
        "3. There should be no high correlations (multicollinearity) among the predictors (i.e. we'll have to set up a correlation matrix among the predictors and look for small Pearson Correlation Coefficients (r, with which we have already worked multiple times).\n",
        "\n",
        "###**How does Logistic Regression Work?**\n",
        "Logistic Regression isn't all that different from Multiple Linear Regression. But where the output of Multiple Linear Regression is continuous, the output of Logistic Regression is binary or \"dichotomous.\" In other words, we don't want a number at the end; we want one of two values. That requires some additional math--namely [a logistic function](https://careerfoundry.com/en/blog/data-analytics/what-is-logistic-regression/), which is based on calculating the odds of an outcome. For example, let's say that, since in the past your soccer team has won 20 out of 21 games in the World Cup tournament, the odds of them winning finals are 20 to 21, or, when expressed as an odds ratio, they are 20/21. To standardize these odds ratios to a 1 or 0 outcome, we need the *logit function*, which is there Logistic Regression gets its name. This function applies a log odds logarithm to the outcome of a multiple regression.\n",
        "\n",
        "To learn more about the math behind Logistic Regression, watch this great 9-minute video:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "65_kl-FahUJY",
        "outputId": "6446bd9e-2c54-44b9-d2ab-163577a4bd6f"
      },
      "source": [
        "IFrame(src=\"https://www.youtube.com/embed/yIYKR4sgzI8\", width=560, height=315)"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7889a69123b0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"560\"\n",
              "            height=\"315\"\n",
              "            src=\"https://www.youtube.com/embed/yIYKR4sgzI8\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CD3LbCqhtnt"
      },
      "source": [
        "##**3.2 Working with Logistic Regression**\n",
        "Here is how Logistic Regression works for our example:\n",
        "\n",
        "Remember how, when working with the insurance2 dataset, we deleted that insuranceclaim column? Well, we are going to use it now. For that purpose, we are reading the dataset in again and name it insurance3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccjvFo9ugIgy"
      },
      "source": [
        "insurance3 = pd.read_csv(\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/data/insurance.csv\")\n",
        "insurance3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doEJdDDviPPE"
      },
      "source": [
        "###**3.2.1 Satisfying the Three Rules**\n",
        "We already see that insuranceclaim is binary or dichotomous and seems to have values of 1 and 0. That satisfies Rule #1 above. On to our second rule--we need to check for multicollinearity, i.e. connections among the attributes themselves. For that, we are setting up a correlation matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15Q9_Y6wi6WE"
      },
      "source": [
        "corr = insurance3.corr()\n",
        "corr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMnRyOGojHrQ"
      },
      "source": [
        "##Your Turn\n",
        "Which variables have > 50% correlation? Type the variable below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGU-GtAdjjLO"
      },
      "source": [],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iAH5yI-jkWB"
      },
      "source": [
        "To make things fancy, we can even visualize the correlations in a heat map using the [seaborn package](https://seaborn.pydata.org/introduction.html), which is one of the really great graphics packages in Python! This works like so (and will give you the answer to the Your Turn question above):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHWDG0qMjxPD"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "f, ax = plt.subplots(figsize = (10, 10))\n",
        "sns.heatmap(corr, mask = np.zeros_like(corr, dtype = bool),\n",
        "            cmap = sns.diverging_palette(150, 275, as_cmap = True), square = True, ax = ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEtlLG4bkqCl"
      },
      "source": [
        "Compare what you see on this heatmap of correlations with the answer you typed in the field above. Does your answer match what we see here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5emqGlflAz2"
      },
      "source": [
        "###**3.2.2 Preprocessing the Data: Training and Test Set**\n",
        "The data set consists of record of 1338 patients in total. To train our model we will be using 1000 records. We will be using 300 records for testing, and we will use the last 38 records to cross check our model. Instead of splitting percentage-wise, as we did above, we are now splitting by row number:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REMMALG2lXAn"
      },
      "source": [
        "dfTrain = insurance3[:1000]\n",
        "dfTest = insurance3[1000:1300]\n",
        "dfCheck = insurance3[1300:]"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncO3C2H2luMz"
      },
      "source": [
        "Next, we separate the label and features (for both training and test dataset). In addition to that, we will also convert them into NumPy arrays as our machine learning algorithm processes data in NumPy array format."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'insuranceclaim' column to numpy array for training and testing sets\n",
        "train_y = np.asarray(dfTrain['insuranceclaim'])\n",
        "train_x = np.asarray(dfTrain.drop('insuranceclaim', axis=1)) # We remove insurancelcaim from the predictors (because it's the output variable)\n",
        "\n",
        "test_y = np.asarray(dfTest['insuranceclaim'])\n",
        "test_x = np.asarray(dfTest.drop('insuranceclaim', axis=1))\n",
        "\n",
        "# Display train_x to verify the changes\n",
        "train_x"
      ],
      "metadata": {
        "id": "c2avUK9Nz6zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "betS8VQImVFp"
      },
      "source": [
        "This is what the numpy array looks like. No worries. All the data is there.\n",
        "\n",
        "As the final step before using machine learning, we will normalize our inputs. Machine Learning models often benefit substantially from input normalization. It also makes it easier for us to understand the importance of each feature later, when we’ll be looking at the model weights.  We’ll standardize the data such that each variable has 0 mean and standard deviation of 1.\n",
        "\n",
        "One way of doing this is by using the means:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvSUsuPqmlrE"
      },
      "source": [
        "means = np.mean(train_x, axis=0)\n",
        "std = np.std(train_x, axis=0)\n",
        "\n",
        "train_x = (train_x - means)/std\n",
        "test_x = (test_x - means)/std"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way of doing this is with a function called StandardScaler that we need to import (because we didn't import it at the beginning of this file):"
      ],
      "metadata": {
        "id": "nvcYKu-S4R4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize the numeric columns\n",
        "scaler = StandardScaler()\n",
        "train_x = scaler.fit_transform(train_x)\n",
        "test_x = scaler.transform(test_x)"
      ],
      "metadata": {
        "id": "jim44jnh4Ve7"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIc8rrtmmpqm"
      },
      "source": [
        "Now, we can start with our Logistic Regression!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDUMkaanmwZz"
      },
      "source": [
        "###**3.2.3 Building the Model**\n",
        "We can now train our classification model with logistic regression. First, we create a model and then use the fit function to train the model. We use the [LogisticRegression() function](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) from scikit-learn. Follow the link to learn more about this very versatile function!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tpyBEWUm8wu"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(train_x, train_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hnj3pE-MnB-U"
      },
      "source": [
        "###**3.2.4 Testing the Model**\n",
        "Now we use our test data to determine the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gMgKplInId-"
      },
      "source": [
        "accuracy = model.score(test_x, test_y)\n",
        "print(\"accuracy = \", accuracy * 100, \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MA3nhscnhju"
      },
      "source": [
        "###**3.2.5 Using the Model for Predictions**\n",
        "Now we can use the model for predictions on the test set!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U4TYyrynuvn"
      },
      "source": [
        "pred = model.predict(test_x)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO7gX0N-n8Rn"
      },
      "source": [
        "And here is our entire analysis process--and the calculated output! We are seeing how the function allowed us to predict 1 or 0, just as we needed!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WjwSDqCn21_"
      },
      "source": [
        "# Let's compare the actual y and the predicted y\n",
        "\n",
        "realvsmodel = pd.DataFrame(pred,test_y)\n",
        "realvsmodel = pd.DataFrame({'predicted':pred,'original':test_y})\n",
        "realvsmodel.head(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8csdTkgQoIdv"
      },
      "source": [
        "##Your Turn\n",
        "1. Create a subset of the adult dataset with just the numeric variables.\n",
        "2. Can you use Logistic Regression to predict the sex of someone based on the all the numeric inputs? Use this as your guiding question for your exploration. Follow all the steps in section 3.2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcjpNDuuohsh"
      },
      "source": [],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a-re79Coh4W"
      },
      "source": [],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeBimbe1oiF5"
      },
      "source": [],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4. If you get stuck ...**"
      ],
      "metadata": {
        "id": "BFixOSg8PuwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatterplot children vs charges\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(insurance['children'], insurance['charges'], alpha=0.5) # alpha=0.5 is added to make the points slightly transparent, making it easier to see overlapping points.\n",
        "plt.title('Scatter Plot of Children vs. Charges')\n",
        "plt.xlabel('Number of Children')\n",
        "plt.ylabel('Charges')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-IJCqJR1Qrgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Exploratory Data Analysis\n",
        "\n",
        "\t        age\t      bmi\t      children\tcharges\n",
        "\n",
        "age\t      1.000000\t0.109272\t0.042469\t**0.299008**\n",
        "\n",
        "bmi\t      0.109272\t1.000000\t0.012759\t0.198341\n",
        "\n",
        "children\t0.042469\t0.012759\t1.000000\t0.067998\n",
        "\n",
        "charges\t **0.299008**\t0.198341\t0.067998\t1.000000"
      ],
      "metadata": {
        "id": "lZWv4Gn3P9Jp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 'charges' and 'smoker' have a correlation of 78%."
      ],
      "metadata": {
        "id": "vzugFTVXTJAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create a subset of the adult dataset with just the numeric variables.\n",
        "# Can you use Logistic Regression to predict the sex of someone based on the all the numeric inputs? Use this as your guiding question for your exploration.\n",
        "# Follow all the steps in section 3.2.\n",
        "\n",
        "# Load the dataset\n",
        "insurance3 = pd.read_csv(\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/data/insurance.csv\")\n",
        "\n",
        "# Create a subset of the dataset with numeric variables and 'sex'\n",
        "insurance_num = insurance[['age', 'bmi', 'children', 'charges', 'sex']]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "dfTrain1 = insurance_num[:1000] # Reducing the dataframe to 1000 rows for the training set\n",
        "dfTest1 = insurance_num[1000:1300] # Defining the test set\n",
        "dfCheck1 = insurance3[1300:] # Everything else\n",
        "\n",
        "# Extract the target variable ('sex') and the predictors\n",
        "train_y = np.asarray(dfTrain1['sex'])\n",
        "train_x = np.asarray(dfTrain1.drop('sex', axis=1))\n",
        "test_y = np.asarray(dfTest1['sex'])\n",
        "test_x = np.asarray(dfTest1.drop('sex', axis=1))\n",
        "\n",
        "# Standardize the numeric columns\n",
        "means = np.mean(train_x, axis=0)\n",
        "std = np.std(train_x, axis=0)\n",
        "train_x = (train_x - means) / std\n",
        "test_x = (test_x - means) / std\n",
        "\n",
        "# Train a logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(train_x, train_y)\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(test_x)\n",
        "accuracy = accuracy_score(test_y, predictions)\n",
        "\n",
        "print(\"Accuracy:\", accuracy * 100, \"%\")\n",
        "print(\"Predictions:\", predictions)\n",
        "\n",
        "# Create a DataFrame to compare predictions with actual values\n",
        "real_vs_predicted = pd.DataFrame({'Predicted': predictions, 'Actual': test_y})\n",
        "realvsmodel.head(25)\n",
        "# print(real_vs_predicted.head(25)) # Another way to show the values on screen\n"
      ],
      "metadata": {
        "id": "3Yg-ErZU1h0t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}