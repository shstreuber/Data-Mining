{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNTrNymf14/vEPNieGHyQpO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shstreuber/Data-Mining/blob/master/Module10_Recurrent_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**What is a Recurrent Neural Network**\n",
        "A recurrent neural network (RNN) is the type of artificial neural network (ANN) that is used in Apple’s Siri and Google’s voice search.\n",
        "<center>\n",
        "<img src=\"https://www.apple.com/v/siri/h/images/overview/routers_tile_1__gds6mleh3lea_large.png\">\n",
        "</center>\n",
        "\n",
        "At a high level, a recurrent neural network (RNN) **processes sequences** — whether daily stock prices, sentences, or sensor measurements — one element at a time **while retaining a memory** (called a state) of what has come previously in the sequence. In other words, RNN remembers past inputs due to an internal memory which is useful for predicting stock prices, generating text, transcriptions, and machine translation.\n",
        "\n",
        "<img src = \"https://media.geeksforgeeks.org/wp-content/uploads/20231204130132/RNN-vs-FNN-660.png\">\n",
        "\n",
        "**RECURRENT** means the output at the current time step becomes the input to the next time step. At each element of the sequence, the model considers not just the current input, but what it remembers about the preceding elements.\n",
        "\n",
        "In an RNN, the information cycles through the loop, so the output is determined by the current input and previously received inputs. The input layer  processes the initial input and passes it to the middle layer RNN. The middle layer consists of multiple hidden layers, each with its activation functions, weights, and biases. These parameters are standardized across the hidden layer so that instead of creating multiple hidden layers, it will create one and loop it over.\n",
        "<center>\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1500/1*czgLJc2bXADt9N7yJX6S1w.png\">\n",
        "</center>\n",
        "\n",
        "Instead of using traditional backpropagation, like this:\n",
        "<center>\n",
        "<img src = \"https://editor.analyticsvidhya.com/uploads/18870backprop2.png\">\n",
        "</center>\n",
        "\n",
        "Recurrent Neural Networks use **backpropagation through time (BPTT) algorithms** to determine the gradient.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZDOQJis8y7-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import IFrame  # This is just for me so I can embed videos\n",
        "IFrame(src=\"https://www.youtube.com/embed/0XdPIqi0qpg\", width=560, height=315)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "iXbF7DClOtOl",
        "outputId": "de336d94-4678-4cba-c41a-264add4e4283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f94e40fdae0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"560\"\n",
              "            height=\"315\"\n",
              "            src=\"https://www.youtube.com/embed/0XdPIqi0qpg\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/0XdPIqi0qpg?si=OrjrTrK88BJFrF5I\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n",
        "\n",
        "In backpropagation, the model adjusts the parameter by calculating errors from the output to the input layer. BPTT sums the error at each time step as RNN shares parameters across each layer.\n",
        "\n",
        "NOW, take a look at this great video that explains RNNs in very clear terms:\n",
        "\n"
      ],
      "metadata": {
        "id": "8E1ymFt6Oqsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IFrame(src=\"https://www.youtube.com/embed/AsNTP8Kwu80\", width=560, height=315)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "vTpKDAEtPnhM",
        "outputId": "9ca83958-b432-4221-8402-3343bb56db57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f94e40fe290>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"560\"\n",
              "            height=\"315\"\n",
              "            src=\"https://www.youtube.com/embed/AsNTP8Kwu80\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Recurrent Neural Network Example: Text Prediction**\n",
        "RNNs are often used to predict text. So, that's what we will do.\n",
        "<center>\n",
        "<img src = \"https://m.media-amazon.com/images/I/819+5EFvhBL._AC_UF1000,1000_QL80_.jpg\" width = 300>\n",
        "</center>\n",
        "\n",
        "In the example below, we will use a Recurrent Neural Network to predict lines from [William Shakespeare's famous Sonnets](https://www.shakespeare.org.uk/explore-shakespeare/shakespedia/shakespeares-poems/).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O9zCTE0tENwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**0. Importing the Libraries and Preparing the Data**\n",
        "1. We import the libraries we need (including numpy and tensorflow):\n",
        "\n",
        " 1. **`import requests`**:\n",
        "   - **Purpose**: This library helps your program communicate with websites.\n",
        "   - **Example Use**: If you want to download a text file from the internet, you can use `requests` to fetch the file.\n",
        "\n",
        " 2. **`from tensorflow.keras.preprocessing.text import Tokenizer`**:\n",
        "   - **Purpose**: This tool helps convert text into numerical data that a computer can understand.\n",
        "   - **Example Use**: If you have a sentence and you want to turn each word into a number, the `Tokenizer` can do that for you.\n",
        "\n",
        " 3. **`from tensorflow.keras.preprocessing.sequence import pad_sequences`**:\n",
        "   - **Purpose**: This tool ensures that all your sequences of numbers are the same length by adding extra zeros where needed.\n",
        "   - **Example Use**: If you have sentences of different lengths and you need them all to be the same length for processing, `pad_sequences` can add padding to make them uniform.\n",
        "\n",
        "Together, these libraries and tools help us fetch data from the internet, process it into a format suitable for machine learning, and build models to learn from and make predictions with this data.\n",
        "2. Then we download the text of Shakespeare's Sonnets and break it down into smaller pieces (words and sentences)."
      ],
      "metadata": {
        "id": "Q0-vkOBwFr1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Download the text of Shakespeare's Sonnets\n",
        "url = \"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/data/ShakespeareSonnets.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text"
      ],
      "metadata": {
        "id": "VVQq_A-LFNKl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3. Next, we **tokenize** the Sonnets as follows:"
      ],
      "metadata": {
        "id": "FbOj1NhdHc-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "npcreFkjIFBn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **`tokenizer = Tokenizer()`**:\n",
        "   - This line creates an instance of the `Tokenizer` class from the `tensorflow.keras.preprocessing.text` module. The `Tokenizer` is a tool that helps convert text into sequences of numbers. Each unique word in the text will be assigned a unique number.\n",
        "\n",
        "2. **`tokenizer.fit_on_texts([text])`**:\n",
        "   - The `fit_on_texts` method updates the internal vocabulary based on the list of texts provided. In this case, we are passing the entire text (which contains Shakespeare's Sonnets) as a list with one element.\n",
        "   - What happens here is that the `Tokenizer` goes through the entire text and builds a dictionary (`word_index`) where each unique word is assigned a unique integer index. For example, the word \"the\" might be assigned the index 1, \"and\" might be assigned 2, and so on.\n",
        "\n",
        "3. **`total_words = len(tokenizer.word_index) + 1`**:\n",
        "   - This line calculates the total number of unique words in the text by getting the length of the `word_index` dictionary.\n",
        "   - `tokenizer.word_index` is a dictionary where the keys are the words and the values are their corresponding integer indices.\n",
        "   - `len(tokenizer.word_index)` gives the number of unique words in the text. Adding 1 is often done to account for padding tokens (e.g., `0`), making the total word count useful for defining the vocabulary size in subsequent neural network layers.\n",
        "\n",
        "In summary, this code snippet prepares the text for use in a machine learning model by converting words into numerical representations. This numerical representation is essential because our RNNs work with numbers rather than raw text."
      ],
      "metadata": {
        "id": "2TRL5l2sIQHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next code snippet creates sequences of tokenized words from the text. For each line in the text, it generates multiple n-gram sequences. Each n-gram sequence starts from the beginning of the line and extends one word at a time. These sequences are stored in the input_sequences list and will be used to train a model to predict the next word in a sequence, which is a common task in text generation and language modeling.\n",
        "\n",
        "For example, if the line is \"shall I compare thee,\" the sequences created will be:\n",
        "\n",
        "\"shall I\"\n",
        "\"shall I compare\"\n",
        "\"shall I compare thee\"\n",
        "Each sequence helps the model learn to predict the next word given a sequence of words."
      ],
      "metadata": {
        "id": "teVwS3DgIuTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input sequences using the tokenized text\n",
        "input_sequences = []\n",
        "for line in text.split('\\n'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)"
      ],
      "metadata": {
        "id": "BA-4B6rxHdQ6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next code snippet ensures that all sequences in input_sequences have the same length by padding shorter sequences with zeros at the beginning. This is necessary because neural networks require input data to have a uniform shape. By padding the sequences, the model can process them in batches without encountering shape mismatches. The longest sequence determines the length to which all other sequences are padded."
      ],
      "metadata": {
        "id": "giGQSbTjJM7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences for equal length\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
      ],
      "metadata": {
        "id": "IJJaRacjIxEu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1. Preprocessing**\n",
        "Now that our data is prepared, we can create our predictors (input attributes) and label (output attribute). As with any other Neural Network, we need to one-hot encode the output."
      ],
      "metadata": {
        "id": "DzZM6XoVJWIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create predictors and label\n",
        "X, y = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "# One-hot encode the label\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)"
      ],
      "metadata": {
        "id": "pope0sqoJWhS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2. Building and Compiling the Model**\n",
        "Now we are ready to build and compils a recurrent neural network for text generation. The model consists of an embedding layer that converts word indices into dense vectors, a simple RNN layer with 150 units that processes the sequences, and a dense output layer with softmax activation that produces a probability distribution over the vocabulary. The model is compiled with categorical cross-entropy loss and the Adam optimizer, and it tracks accuracy as a performance metric."
      ],
      "metadata": {
        "id": "BQoDAzWrJ2Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 100, input_length=max_sequence_len-1),\n",
        "    tf.keras.layers.SimpleRNN(150, return_sequences=False),\n",
        "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "avFykTVfJ2hM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Here is how we are building the Sequential model as an RNN:**\n",
        "\n",
        "**1. Embedding Layer:**\n",
        "\n",
        "`tf.keras.layers.Embedding(total_words, 100, input_length=max_sequence_len-1)`\n",
        "\n",
        "This layer turns positive integers (indexes) into dense vectors of fixed size.\n",
        "* total_words is the size of the vocabulary.\n",
        "* 100 is the dimension of the dense embedding.\n",
        "* input_length=max_sequence_len-1 specifies the length of input sequences after padding.\n",
        "\n",
        "This layer effectively learns a vector representation for each word in the vocabulary.\n",
        "\n",
        "**2. SimpleRNN Layer:**\n",
        "\n",
        "`tf.keras.layers.SimpleRNN(150, return_sequences=False)`\n",
        "This is a simple recurrent neural network (RNN) layer with 150 units.\n",
        "* 150 is the number of units in the RNN.\n",
        "* return_sequences=False means the layer will only return the output of the last time step, not the entire sequence.\n",
        "\n",
        "This is suitable for many-to-one problems.\n",
        "\n",
        "**3. Dense Layer:**\n",
        "\n",
        "`tf.keras.layers.Dense(total_words, activation='softmax')`\n",
        "This is a fully connected (Dense) layer with total_words units.\n",
        "* total_words corresponds to the size of the vocabulary, as each unit will output a probability for each word.\n",
        "* activation='softmax' ensures the output is a probability distribution over the vocabulary, making it suitable for classification tasks.\n",
        "\n",
        "**Compile the Model:**\n",
        "\n",
        "`model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])`\n",
        "* **loss**:\n",
        "categorical_crossentropy is used as the loss function, which is suitable for multi-class classification problems.\n",
        "* **optimizer:**\n",
        "adam is an optimization algorithm that adjusts the learning rate throughout training. It is widely used due to its efficiency and effectiveness.\n",
        "* **metrics:**\n",
        "['accuracy'] indicates that the model will also track accuracy as a metric during training and evaluation.\n",
        "\n",
        "Not much of this should be new at this point."
      ],
      "metadata": {
        "id": "PLWvzVAtKS9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3. Training the Model**\n",
        "The next code snippet trains the neural network model using the input data (`X`) and target data (`y`). The model is trained for 100 epochs, meaning the entire dataset is passed through the network 100 times. The training process is monitored and printed to the console due to the `verbose=1` parameter. The result of this training process is stored in the `history` variable, which contains details about the training loss and accuracy for each epoch. This information can be useful for evaluating the model's performance and for plotting training curves."
      ],
      "metadata": {
        "id": "WHAbT06ZLzNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "history = model.fit(X, y, epochs=100, verbose=1)"
      ],
      "metadata": {
        "id": "sHgYalrYKTLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step-by-Step Breakdown:\n",
        "\n",
        "1. **Fit the Model**:\n",
        "   ```python\n",
        "   history = model.fit(X, y, epochs=100, verbose=1)\n",
        "   ```\n",
        "\n",
        "   - **model.fit**:\n",
        "     - This method trains the model on the given data (`X` and `y`).\n",
        "     - `X` represents the input sequences used for training.\n",
        "     - `y` represents the corresponding labels (one-hot encoded target words).\n",
        "\n",
        "   - **Parameters**:\n",
        "     - `X`: The input data, which are the sequences of tokens created from the text. Each sequence is padded to the same length.\n",
        "     - `y`: The target data, which are the one-hot encoded next words corresponding to each input sequence.\n",
        "     - `epochs=100`: The number of times the entire training dataset is passed forward and backward through the neural network. Training for more epochs usually leads to better performance, up to a point where the model starts to overfit.\n",
        "     - `verbose=1`: This parameter controls how much information is displayed during training. A value of `1` means that the progress bar and training status will be printed to the console for each epoch."
      ],
      "metadata": {
        "id": "5eNeH3XTL6aj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**4. Using the Model**\n",
        "This is different from what you have seen before--because now we are generating text.\n",
        "\n",
        "The generate_text function below iteratively generates text using a deep learning model (model) trained on sequences of tokens. It starts with a seed_text, predicts the next word based on the model's output probabilities, and continues until the desired number of words (next_words) is generated. The function relies on the tokenization and padding we did initially, and on the prediction mechanisms we just set up to produce coherent text based on the learned patterns in the training data."
      ],
      "metadata": {
        "id": "oimRGazaMp3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate text\n",
        "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text"
      ],
      "metadata": {
        "id": "qp_bGWhpL6ov"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an explanation of what happens in this function:\n",
        "\n",
        "```python\n",
        "# Function to generate text\n",
        "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "```\n",
        "\n",
        "### Step-by-Step Breakdown:\n",
        "\n",
        "1. **Function Definition**:\n",
        "   ```python\n",
        "   def generate_text(seed_text, next_words, model, max_sequence_len):\n",
        "   ```\n",
        "   - **Parameters**:\n",
        "     - `seed_text`: The starting text sequence from which to generate additional words.\n",
        "     - `next_words`: The number of words to generate beyond the `seed_text`.\n",
        "     - `model`: The trained deep learning model used for text generation.\n",
        "     - `max_sequence_len`: The maximum length of input sequences expected by the model.\n",
        "\n",
        "2. **Tokenization and Padding**:\n",
        "   ```python\n",
        "   token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "   token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "   ```\n",
        "   - **`tokenizer.texts_to_sequences([seed_text])[0]`**:\n",
        "     - Converts `seed_text` into a sequence of tokens (numbers) based on the tokenizer's vocabulary.\n",
        "   - **`pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')`**:\n",
        "     - Pads the token sequence to match the expected input shape of the model (`max_sequence_len-1`).\n",
        "\n",
        "3. **Prediction and Output**:\n",
        "   ```python\n",
        "   predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
        "   ```\n",
        "   - **`model.predict(token_list, verbose=0)`**:\n",
        "     - Generates predictions from the model for the padded token sequence `token_list`.\n",
        "   - **`np.argmax(..., axis=-1)`**:\n",
        "     - Retrieves the index of the word with the highest predicted probability from the model's output.\n",
        "\n",
        "4. **Mapping Predictions to Words**:\n",
        "   ```python\n",
        "   output_word = \"\"\n",
        "   for word, index in tokenizer.word_index.items():\n",
        "       if index == predicted:\n",
        "           output_word = word\n",
        "           break\n",
        "   ```\n",
        "   - **`tokenizer.word_index.items()`**:\n",
        "     - Provides a dictionary mapping from integer indices to words in the tokenizer's vocabulary.\n",
        "   - **Matching Predicted Index**:\n",
        "     - Finds the word corresponding to the predicted index returned by `np.argmax`.\n",
        "\n",
        "5. **Generating Text**:\n",
        "   ```python\n",
        "   seed_text += \" \" + output_word\n",
        "   ```\n",
        "   - Appends the predicted word to `seed_text`, preparing it for the next iteration or as the final generated text.\n",
        "\n",
        "6. **Return**:\n",
        "   ```python\n",
        "   return seed_text\n",
        "   ```\n",
        "   - Returns the generated text sequence after the specified number of words (`next_words`) have been added"
      ],
      "metadata": {
        "id": "TpMgcVFPNTQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**5. Processing the User-Generated Input**\n",
        "Another new thing! The user now supplies input.\n",
        "\n",
        "This part of the code shows us how to use our trained deep learning model (model) to generate text based on an initial seed_text. It leverages the generate_text function to iteratively predict and append words to the seed_text, ultimately producing a longer text sequence (generated_text). The number of words to generate beyond the seed_text is controlled by the next_words parameter. This approach showcases the application of natural language generation using deep learning techniques.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dNGu7jO8NX5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "seed_text = \"shall i compare thee to a summer's day?\"\n",
        "next_words = 100\n",
        "generated_text = generate_text(seed_text, next_words, model, max_sequence_len)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "qPl8My1HNTc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet generates text based on a provided `seed_text` using our pre-trained deep learning model (`model`). Here's a breakdown of what each line does:\n",
        "\n",
        "1. **Seed Text Definition**:\n",
        "   ```python\n",
        "   seed_text = \"shall i compare thee to a summer's day?\"\n",
        "   ```\n",
        "   - Defines the starting text sequence (`seed_text`) from which the text generation process will begin.\n",
        "\n",
        "2. **Next Words to Generate**:\n",
        "   ```python\n",
        "   next_words = 100\n",
        "   ```\n",
        "   - Specifies the number of words (`next_words`) to generate beyond the `seed_text`.\n",
        "\n",
        "3. **Text Generation**:\n",
        "   ```python\n",
        "   generated_text = generate_text(seed_text, next_words, model, max_sequence_len)\n",
        "   ```\n",
        "   - Calls the `generate_text` function with parameters:\n",
        "     - `seed_text`: The initial text sequence to start generating from.\n",
        "     - `next_words`: The number of words to generate beyond the `seed_text`.\n",
        "     - `model`: The pre-trained deep learning model used for text generation.\n",
        "     - `max_sequence_len`: The maximum length of input sequences expected by the model.\n",
        "\n",
        "4. **Print Generated Text**:\n",
        "   ```python\n",
        "   print(generated_text)\n",
        "   ```\n",
        "   - Outputs (`print`) the generated text (`generated_text`) to the console.\n",
        "\n",
        "And that is your first Recurrent Neural Network in action!"
      ],
      "metadata": {
        "id": "tD1OM8XtOB7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Turn\n",
        "Run the code above a number of times. How does the output change when you run the code twice, 5 times, 10 times? Record your observations below!"
      ],
      "metadata": {
        "id": "F8C3agiZyEch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jXGL182QynG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Limitations**\n",
        "Simple RNN models usually run into **two major issues**. These issues are related to gradient, which is the slope of the loss function along with the error function.\n",
        "\n",
        "1. **Vanishing Gradient problem**\n",
        "  <img src = \"https://www.kdnuggets.com/wp-content/uploads/vanishing-gradient-problem-12.png\">\n",
        "  occurs when the gradient becomes so small that\n",
        "updating parameters becomes insignificant; eventually the algorithm stops learning.\n",
        "2. **Exploding Gradient problem**\n",
        "<img src = \"https://emergency.princeton.edu/sites/g/files/toruqf5936/files/styles/freeform_750w/public/2023-07/adobestock_explosion.jpeg?itok=clWTY0kY\">\n",
        " occurs when the gradient becomes too large, which makes the model unstable. In this case, larger error gradients accumulate, and the model weights become too large. This issue can cause longer training times and poor model performance. It is less common than the Vanishing Gradient\n",
        "\n",
        "Advanced RNN architectures such as LSTM and GRU mitigate the Vanishing Gradient problem.\n",
        "\n",
        "Read more about RNNs [here](https://www.datacamp.com/tutorial/tutorial-for-recurrent-neural-network).\n",
        "\n",
        "To preview LSTMs, go [here](https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470).\n"
      ],
      "metadata": {
        "id": "cjy0XNwzPYcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MasterCard Stock Price Prediction Using LSTM & GRU**\n",
        "Text generation is only one thing that we can do with Recurrent Neural Networks. We can also use them to predict numbers.\n",
        "\n",
        "In the next project, we are going to use Kaggle’s MasterCard stock dataset from May-25-2006 to Oct-11-2021 and train the LSTM and GRU models to forecast the stock price. This is a simple project-based tutorial where we will analyze data, preprocess the data to train it on advanced RNN models, and finally evaluate the results.\n",
        "\n",
        "The project requires Pandas and Numpy for data manipulation, Matplotlib.pyplot for data visualization, scikit-learn for scaling and evaluation, and TensorFlow for modeling. We will also set seeds for reproducibility."
      ],
      "metadata": {
        "id": "wVP_2zcY25qN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**0. Importing the Libraries and the Data**"
      ],
      "metadata": {
        "id": "5MGI6gYk3XNV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT6Mhg51yy3h"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.random import set_seed\n",
        "set_seed(455)\n",
        "np.random.seed(455)\n",
        "\n",
        "dataset = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/kalilurrahman/MasterCardStockData/main/Mastercard_stock_history.csv\", index_col=\"Date\", parse_dates=[\"Date\"]\n",
        ").drop([\"Dividends\", \"Stock Splits\"], axis=1)\n",
        "print(dataset.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1. Exploratory Data Analysis (EDA)**"
      ],
      "metadata": {
        "id": "GS-5INr1zIFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.describe()"
      ],
      "metadata": {
        "id": "Iqy-JsbL4Juv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.isna().sum()"
      ],
      "metadata": {
        "id": "uzOXMon-4Ue_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the data distribution\n",
        "tstart = 2016\n",
        "tend = 2020\n",
        "\n",
        "def train_test_plot(dataset, tstart, tend):\n",
        "    dataset.loc[f\"{tstart}\":f\"{tend}\", \"High\"].plot(figsize=(16, 4), legend=True)\n",
        "    dataset.loc[f\"{tend+1}\":, \"High\"].plot(figsize=(16, 4), legend=True)\n",
        "    plt.legend([f\"Train (Before {tend+1})\", f\"Test ({tend+1} and beyond)\"])\n",
        "    plt.title(\"MasterCard stock price\")\n",
        "    plt.show()\n",
        "\n",
        "train_test_plot(dataset,tstart,tend)"
      ],
      "metadata": {
        "id": "pIoUWWDv4iPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2. Preprocessing**\n"
      ],
      "metadata": {
        "id": "nvXhV-_m5cRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up Training and Test Sets\n",
        "def train_test_split(dataset, tstart, tend):\n",
        "    train = dataset.loc[f\"{tstart}\":f\"{tend}\", \"High\"].values\n",
        "    test = dataset.loc[f\"{tend+1}\":, \"High\"].values\n",
        "    return train, test\n",
        "training_set, test_set = train_test_split(dataset, tstart, tend)"
      ],
      "metadata": {
        "id": "dJzJ9z475q0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardizing the inputs with MinMaxScaler--this is a different form of normalization\n",
        "sc = MinMaxScaler(feature_range=(0, 1))\n",
        "training_set = training_set.reshape(-1, 1)\n",
        "training_set_scaled = sc.fit_transform(training_set)"
      ],
      "metadata": {
        "id": "mW56jLkS5xnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"This is the beginning of the Training Set BEFORE scaling \\n\",training_set)\n",
        "print(\"This is the beginning of the Training Set AFTER scaling \\n\",training_set_scaled)"
      ],
      "metadata": {
        "id": "oyWfT6oH57CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The split_sequence function uses a training dataset and converts it into inputs (X_train) and outputs (y_train).\n",
        "\n",
        "def split_sequence(sequence, n_steps):\n",
        "    X, y = list(), list()   # initialize two empty lists called X and y\n",
        "    for i in range(len(sequence)): # loop through the sequence argument and calculate the end_ix variable by adding the current index i to the n_steps argument\n",
        "        end_ix = i + n_steps\n",
        "        if end_ix > len(sequence) - 1: # If end_ix is greater than the length of the sequence minus 1, the loop is broken\n",
        "            break\n",
        "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] # create two variables seq_x and seq_y by slicing the sequence from the current index i to end_ix and selecting the value at end_ix\n",
        "        X.append(seq_x) # append to the X and y lists\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y) # return X and y as numpy arrays\n",
        "\n",
        "\n",
        "n_steps = 60 # initialize with 60\n",
        "features = 1\n",
        "# split into samples\n",
        "X_train, y_train = split_sequence(training_set_scaled, n_steps) # assign the output of calling the split_sequence function with the training_set_scaled argument and n_steps variable"
      ],
      "metadata": {
        "id": "jSwZjY_X6pIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are working with univariate series, so the number of features is one, and we need to reshape the X_train to fit on the LSTM model.\n",
        "# The X_train has [samples, timesteps], and we will reshape it to [samples, timesteps, features].\n",
        "\n",
        "# Reshaping X_train for model\n",
        "X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],features)"
      ],
      "metadata": {
        "id": "i2SXB1IH7w2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3. LSTM Model**\n",
        "The Long Short Term Memory (LSTM) is an advanced type of RNN, designed to prevent both decaying and exploding gradient problems. Just like RNN, LSTM has repeating modules, but the structure is different. Instead of having a single layer of tanh, LSTM has four interacting layers that communicate with each other through three gates: The Forget Gate, the Input Gate, and the Output Gate. This four-layered structure helps LSTM retain long-term memory and can be used in several sequential problems including machine translation, speech synthesis, speech recognition, and handwriting recognition.\n",
        "\n",
        "This is what an LSTM model looks like:\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1500/1*Mw4W7FZUbSr4EoriB5GuqQ.jpeg\">\n",
        "\n",
        "Our model will consist of a single hidden layer of LSTM and an output layer. You can experiment with the number of units, as more units will give you better results. For this experiment, we will set LSTM units to 125, tanh as activation, and set input size.\n",
        "\n",
        "We will compile the model with an RMSprop optimizer and mean square error as a loss function."
      ],
      "metadata": {
        "id": "8OzkTzu_76dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3.1 Build the Model**"
      ],
      "metadata": {
        "id": "ClCnYvx48aGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The LSTM architecture\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(LSTM(units=125, activation=\"tanh\", input_shape=(n_steps, features)))\n",
        "model_lstm.add(Dense(units=1))\n",
        "# Compiling the model\n",
        "model_lstm.compile(optimizer=\"RMSprop\", loss=\"mse\")\n",
        "\n",
        "model_lstm.summary()"
      ],
      "metadata": {
        "id": "j-JKE8Rs73vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3.2 Train the Model**"
      ],
      "metadata": {
        "id": "YTgMc7uy8e1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.fit(X_train, y_train, epochs=50, batch_size=32) # train on 50 epochs with 32 batch sizes."
      ],
      "metadata": {
        "id": "D9ko3uAd8TmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the loss value is very small, which indicates that the model is performing well on the training data."
      ],
      "metadata": {
        "id": "OkK9ak5T9AqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3.3 Running the model on the Test Set**\n",
        "We are going to repeat preprocessing and normalize the test set. First of all we will transform then split the dataset into samples, reshape it, predict, and inverse transform the predictions into standard form."
      ],
      "metadata": {
        "id": "wLKaDUta9VFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_total = dataset.loc[:,\"High\"] # select the \"High\" column from the dataset and assigns it to the variable dataset_total.\n",
        "inputs = dataset_total[len(dataset_total) - len(test_set) - n_steps :].values # selects the inputs for the test set by taking the values from dataset_total starting from the index len(dataset_total) - len(test_set) - n_steps\n",
        "inputs = inputs.reshape(-1, 1) # reshape inputs to a 2D array with one column using inputs.reshape(-1, 1).\n",
        "#scaling\n",
        "inputs = sc.transform(inputs) #  scale inputs using the sc.transform() metho\n",
        "\n",
        "# Split into samples\n",
        "X_test, y_test = split_sequence(inputs, n_steps)\n",
        "# reshape\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], features) # reshape X_test array to a 3D array with dimensions (number of samples, n_steps, features)\n",
        "#prediction\n",
        "predicted_stock_price = model_lstm.predict(X_test) # predict the stock prices based on the input sequences in X_test\n",
        "#inverse transform the values\n",
        "predicted_stock_price = sc.inverse_transform(predicted_stock_price) # inverse transform using sc.inverse_transform() to obtain the actual stock prices."
      ],
      "metadata": {
        "id": "SqJqOIzf9mrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3.4 Plot Predictions vs. Actual Values from the Test Set**"
      ],
      "metadata": {
        "id": "y5vRRbEc-iVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(test, predicted): #  take in two arguments, test and predicted, which are arrays representing the real and predicted values of a stock price over time\n",
        "    plt.plot(test, color=\"gray\", label=\"Real\") # use the matplotlib  to plot these two arrays on a graph, with the real values in gray and the predicted values in red.\n",
        "    plt.plot(predicted, color=\"red\", label=\"Predicted\")\n",
        "    plt.title(\"MasterCard Stock Price Prediction\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"MasterCard Stock Price\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def return_rmse(test, predicted):\n",
        "    rmse = np.sqrt(mean_squared_error(test, predicted)) #  use numpy to calculate the root mean squared error (RMSE) between the two arrays\n",
        "    print(\"The root mean squared error is {:.2f}.\".format(rmse))\n",
        "\n",
        "plot_predictions(test_set,predicted_stock_price)\n",
        "return_rmse(test_set,predicted_stock_price)\n"
      ],
      "metadata": {
        "id": "zfMLEz83-hY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4. GRU Model**\n",
        "\n",
        "The gated recurrent unit (GRU) is a variation of LSTM as both have design similarities, and in some cases, they produce similar results. GRU uses an update gate and reset gate to solve the vanishing gradient problem. These gates decide what information is important and pass it to the output. The gates can be trained to store information from long ago, without vanishing over time or removing irrelevant information.\n",
        "\n",
        "Unlike LSTM, GRU does not have cell state Ct. It only has a hidden state ht, and due to the simple architecture, GRU has a lower training time compared to LSTM models. The GRU architecture is simpler as it takes input x(t) and the hidden state from the previous timestamp h(t-1) and outputs the new hidden state h(t).\n",
        "\n",
        "<img src =\"https://cdn-images-1.medium.com/max/1500/1*zFhmhw_SZcX4kUVQH-z2aw.jpeg\">\n",
        "\n",
        "We are going to keep everything the same and just replace the LSTM layer with the GRU layer so we can compare the results. The model structure contains a single GRU layer with 125 units and an output layer.\n"
      ],
      "metadata": {
        "id": "IOsXt-Ns_ZbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4.1 Build the Model**"
      ],
      "metadata": {
        "id": "34T2M9Dt_2PI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_gru = Sequential()\n",
        "model_gru.add(GRU(units=125, activation=\"tanh\", input_shape=(n_steps, features)))\n",
        "model_gru.add(Dense(units=1))\n",
        "# Compiling the RNN\n",
        "model_gru.compile(optimizer=\"RMSprop\", loss=\"mse\")\n",
        "\n",
        "model_gru.summary()"
      ],
      "metadata": {
        "id": "RI-G7H3S_jLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4.2 Train the Model**"
      ],
      "metadata": {
        "id": "PqtBZedS_7ap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_gru.fit(X_train, y_train, epochs=50, batch_size=32)"
      ],
      "metadata": {
        "id": "fSFt6vIY_qe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4.3 Run the Model on the Test Set**"
      ],
      "metadata": {
        "id": "6JKu37gPADRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GRU_predicted_stock_price = model_gru.predict(X_test)\n",
        "GRU_predicted_stock_price = sc.inverse_transform(GRU_predicted_stock_price)"
      ],
      "metadata": {
        "id": "XJOoCZaq_wth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4.4 Plot Predictions vs. Actual Values from the Test Set**"
      ],
      "metadata": {
        "id": "YeaysmM9AOcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(test_set, GRU_predicted_stock_price)\n",
        "return_rmse(test_set,GRU_predicted_stock_price)"
      ],
      "metadata": {
        "id": "PV9B1DG0AVc4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}