{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1CMgAOxSe0ui6YM31wrSg942Np7tDjlCh",
      "authorship_tag": "ABX9TyMHIXxl7iDu23LV6Vc8x1FL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shstreuber/Data-Mining/blob/master/Module9_Convolutional_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Module 9: Convolutional Neural Networks**\n",
        "A Convolutional Neural Network (CNN) is a specialized type of deep neural network designed for processing grid-like data, such as images or videos. It is particularly effective in tasks where the spatial relationships between neighboring pixels or regions are important, making it well-suited for tasks like image classification, object detection, and segmentation.\n",
        "\n",
        "At the end of this module, you will be able to:\n",
        "\n",
        "* Explain the components of a Convolutional Neural Network\n",
        "* Configure TensorFlow and Keras to work with a CNN\n",
        "* Solve a simple CNN problem"
      ],
      "metadata": {
        "id": "rMCdNqF0gV6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**0. Key Components and Functions of CNNs**\n",
        "\n",
        "\n",
        "1. **Convolutional Layers**\n",
        " * **Function**: These layers apply learnable filters (kernels) to small regions of the input data, allowing the network to detect features like edges, textures, and patterns at different spatial scales.\n",
        " * **Example:** In an image classification task, early convolutional layers might learn to detect simple features like edges or corners, while deeper layers might combine these features into more complex patterns like textures or parts of objects.\n",
        "**2. Pooling (Subsampling) Layers**\n",
        " * **Function:** Pooling layers reduce the spatial dimensions (width and height) of the input, while retaining important information. They help make the learned features more robust to variations in input size or position.\n",
        " * **Example:** Max pooling, for instance, takes the maximum value from each patch of the feature map, reducing its size and focusing on the most important features detected by the convolutional layers.\n",
        "\n",
        "**3. Activation Functions and Non-linearity**\n",
        " * **Function:** Activation functions like ReLU (Rectified Linear Unit) introduce non-linearities into the network, allowing it to learn and model complex relationships in the data.\n",
        " * **Example:** ReLU is commonly used after convolutional and fully connected layers to introduce non-linearities, enabling the network to approximate more complex functions.\n",
        "\n",
        "**4. Fully Connected Layers**\n",
        " * **Function:** These layers integrate the spatial information learned by the convolutional layers and pooling layers, producing the final output of the network. In image classification, they map the extracted features to the output classes.\n",
        " * **Example:** In an image classification CNN, fully connected layers take the flattened output of the preceding layers and produce logits or probabilities for each class using softmax activation.\n",
        "\n",
        "Here is a great representation of this process, from [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2022/01/convolutional-neural-network-an-overview/):\n",
        "\n",
        "<img src = \"https://editor.analyticsvidhya.com/uploads/59954intro%20to%20CNN.JPG\">\n",
        "\n",
        "And here is [a great code example](https://www.kaggle.com/code/ifeoluwaoduwaiye/cats-vs-dogs-image-classification-using-cnn-95) based on the famous Cats vs Dogs dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "JuBnpvMvhNm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Example of CNN in Image Classification with the CIFAR Dataset**\n",
        "\n",
        "Let's consider an example of using a CNN to classify images from the CIFAR-10 dataset, which contains 60,000 32x32 color images in 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck).\n",
        "\n",
        "To learn more about the CIFAR, click [here](https://www.cs.toronto.edu/~kriz/cifar.html). The following description comes from the introduction to this dataset:\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class ... The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks.\n",
        "\n",
        "Here are the classes in the dataset, as well as 10 random images from each:\n",
        "<center>\n",
        "<img src= \"https://github.com/shstreuber/Data-Mining/blob/master/images/CIFAR.JPG?raw=true\">\n",
        "</center>\n",
        "\n"
      ],
      "metadata": {
        "id": "uUs4FA1Zi5bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we load and preprocess the CIFAR-10 dataset, normalizing pixel values to the range [0, 1]. Normalizing pixel values by dividing by 255 is a common preprocessing step in machine learning tasks, especially when dealing with images. Pixel values in typical images range from 0 to 255, where 0 represents black and 255 represents white (for grayscale images). For RGB color images, each color channel (Red, Green, Blue) also ranges from 0 to 255. Dividing pixel values by 255 ensures that the input data is within a standardized range (0 to 1 or -1 to 1) suitable for neural networks to process efficiently and effectively."
      ],
      "metadata": {
        "id": "v1VvEUCpoRL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, datasets\n",
        "\n",
        "# Load and preprocess CIFAR-10 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0  # Normalize pixel values"
      ],
      "metadata": {
        "id": "19217Kx3oOKZ"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we define the Model Architecture and Compile the Model.\n",
        "\n",
        "* **Architecture**\n",
        " * The CNN starts with a series of convolutional and max pooling layers to extract and downsample features from the images. These layers detect patterns in images by sliding small filters (like edge detectors) across the image.\n",
        "Each filter learns to recognize different features (like edges or textures). These layers detect patterns (like edges or textures) in the images.\n",
        " * The *Conv2D* layers with relu activation start the process. They apply convolutional filters to detect patterns\n",
        " * The *MaxPooling2D* layers reduce reduce the size of the image representation while retaining important information. They help make the network more robust to variations in the position or size of objects in the images. *MaxPooling2D* with (2, 2) reduces each dimension by half.\n",
        " * The Flatten layer transforms the 2D matrix data into a 1D vector, preparing it for the fully connected layers.\n",
        " * Dense layers (aka Fully Connected Layers) at the end classify the images into one of the 10 classes using softmax activation. These layers integrate all the features learned by the convolutional and pooling layers and perform the final classification. The last layer (Dense(10, activation='softmax')) outputs probabilities for each of the 10 classes (like 'cat', 'dog', 'car').\n",
        "\n",
        "* **Compiling**\n",
        " * model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) configures the model for training.\n",
        " * optimizer='adam' specifies the Adam optimizer, a popular choice for gradient-based optimization.\n",
        " * loss='sparse_categorical_crossentropy' sets the loss function appropriate for multi-class classification tasks where the labels are integers.\n",
        " * metrics=['accuracy'] specifies that accuracy should be monitored during training and evaluation."
      ],
      "metadata": {
        "id": "pZSajCHpofCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CNN architecture\n",
        "model = models.Sequential([\n",
        "    # Convolutional layers: detect patterns in images\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),  # Pooling layers: downsample the image\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "\n",
        "    # Flatten layer: prepare data for fully connected layers\n",
        "    layers.Flatten(),\n",
        "\n",
        "    # Fully connected layers: make the final classification\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')  # Output layer with softmax for 10 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',  # Loss function for classification\n",
        "              metrics=['accuracy'])  # Metric to monitor during training"
      ],
      "metadata": {
        "id": "H-CAuLL-o3DB"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we train and evaluate the model:\n",
        "\n",
        "* **Training:** The model is trained (fit) on the training data (train_images, train_labels) for 10 epochs and then evaluated (evaluate) on the test data to see how well it can classify new images it hasn't seen before.\n",
        "* **Evaluation:** After training, the model's performance is evaluated on the test data (test_images, test_labels) using accuracy as the metric.\n",
        " * test_loss, test_acc = model.evaluate(test_images, test_labels) evaluates the model on the test data (test_images, test_labels) and computes the loss and accuracy.\n",
        " * print(f\"Test Accuracy: {test_acc}\") prints the test accuracy after evaluation."
      ],
      "metadata": {
        "id": "01N3FpS9o7hY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model (this will take about 3-5 minutes)\n",
        "model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test Accuracy: {test_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzVASAuGi4dw",
        "outputId": "f0a636ba-4041-4253-8967-be7b0f8a55d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  47/1563 [..............................] - ETA: 58s - loss: 2.2569 - accuracy: 0.1463"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Steps to Classify a New Image**\n",
        "**1. Preprocess the Image**\n",
        "\n",
        "* Ensure the new image is in the same format as the CIFAR-10 dataset images: 32x32 pixels with RGB channels (3 channels).\n",
        "* Normalize the pixel values of the new image to be in the range [0, 1], similar to how the training and test images were preprocessed.\n",
        "\n",
        "We will be using a zipfile of images located [here](https://github.com/shstreuber/Data-Mining/blob/master/images/CIFAR_exercise.zip).\n",
        "\n",
        "\n",
        "**2. Load the Trained Model**\n",
        "* Load the saved model that you trained on the CIFAR-10 dataset. If you haven't saved it yet, you should save it after training using model.save('cifar_model.h5').\n",
        "\n",
        "**3. Upload the Images**\n",
        "* Download the [zipfile](https://github.com/shstreuber/Data-Mining/blob/master/images/CIFAR_exercise.zip) from the instructor's GitHub.\n",
        "* Unzip the file on your computer\n",
        "* In Colab, locate the file folder icon on the left side of the screen and click on it\n",
        "* Drag and drop the unzipped image files from your computer into the Files space in Colab or use the file upload icon.\n",
        "* Once the files are uploaded, right click on a filename and select Copy Path from the popup menu, then paste the path into the new_image_path variable (you'll also need the '').\n",
        "\n",
        "**4. Perform Prediction**\n",
        "* Use the loaded model to predict the class probabilities for the new image.\n",
        "Convert the model's output (probabilities) into a class label by selecting the class with the highest probability.\n",
        "\n",
        "Classify the images you have uploaded under point 3 above using the trained CIFAR-10 model:"
      ],
      "metadata": {
        "id": "HJcv1_LTqJmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, we install OpenCV to load the image\n",
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "rtRvro65s5YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMmvm4KLgM1q"
      },
      "outputs": [],
      "source": [
        "# Now, we are ready to go\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models\n",
        "import numpy as np\n",
        "import cv2  # Assuming you have OpenCV installed for image loading\n",
        "\n",
        "# Load the CIFAR-10 model\n",
        "# model = models.load_model('cifar_model.h5')  # Replace 'cifar_model.h5' with your saved model path\n",
        "model = model\n",
        "\n",
        "# Now, put a .jpg image into the\n",
        "# Load the new image\n",
        "new_image_path = '<paste here the path to the image from the zip file you want to classify. See 3. above>'\n",
        "new_image = cv2.imread(new_image_path)  # Load the image using OpenCV\n",
        "print(new_image.shape) # Verifying that the image has loaded"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the image\n",
        "new_image = cv2.resize(new_image, (32, 32))  # Resize the image to 32x32 pixels\n",
        "new_image = new_image.astype('float32') / 255.0  # Normalize pixel values to [0, 1]\n",
        "\n",
        "# Expand dimensions to create a batch of 1 image (required by the model)\n",
        "new_image = np.expand_dims(new_image, axis=0)\n",
        "print(new_image.shape) # Verifying that the image has been resized; output should show 32"
      ],
      "metadata": {
        "id": "QhIB5g1juT6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see an output like 1, 32, 32, 3. The dimensions 1, 32, 32, 3 represent the shape of a 4D array that is typically used as input to a convolutional neural network (CNN) for image classification tasks. Hereâ€™s a breakdown of each dimension:\n",
        "\n",
        "* 1: This is the batch size, indicating the number of images in the batch. In this case, it is 1, meaning we are processing a single image.\n",
        "* 32: This is the height of the image in pixels. The image has 32 pixels in height.\n",
        "* 32: This is the width of the image in pixels. The image has 32 pixels in width.\n",
        "* 3: This is the number of color channels in the image. A typical color image has three channels corresponding to Red, Green, and Blue (RGB)."
      ],
      "metadata": {
        "id": "N4KS2MU5MBBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform prediction\n",
        "predictions = model.predict(new_image)\n",
        "predicted_class_index = np.argmax(predictions[0])  # Get the index of the class with the highest probability\n",
        "\n",
        "# Print the predicted class\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "predicted_class = class_names[predicted_class_index]\n",
        "print(f\"Predicted Class: {predicted_class}\")"
      ],
      "metadata": {
        "id": "DyFQiqTvyRqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Turn\n",
        "Go to https://www.kaggle.com/code/ifeoluwaoduwaiye/cats-vs-dogs-image-classification-using-cnn-95\n",
        "\n",
        "Here, you will see a great notebook based on the famous [Cats vs Dogs](https://www.kaggle.com/c/dogs-vs-cats/data) dataset.\n",
        "<center>\n",
        "<img src= \"https://storage.googleapis.com/kaggle-media/competitions/kaggle/3362/media/woof_meow.jpg\">\n",
        "</center>\n",
        "\n",
        "As you can see, the model works pretty well given its training data. Now you will use it to analyze the data from the instructor's zip file.\n",
        "\n",
        "**Here is your To Do**\n",
        "* Copy the code from the Kaggle page into your own Google Colab Notebook\n",
        "* Follow point 3 above to download, unzip, and install the images in the [zipfile](https://github.com/shstreuber/Data-Mining/blob/master/images/CIFAR_exercise.zip) from the instructor's GitHub.\n",
        "* Use the Kaggle model to classify the images from the zipfile. You can use the code field below to get started.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7_Ra4OpZOyfA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aCLppo5VRe28"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}