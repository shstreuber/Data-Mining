{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPWm4lXXh35BczwqnzXg9OI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shstreuber/Data-Mining/blob/master/Module7_NeuralNetworks2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRDji9Ayj-1x"
      },
      "source": [
        "# **1. Introduction to Neural Networks**\n",
        "Artificial Neural Networks are modeled on the human brain. Human brain cells, called neurons, form a complex, highly interconnected network and send electrical signals to each other to help humans process information. This means that human brain cells are designed for learning.\n",
        "\n",
        "It also means that Neural Networks are true learning networks in that they **self-optimize** with a set of **learning functions** designed to reduce the error of each processing cycle. This is why Neural Networks work well with large datasets and unstructured data.\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*SJPacPhP4KDEB1AdhOFy_Q.png\" height =400>\n",
        "</center>\n",
        "\n",
        "Want to know more about how human and computer neural networks are different and yet oh so similar? Look [here](https://towardsdatascience.com/the-differences-between-artificial-and-biological-neural-networks-a8b46db828b7) (this is the article from which the picture above stems).\n",
        "\n",
        "At the end of this module, you will be able to:\n",
        "* Describe how a simple Neural Network works\n",
        "* Identify the summation and the activation functions\n",
        "* Take appropriate preprocessing steps for Neural Networks\n",
        "* Build a simple Neural Network\n",
        "\n",
        "Let's get started!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUBGI9YKNzOo"
      },
      "source": [
        "## **What is a Neural Network?**\n",
        "Neural networks are modeled after the human brain. As the name indicates, Neural Networks consist of neurons, where the data processing happens, and dendrites and axons that make up the pathways between neurons.\n",
        "\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/1200px-Neural_network_example.svg.png\" width=\"200\">\n",
        "</div>\n",
        "\n",
        "The data enter the Neural Network through the **INPUT LAYER** (green) and the classification results are found in the **OUTPUT LAYER** (purple). As the data makes its way through the network, its value is determined by weights. These weights are adjusted by an algorithm called a perceptron, whose goal is to minimize error values. That happens int he **HIDDEN LAYER** (teal).\n",
        "\n",
        "Each neuron in the teal hidden layer contains essentially two functions:\n",
        "1. The **SUMMATION FUNCTION**, which aggregates input data and passes its output on to the\n",
        "2. The **ACTIVATION FUNCTION**, which applies a previously defined algorithm to the data it has received from the summary function.\n",
        "\n",
        "Below is a close-up (from [an awesome article on Towards Data Science](https://towardsdatascience.com/inroduction-to-neural-networks-in-python-7e0b422e6c24) that you should read!) of what happens inside one of the teal dots above. The **SUMMATION FUNCTION** is blue and the **ACTIVATION FUNCTION** is purple. Note that the output is red:\n",
        "\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/neuralnetwork.png\" width=\"600\">\n",
        "</div>\n",
        "\n",
        "And now take a look at the instructor video explaining this in detail and demonstrating how this works mathematically:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "u4TtTBoEPyVL",
        "outputId": "3654f1bc-4144-4638-f924-648d6b82b337"
      },
      "source": [
        "from IPython.display import IFrame  # This is just for me so I can embed videos\n",
        "IFrame(src=\"https://www.youtube.com/embed/NyAcHViPlCg\", width=560, height=315)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x784aba4fba90>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"560\"\n",
              "            height=\"315\"\n",
              "            src=\"https://www.youtube.com/embed/NyAcHViPlCg\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1sKIHO-WD_P"
      },
      "source": [
        "#**0. Preparation and Setup**\n",
        "\n",
        "As before, we are following the basic classification steps:\n",
        "\n",
        "1. **Exploratory Data Analysis (EDA)** to see how the data is distributed and to determine what the class attribute in the dataset should be. This will be the attribute you'll predict later on\n",
        "2. **Preprocess the data** (remove n/a, transform data types as needed, deal with missing data) --> here is where we will need to take a few additional steps to **configure our data for the Neural Network**\n",
        "3. **Split the data** into a training set and a test set\n",
        "4. **Build the model** based on the training set\n",
        "5. **Test the model** on the test set\n",
        "6. **Determine the quality of the model** with the help of a Confusion Matrix and a Classification Report.\n",
        "\n",
        "As with our previous problems, we will use the insurance dataset again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3lMYcL5XNLT"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # for visualization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)\n",
        "\n",
        "#Reading in the data as insurance dataframe\n",
        "insurance = pd.read_csv(\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/data/insurance_with_categories.csv\")\n",
        "\n",
        "#Verifying that we can see the data\n",
        "insurance.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nngYDxPqnYd"
      },
      "source": [
        "# **1. Exploratory Data Analysis (EDA)**\n",
        "As before, we have the option to either do this in a code cell just like above, or to import the HTML-based [ydata_profiling](https://docs.profiling.ydata.ai/latest/) package.\n",
        "\n",
        "##Your Turn\n",
        "Remember the [pandas_profiling package](https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/) that you encountered in the module on kNN and Naive Bayes? Install it below to complete this week's EDA section:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoivB2ttodLB"
      },
      "source": [],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzVg0H4MophR"
      },
      "source": [
        "# Visualizing the data with a pairplot because why not?\n",
        "# Let's investigate what kinds of relationships exist between the variables.\n",
        "\n",
        "sns.pairplot(insurance,hue=\"region\", height=3, diag_kind=\"kde\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww0tyu8CpjYP"
      },
      "source": [
        "We can see that the dataset is somewhat structured with linear relationships among age and charges. However, we don't see a clear distribution for region--and we remember that the results from our Random Forest analysis weren't too convincing at about 38% accuracy. Let's see if a Neural Network gives us a better understanding of how the numeric predictors in the dataset can help us determine the region attribute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXOvtOqhfxrb"
      },
      "source": [
        "# **2. Preprocessing**\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.1 Reducing the Data**\n",
        "You have done this before.\n",
        "\n",
        "##Your Turn\n",
        "In the space below, build an insurance_nn dataset consisting of age, bmi, children, charges, and--again--region as the class attribute. You will need this in order to progress through this workbook.\n",
        "\n"
      ],
      "metadata": {
        "id": "B48DsLFdX5B4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z_thTiVf62o"
      },
      "source": [],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e2UHn8VgTor"
      },
      "source": [
        "##**2.2 Preparing the Data for use with a Neural Network**\n",
        "In this section, you will see that preparing data to work with a Neural Network requires a bit more preprocessing than what you may be used to from previous algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jxA9Fyeql_Q"
      },
      "source": [
        "### 2.2.1 Encoding\n",
        "Our Neural Network code will require **numeric labels in the output layer**. It will not work with categorical variables. This is why we will need [LabelEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) to transform the non-numerical labels in 'region' (as long as they are hashable and comparable) to numerical labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOsmo_oPpaNZ"
      },
      "source": [
        "# Replace southwest with 0, southeast with 1, northwest with 2, and northeast with 3\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "insurance_nn[\"region\"] = labelencoder.fit_transform(insurance_nn[\"region\"])\n",
        "region = pd.DataFrame({'region': ['southwest', 'southeast', 'northwest', 'northeast']})\n",
        "insurance_nn.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huDHW4BLtAEp"
      },
      "source": [
        "**NICE!**\n",
        "\n",
        "**UH, WAIT!:** LabelEncoder introduced **A NEW PROBLEM**: The numbers in 'region' look like ordinal relationships--but southeast(1) is not higher than southwest(0) and southeast(1) is not smaller than northwest(2).\n",
        "\n",
        "That's why, in our case, LabelEncoder was the **WRONG SOLUTION**, and we need instead a **four-dimensional vector**.\n",
        "\n",
        "With a four-dimensional vector, our Neural Network can then assign presence (=1) or absence (=0) to each of our four labels. Thus, southwest would be [1,0,0,0], southeast would be [0,1,0,0], northwest would be [0,0,1,0], and northeast [0,0,0,1]. If you google around, you'll find that that's what [Onehotencorder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder) does. That's why this encoding process is often also called OneHot Encoding. **BUT**\n",
        "OneHotEncoder doesn't handle strings very well--and that's what we have in our 'region' attribute: Categorical data in words, i.e. strings.\n",
        "\n",
        "###**A BETTER SOLUTION: get_dummies**\n",
        "\n",
        "To preprocess our data, we will use the Pandas **[get_dummies function](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html)**.\n",
        "\n",
        "**What is a dummy?**\n",
        "\n",
        "A dummy variable is a numeric variable that encodes categorical information. Dummy variables have two possible values: 0 or 1. In a dummy variable:\n",
        "\n",
        "* 1 encodes the presence of a category\n",
        "* 0 encodes the absence of a category\n",
        "\n",
        "For our sex attribute, it would look like this:\n",
        "\n",
        "<img src = \"https://www.sharpsightlabs.com/wp-content/uploads/2022/04/pandas-get-dummies_simple-visual-example.png\" height = 300>\n",
        "\n",
        "**NOTE**: To learn more about get_dummies, check out [this blog post](https://www.sharpsightlabs.com/blog/pandas-get-dummies/), from which the graphic above stems.\n",
        "\n",
        "Both, get_dummies and OneHotEncoder are often both referred to with the same term as OneHot Encoding (which really just means indicating presence or absence of an attribute value with a 1 or a 0, respectively). To learn more about the difference between the two, go [here](https://www.linkedin.com/pulse/both-onehotencoder-pdgetdummies-used-convert-data-what-berger-perkins/).\n",
        "\n",
        "So, let's get started with get_dummies. Before we go any further, though, you will need the insurance_nn dataframe that you have built above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUapVSbAtTHH",
        "collapsed": true
      },
      "source": [
        "def create_dummies(df,column_name):  # We build a function that identifies what to do with the dummies\n",
        "    dummies = pd.get_dummies(df[column_name],prefix=column_name) # We build a variable for the dataframe (df) and the column (column_name)\n",
        "    df = pd.concat([df,dummies],axis=1) # We concatenate the dataframe with the dummies we have built; these dummies are in columns (axis=1)\n",
        "    return df\n",
        "insurance_nn = create_dummies(insurance_nn,\"region\") # We apply the create_dummies function to the insurance_nn dataframe and the region column\n",
        "\n",
        "insurance_nn.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4AwZ60oS1It"
      },
      "source": [
        "And here we are! As you can see above, the region attribute--our class--is no longer just one column (although we left 'region' in the dataframe to prove the point--we will need to delete the column later); it is now 4 columns, indexed from 0 to 3. These are the four labels in the output layer. Now the Neural Network just needs to check which of the \"region indicators\" is switched on, and the data tuple can be dropped under that respective label.\n",
        "\n",
        "But we are not yet done with preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCitftdAuaZd"
      },
      "source": [
        "### 2.2.2 Standardizing the Data ##\n",
        "Well, not only do Neural Networks not like string-type labels in the output layer; they also don't like non-standardized input attributes. That's because the Summation and Activation functions treat the values from each input attribute the same. Hence, these values need to fall into the same scale.\n",
        "\n",
        "For this, we will scale our data with [StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). **Standardization with StandardScaler** is a common preprocessing step that involves transforming your data so that it has a mean of 0 and a standard deviation of 1. That's why it's also called **mean normalization**. The exact math is explained [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
        "\n",
        "**Benefits of Using StandardScaler (aka mean normalization)**\n",
        "- Improves Performance: Many algorithms converge faster and perform better when features are standardized.\n",
        "- Reduces Bias: Standardization ensures that no single feature dominates the algorithm due to its scale.\n",
        "- Comparability: Makes features directly comparable, which is especially important for distance-based algorithms.\n",
        "\n",
        "This helps us to speed up our algorithm (gradient descent) and have a more accurate classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcxOklFFufFl"
      },
      "source": [
        "# Features before mean normalization\n",
        "unscaled_features = insurance_nn[['age','bmi','children','charges']]\n",
        "\n",
        "# Mean Normalization with StandardScaler to have a more reliable classifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "\n",
        "# Calculate mean and standard deviation (fit_) and apply the transformation (_transform)\n",
        "unscaled_features_array = sc.fit_transform(unscaled_features.values)\n",
        "\n",
        "# Assign the scaled data to a DataFrame & use the index and columns arguments to keep your original indices and column names:\n",
        "scaled_features = pd.DataFrame(unscaled_features_array, index=unscaled_features.index, columns=unscaled_features.columns)\n",
        "\n",
        "scaled_features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt-s_OgpW8YR"
      },
      "source": [
        "**Do you notice something?**\n",
        "\n",
        "Yes, the region attribute is gone because all our input attributes are numeric.\n",
        "\n",
        "On to the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgf59SOCqvcr"
      },
      "source": [
        "#**3. Splitting the data into Training and Test Set**\n",
        "We have already used the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from Scikit-Learn, so these steps should look familiar. If you can't remember how it works, here is [a great explanation](https://www.geeksforgeeks.org/how-to-split-the-dataset-with-scikit-learns-train_test_split-function/) on the Geeks for Geeks site."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3A6hRfuphvp"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = scaled_features # those are the scaled input attributes from the StandardScaler\n",
        "y = insurance_nn['region']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLA3RM0-0ZyM"
      },
      "source": [
        "#**4. Determine Neural Network Architecture, then Build and Train the Model**\n",
        "\n",
        "When we build the Neural Network architecture, we have to configure the following settings:\n",
        "* the number of nodes in each layer\n",
        "* the number of layers (if we are using more than one hidden layer)\n",
        "* the way in which nodes are connected (feedforward, no loops between the units).\n",
        "\n",
        "The number of nodes in the INPUT and OUTPUT layers is determined by the dimensionality of the problem: We have 4 input units (age, bmi, children, charges) and 4 output units ('southwest', 'southeast', 'northwest', 'northeast'). We will experiment with the hidden layer.\n",
        "\n",
        "To build the network, we will use the [Multilayer Perceptron classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l9RaApJ1TYS"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp1 = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, activation='relu', solver='adam', random_state=42, verbose=1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb84tiHT5411"
      },
      "source": [
        "Above, we initialize our MLPClassifier like this:\n",
        "\n",
        "```\n",
        "MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, activation='relu', solver='adam', random_state=42, verbose=1)\n",
        "```\n",
        "\n",
        "Let's step through the configuration:\n",
        "\n",
        "* It contains one hidden layer of 100 neurons\n",
        "\n",
        "* It runs through (= iterates through) all the datapoints 500 times (the default is 200 times)\n",
        "\n",
        "* It uses the [ReLU activation function](https://www.kaggle.com/code/dansbecker/rectified-linear-units-relu-in-deep-learning) , where all negative values are converted to 0 and all positive values are y = f(x)\n",
        "<center>\n",
        "<img src = \"https://www.researchgate.net/profile/Anas-Issa/publication/370465617/figure/fig2/AS:11431281155101569@1683057835840/Activation-function-ReLu-ReLu-Rectified-Linear-Activation.png\" height = 300>\n",
        "</center>\n",
        "* It uses the [Adam (=adaptive moment estimation) optimizer](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) as its learning function, which is the most frequently used learning function for neural networks.\n",
        "\n",
        "* When you set random_state to a specific integer (like 42 because, well, it is [the answer to “the Ultimate Question of Life, the Universe, and Everything\"]\n",
        "(https://news.mit.edu/2019/answer-life-universe-and-everything-sum-three-cubes-mathematics-0910)), you ensure that every time you run your code, you get the same result.\n",
        "<center>\n",
        "<img src=\"https://ih1.redbubble.net/image.1970558774.0297/flat,750x,075,f-pad,750x1000,f8f8f8.jpg\" height = 300>\n",
        "</center>\n",
        "* Verbose=1 means that we can observe the output as the model runs. We can turn this off with verbose=0\n",
        "\n",
        "Now that we have built the model, we will train the model on our training set.\n",
        "\n",
        "Since this will run in verbose mode, you will be able to see the self-optimization as it happens. Watch for the loss function (i.e. the error rate, which Adam calculates for us) becoming smaller with each iteration. This means that our model is learning from its errors and improving!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb7H9aCf53qG"
      },
      "source": [
        "# As this runs in verbose mode, note how the loss function steps down with every single iteration!\n",
        "\n",
        "mlp1.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How Can We Tune the Multilayer Perceptron?**\n",
        "\n",
        "There are many ways to adjust the parameters of (=tune) the Multilayer Perceptron function in order to make it work more smoothly for us. The most salient parameters are:\n",
        "\n",
        "* hidden_layer_sizes tuple, length = n_layers - 2, default=(100,)\n",
        "The ith element represents the number of neurons in the ith hidden layer.\n",
        "\n",
        "* activation{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default=’relu’\n",
        "Activation function for the hidden layer.\n",
        "\n",
        "  * ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n",
        "\n",
        "  * ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
        "\n",
        "  * ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n",
        "\n",
        "  * ‘relu’, the rectified linear unit function, returns f(x) = max(0, x)\n",
        "\n",
        "* solver{‘lbfgs’, ‘sgd’, ‘adam’}, default=’adam’\n",
        "The solver for weight optimization. We will talk about this more when we talk about Gradient Descent\n",
        "\n",
        "  * ‘sgd’ refers to stochastic gradient descent.\n",
        "\n",
        "  * ‘adam’ refers to a stochastic gradient-based optimizer\n",
        "\n",
        "* batch_size int, default=’auto’\n",
        "Size of minibatches for stochastic optimizers. When set to “auto”, batch_size=min(200, n_samples)\n",
        "\n",
        "* max_iter int, default=200\n",
        "Maximum number of iterations. For stochastic solvers (‘sgd’, ‘adam’), note that this determines the number of epochs (how many times each data point will be used).\n",
        "\n",
        "* verbose bool, default=False\n",
        "Whether to print progress messages to stdout.\n",
        "\n",
        "* validation_fraction float, default=0.1\n",
        "The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True\n",
        "\n",
        "As you can see, tuning can be really useful."
      ],
      "metadata": {
        "id": "cR334sOVvSSo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKWALsXd8nOC"
      },
      "source": [
        "#**5. Test the Model**\n",
        "The moment of truth! So far, we have seen that the error rate decreases, whihc means that our model learns something. But how much does the model improve with each iteration? That's what the **accuracy score** measures.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQonOUkG83NV"
      },
      "source": [
        "# Score takes a feature matrix X_test and the expected target values y_test.\n",
        "# Predictions for X_test are compared with y_test\n",
        "\n",
        "print (mlp1.score(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are seeing the output above as a decimal number. Translate it into percent, and you'll see how many percent of the original data has been categorized correctly. Now let's see what the predictions look like:"
      ],
      "metadata": {
        "id": "Uu8InrSd9SZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's see what the predictions look like\n",
        "y_pred = mlp1.predict(X_test)\n",
        "print(\"Test set predictions: \\n {}\".format(y_pred))"
      ],
      "metadata": {
        "id": "e-NtC0VoBEJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://media.istockphoto.com/id/874505226/vector/contempt-emoticon.jpg?s=612x612&w=0&k=20&c=9ELAryuCY6EPIBlBiMwYf5lz-7QZ_o6WOQPqi5pbxks=\" height = 200>\n",
        "Hm. Does this look good to you?"
      ],
      "metadata": {
        "id": "9OUe4eGIBHK8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB9Y4dSr9JeF"
      },
      "source": [
        "#**6. Evaluate the Quality**\n",
        "You know the drill! First, we set up the Confusion Matrix, then the Classification Report. You did this for the modules on k Nearest Neighbor, Naive Bayes, and Random Forest, so you're already a pro at this!\n",
        "\n",
        "##Your Turn\n",
        "Build the Confusion Matrix and the Classification Report below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6N7CNRS9iU3"
      },
      "source": [
        "# Build the Confusion Matrix here\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the Classification Report here\n"
      ],
      "metadata": {
        "id": "AfPRP75lbRsy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your Turn\n",
        "What do you think of our model, given this Confusion Matrix and the Classification Report? Is it better or worse than our Random Forest? How does it compare with our k Nearest Neighbor or our Naive Bayes output? Record your answer below."
      ],
      "metadata": {
        "id": "2mF8wNWnAto2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "irxtIhvhA1Mk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s6Og2M4vCNk1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4joVMw04GdY"
      },
      "source": [
        "# **7. Using Gradient Descent**\n",
        "\n",
        "Gradient Descent is a way to optimize how quickly the weights are adjusted in the Neural Network, i.e. how often the Feed Forward-Backpropagation loop has to run to optimize the error aka loss function. To learn more about Gradient Descent, read through [this explanation](https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html) and watch this awesome video below:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RXJuzByrMT-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "164acbda-82fc-4465-9ae1-b33c08ef6c60"
      },
      "source": [
        "from IPython.display import IFrame  # This is just for me so I can embed videos\n",
        "IFrame(src=\"https://www.youtube.com/embed/sDv4f4s2SB8\", width=560, height=315)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x784ab9372d10>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"560\"\n",
              "            height=\"315\"\n",
              "            src=\"https://www.youtube.com/embed/sDv4f4s2SB8\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0OtK5yarM1e"
      },
      "source": [
        "Now that you know all about gradient descent, we can re-run the MLP Classifier with the Stochastic Gradient Descent optimizer configured. We will specifically pay attention to the parameters shown.\n",
        "\n",
        "* Stochastic Gradient Descent (SGD) optimizer/solver: updates weights values that minimize the loss function in batches. As the name says, SGD uses the gradient of the loss function. Use parameter solver = sgd.\n",
        "\n",
        "* learning_rate_init=0.01: This parameter controls the step-size in updating the weights, and here is constant. Its value shouldn’t be too large( fail to converge) neither too small (too slow).\n",
        "\n",
        "* max_iter=500 : maximum number of epochs (=how many times each data point will be used until solver convergence)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN3TycH84uY0"
      },
      "source": [
        "mlp2 = MLPClassifier(hidden_layer_sizes=10,solver='sgd',learning_rate_init= 0.01, max_iter=500, verbose = 1)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nav1PUSs_J-2"
      },
      "source": [
        "## YOUR TURN\n",
        "\n",
        "1. Fit the test data to mlp2, then produce a confusion matrix and a correlation report, and see if changing the parameters has made any difference.\n",
        "2. Build an mlp3 model in which you change the solver and the learning rate  however you want (leave verbose on, though). This will change the Gradient Descent. What difference has this made?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**7. THE BIG QUESTION: How Do You Know How To Architect the Network?**\n",
        "\n",
        "Deciding the number of layers and the number of nodes per layer in a neural network involves a combination of domain knowledge, experimentation, and understanding of the problem at hand. We have seen that configuring the INPUT and OUTPUT layers is pretty intuitive. However, determining the number of HIDDEN NODES is not as easy. Here are some guidelines and strategies to help make these decisions:\n",
        "\n",
        "**1. Understanding the Problem and Data**\n",
        " * Complexity of the Problem: Complex problems like image recognition, natural language processing, or playing games typically require deeper networks (more layers) because they need to capture more intricate patterns and hierarchical representations.\n",
        " * Size of the Dataset: Larger datasets can support more complex models. If you have a small dataset, a simpler model (fewer layers and nodes) is often more appropriate to avoid overfitting.\n",
        "\n",
        "**2. Guidelines for the Number of Layers**\n",
        " * Input and Output: Start with the input layer (which has as many nodes as there are input features) and the output layer (which has as many nodes as there are output classes or a single node for regression). See above.\n",
        " * Single Hidden Layer: For many simple problems, a single hidden layer can be sufficient. The Universal Approximation Theorem states that a single hidden layer with enough nodes can approximate any continuous function, but practical performance may vary.\n",
        " * Deep Networks: For more complex tasks, deep networks with multiple hidden layers can be more effective. Common deep learning architectures (like CNNs and RNNs) have predefined structures with many layers.\n",
        "\n",
        "**3. Guidelines for the Number of Nodes**\n",
        " * Input Nodes: The input layer should have one node for each feature in your dataset.\n",
        " * Output Nodes: The output layer should have one node per class (for classification problems) or a single node (for regression problems).\n",
        " * Hidden Nodes: There is no definitive rule, but a common heuristic is to start with a number of nodes between the size of the input layer and the output layer. Some approaches include:\n",
        "  * Increasing Size: Start with fewer nodes and increase the number incrementally.\n",
        "  * Decreasing Size: Use more nodes in the initial layers and decrease the number as you go deeper (e.g., a funnel-like structure).\n",
        "  * Experimentation: Begin with a small network and increase the number of nodes and layers incrementally, observing the impact on performance.\n",
        "\n",
        "**4. Practical Steps for Designing Neural Networks**\n",
        " * Start Simple: Begin with a simple architecture. For example, a single hidden layer with a number of nodes roughly between the input and output layer sizes.\n",
        " * Experiment Incrementally: Gradually increase the number of layers and nodes. Observe how the model's performance on validation data changes.\n",
        " * Early Stopping and Regularization: Implement early stopping (i.e. fewer epochs) to prevent overfitting. Use regularization techniques like dropout, L2 regularization, and batch normalization, which we will discuss in the next module.\n",
        " * Review Architectures of Similar Problems: Look at existing research or common architectures for similar problems to get a sense of what works well.\n",
        "\n",
        "**5. Tools and Resources**\n",
        " * Frameworks: Use deep learning frameworks like TensorFlow, Keras, and PyTorch, which provide high-level APIs to experiment with different architectures easily.\n",
        " * Community: Engage with communities on platforms like Stack Overflow, GitHub, and specialized forums to get feedback and insights on your architecture.\n",
        "\n",
        "**WAIT!**\n",
        "Look at point 2 above! Number of layers? Does this mean that, instead of the single hidden layer (in teal below),\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/1200px-Neural_network_example.svg.png\" width=\"200\">\n",
        "</center>\n",
        "we can actually build more layers?\n",
        "\n",
        "Yes, yes we can. That brings us to Deep Learning, the topic of our next module."
      ],
      "metadata": {
        "id": "Mx5mYc9sIp4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. If You Are Stuck\n",
        "Here are the solutions to the Your Turn problems.\n",
        "\n",
        "**NOTE** that I have included them in text format only so you can't simply run them; you actually have to type them into the spaces above."
      ],
      "metadata": {
        "id": "RJcK6Z-axYaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# 2.1 Building the insurance_nn dataframe\n",
        "\n",
        "insurance_nn = pd.DataFrame(insurance, columns = ['age', 'bmi', 'children','charges','region'])\n",
        "insurance_nn.head()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "G6B7nBOZ_InO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# 6. Confusion Matrix and Classification Report\n",
        "\n",
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred, labels=mlp1.classes_)\n",
        "cm_display = ConfusionMatrixDisplay(cm, display_labels=mlp1.classes_).plot()\n",
        "\n",
        "# Classification Report\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "scX1QazJ_dk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# 7. Gradient Descent Exercises\n",
        "\n",
        "mlp2 = MLPClassifier(hidden_layer_sizes=10,solver='sgd',learning_rate_init= 0.01, max_iter=500, verbose = 1)\n",
        "mlp2.fit(X_train, y_train)\n",
        "cm_mlp2 = confusion_matrix(y_test, y_pred, labels=mlp2.classes_)\n",
        "cm_display_mlp2 = ConfusionMatrixDisplay(cm_mlp2, display_labels=mlp2.classes_).plot()\n",
        "\n",
        "\n",
        "mlp3 = MLPClassifier(hidden_layer_sizes=a_new_number,solver=pick_another_solver,learning_rate_init= 0.01, max_iter=change_the_number_of_iterations, verbose = 1)\n",
        "mlp3.fit(X_train, y_train)\n",
        "cm_mlp3 = confusion_matrix(y_test, y_pred, labels=mlp3.classes_)\n",
        "cm_display_mlp3 = ConfusionMatrixDisplay(cm_mlp3, display_labels=mlp3.classes_).plot()\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "iGKUQYBqDiED"
      }
    }
  ]
}